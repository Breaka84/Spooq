%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[a4paper,10pt, twoside,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
\edef\sphinxdqmaybe{\ifdefined\DeclareUnicodeCharacterAsOptional\string"\fi}
  \DeclareUnicodeCharacter{\sphinxdqmaybe00A0}{\nobreakspace}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2500}{\sphinxunichar{2500}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2502}{\sphinxunichar{2502}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2514}{\sphinxunichar{2514}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe251C}{\sphinxunichar{251C}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage[english]{babel}
\usepackage{times}
\usepackage[Sonny]{fncychap}
\ChNameVar{\Large\normalfont\sffamily}
\ChTitleVar{\Large\normalfont\sffamily}
\usepackage{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}

% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\figurename}{Fig.\@ }}
\makeatletter
\def\fnum@figure{\figurename\thefigure{}}
\makeatother
\addto\captionsenglish{\renewcommand{\tablename}{Table }}
\makeatletter
\def\fnum@table{\tablename\thetable{}}
\makeatother
\addto\captionsenglish{\renewcommand{\literalblockname}{Listing}}

\addto\captionsenglish{\renewcommand{\literalblockcontinuedname}{continued from previous page}}
\addto\captionsenglish{\renewcommand{\literalblockcontinuesname}{continues on next page}}
\addto\captionsenglish{\renewcommand{\sphinxnonalphabeticalgroupname}{Non-alphabetical}}
\addto\captionsenglish{\renewcommand{\sphinxsymbolsname}{Symbols}}
\addto\captionsenglish{\renewcommand{\sphinxnumbersname}{Numbers}}

\addto\extrasenglish{\def\pageautorefname{page}}

\setcounter{tocdepth}{2}


        \usepackage[columns=1]{idxlayout}
        \usepackage{geometry}
    

\title{Spooq2 Documentation}
\date{Aug 17, 2020}
\release{2.1.0}
\author{David Eigenstuhler}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\ifdefined\shorthandoff
  \ifnum\catcode`\=\string=\active\shorthandoff{=}\fi
  \ifnum\catcode`\"=\active\shorthandoff{"}\fi
\fi

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}


\sphinxstylestrong{Spooq} is your PySpark based helper library for ETL data ingestion pipeline in Data Lakes.

Extractors, Transformers, and Loaders are independent components which can be plugged-in into a pipeline instance or used separately.


\chapter{Table of Content}
\label{\detokenize{index:table-of-content}}

\section{Installation / Deployment}
\label{\detokenize{installation:installation-deployment}}\label{\detokenize{installation::doc}}

\subsection{Build egg file}
\label{\detokenize{installation:build-egg-file}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} \PYG{n+nb}{cd} spooq2
\PYGZdl{} python setup.py bdist\PYGZus{}egg
\end{sphinxVerbatim}

The output is stored as \sphinxtitleref{dist/Spooq2-\textless{}VERSION\_NUMBER\textgreater{}-py2.7.egg}


\subsection{Build zip file}
\label{\detokenize{installation:build-zip-file}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} \PYG{n+nb}{cd} spooq2
\PYGZdl{} rm temp.zip
\PYGZdl{} zip \PYGZhy{}r temp.zip src/spooq2
\PYGZdl{} mv temp.zip Spooq2\PYGZus{}\PYG{k}{\PYGZdl{}(}grep \PYG{l+s+s2}{\PYGZdq{}\PYGZus{}\PYGZus{}version\PYGZus{}\PYGZus{}\PYGZdq{}} src/spooq2/\PYGZus{}version.py \PYG{p}{\textbar{}} \PYG{l+s+se}{\PYGZbs{}}
    cut \PYGZhy{}d \PYG{l+s+s2}{\PYGZdq{} \PYGZdq{}} \PYGZhy{}f \PYG{l+m}{3} \PYG{p}{\textbar{}} tr \PYGZhy{}d \PYG{l+s+se}{\PYGZbs{}\PYGZdq{}}\PYG{k}{)}.zip
\end{sphinxVerbatim}

The output is stored as \sphinxtitleref{Spooq2-\textless{}VERSION\_NUMBER\textgreater{}.zip}.


\subsection{Include pre-build package (egg or zip) with Spark}
\label{\detokenize{installation:include-pre-build-package-egg-or-zip-with-spark}}
For Submitting or Launching Spark:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} pyspark \PYGZhy{}\PYGZhy{}py\PYGZhy{}files Spooq2\PYGZhy{}\PYGZlt{}VERSION\PYGZus{}NUMBER\PYGZgt{}.egg
\end{sphinxVerbatim}

The library still has to be imported in the pyspark application!

Within Running Spark Session:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{sc}\PYG{o}{.}\PYG{n}{addFile}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Spooq2\PYGZhy{}\PYGZlt{}VERSION\PYGZus{}NUMBER\PYGZgt{}.egg}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{import} \PYG{n+nn}{spooq2}
\end{sphinxVerbatim}


\subsection{Install local repository as package}
\label{\detokenize{installation:install-local-repository-as-package}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} \PYG{n+nb}{cd} spooq2
\PYGZdl{} python setup.py install
\end{sphinxVerbatim}


\subsection{Install Spooq2 directly from git}
\label{\detokenize{installation:install-spooq2-directly-from-git}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} pip install git+https://github.com/breaka84/spooq@master
\end{sphinxVerbatim}


\subsection{Development, Testing, and Documenting}
\label{\detokenize{installation:development-testing-and-documenting}}
Please refer to {\hyperref[\detokenize{setup_development_testing:dev-setup}]{\sphinxcrossref{\DUrole{std,std-ref}{Setup for Development, Testing, Documenting}}}}.


\section{Examples}
\label{\detokenize{examples:examples}}\label{\detokenize{examples::doc}}

\subsection{JSON Files to Partitioned Hive Table}
\label{\detokenize{examples:json-files-to-partitioned-hive-table}}

\subsubsection{Sample Input Data:}
\label{\detokenize{examples:sample-input-data}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
  \PYG{n+nt}{\PYGZdq{}id\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{18}\PYG{p}{,}
  \PYG{n+nt}{\PYGZdq{}guid\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}b12b59ba\PYGZhy{}5c78\PYGZhy{}4057\PYGZhy{}a998\PYGZhy{}469497005c1f\PYGZdq{}}\PYG{p}{,}
  \PYG{n+nt}{\PYGZdq{}attributes\PYGZdq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}
    \PYG{n+nt}{\PYGZdq{}first\PYGZus{}name\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}Jeannette\PYGZdq{}}\PYG{p}{,}
    \PYG{n+nt}{\PYGZdq{}last\PYGZus{}name\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}O\PYGZsq{}Loghlen\PYGZdq{}}\PYG{p}{,}
    \PYG{n+nt}{\PYGZdq{}gender\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}F\PYGZdq{}}\PYG{p}{,}
    \PYG{n+nt}{\PYGZdq{}email\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}gpirri3j@oracle.com\PYGZdq{}}\PYG{p}{,}
    \PYG{n+nt}{\PYGZdq{}ip\PYGZus{}address\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}64.19.237.154\PYGZdq{}}\PYG{p}{,}
    \PYG{n+nt}{\PYGZdq{}university\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}}\PYG{p}{,}
    \PYG{n+nt}{\PYGZdq{}birthday\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}1972\PYGZhy{}05\PYGZhy{}16T22:17:41Z\PYGZdq{}}\PYG{p}{,}
    \PYG{n+nt}{\PYGZdq{}friends\PYGZdq{}}\PYG{p}{:} \PYG{p}{[}
      \PYG{p}{\PYGZob{}}
        \PYG{n+nt}{\PYGZdq{}first\PYGZus{}name\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}Noémie\PYGZdq{}}\PYG{p}{,}
        \PYG{n+nt}{\PYGZdq{}last\PYGZus{}name\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}Tibbles\PYGZdq{}}\PYG{p}{,}
        \PYG{n+nt}{\PYGZdq{}id\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{9952}
      \PYG{p}{\PYGZcb{}}\PYG{p}{,}
      \PYG{p}{\PYGZob{}}
        \PYG{n+nt}{\PYGZdq{}first\PYGZus{}name\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}Bérangère\PYGZdq{}}\PYG{p}{,}
        \PYG{n+nt}{\PYGZdq{}last\PYGZus{}name\PYGZdq{}}\PYG{p}{:} \PYG{k+kc}{null}\PYG{p}{,}
        \PYG{n+nt}{\PYGZdq{}id\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{3391}
      \PYG{p}{\PYGZcb{}}\PYG{p}{,}
      \PYG{p}{\PYGZob{}}
        \PYG{n+nt}{\PYGZdq{}first\PYGZus{}name\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}Danièle\PYGZdq{}}\PYG{p}{,}
        \PYG{n+nt}{\PYGZdq{}last\PYGZus{}name\PYGZdq{}}\PYG{p}{:} \PYG{k+kc}{null}\PYG{p}{,}
        \PYG{n+nt}{\PYGZdq{}id\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{9637}
      \PYG{p}{\PYGZcb{}}\PYG{p}{,}
      \PYG{p}{\PYGZob{}}
        \PYG{n+nt}{\PYGZdq{}first\PYGZus{}name\PYGZdq{}}\PYG{p}{:} \PYG{k+kc}{null}\PYG{p}{,}
        \PYG{n+nt}{\PYGZdq{}last\PYGZus{}name\PYGZdq{}}\PYG{p}{:} \PYG{k+kc}{null}\PYG{p}{,}
        \PYG{n+nt}{\PYGZdq{}id\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{9939}
      \PYG{p}{\PYGZcb{}}\PYG{p}{,}
      \PYG{p}{\PYGZob{}}
        \PYG{n+nt}{\PYGZdq{}first\PYGZus{}name\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}Anaëlle\PYGZdq{}}\PYG{p}{,}
        \PYG{n+nt}{\PYGZdq{}last\PYGZus{}name\PYGZdq{}}\PYG{p}{:} \PYG{k+kc}{null}\PYG{p}{,}
        \PYG{n+nt}{\PYGZdq{}id\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{18994}
      \PYG{p}{\PYGZcb{}}
    \PYG{p}{]}
  \PYG{p}{\PYGZcb{}}\PYG{p}{,}
  \PYG{n+nt}{\PYGZdq{}meta\PYGZdq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}
    \PYG{n+nt}{\PYGZdq{}created\PYGZus{}at\PYGZus{}sec\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{1547371284}\PYG{p}{,}
    \PYG{n+nt}{\PYGZdq{}created\PYGZus{}at\PYGZus{}ms\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{1547204429000}\PYG{p}{,}
    \PYG{n+nt}{\PYGZdq{}version\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{24}
  \PYG{p}{\PYGZcb{}}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}


\subsubsection{Sample Output Tables}
\label{\detokenize{examples:sample-output-tables}}

\begin{savenotes}\sphinxatlongtablestart\begin{longtable}[c]{|l|l|l|l|l|l|l|l|}
\sphinxthelongtablecaptionisattop
\caption{Table \sphinxstylestrong{“user”}\strut}\label{\detokenize{examples:id1}}\\*[\sphinxlongtablecapskipadjust]
\hline
\sphinxstyletheadfamily 
id
&\sphinxstyletheadfamily 
guid
&\sphinxstyletheadfamily 
forename
&\sphinxstyletheadfamily 
surname
&\sphinxstyletheadfamily 
gender
&\sphinxstyletheadfamily 
has\_email
&\sphinxstyletheadfamily 
has\_university
&\sphinxstyletheadfamily 
created\_at
\\
\hline
\endfirsthead

\multicolumn{8}{c}%
{\makebox[0pt]{\sphinxtablecontinued{\tablename\ \thetable{} -- continued from previous page}}}\\
\hline
\sphinxstyletheadfamily 
id
&\sphinxstyletheadfamily 
guid
&\sphinxstyletheadfamily 
forename
&\sphinxstyletheadfamily 
surname
&\sphinxstyletheadfamily 
gender
&\sphinxstyletheadfamily 
has\_email
&\sphinxstyletheadfamily 
has\_university
&\sphinxstyletheadfamily 
created\_at
\\
\hline
\endhead

\hline
\multicolumn{8}{r}{\makebox[0pt][r]{\sphinxtablecontinued{Continued on next page}}}\\
\endfoot

\endlastfoot

18
&
“b12b59ba…”
&
“Jeannette”
&
“O”Loghlen”
&
“F”
&
“1”
&
NULL
&
1547204429
\\
\hline
…
&
…
&
…
&
…
&
…
&
…
&
…
&
…
\\
\hline
\end{longtable}\sphinxatlongtableend\end{savenotes}

\begin{DUlineblock}{0em}
\item[] 
\end{DUlineblock}


\begin{savenotes}\sphinxatlongtablestart\begin{longtable}[c]{|l|l|l|l|}
\sphinxthelongtablecaptionisattop
\caption{Table \sphinxstylestrong{“friends\_mapping”}\strut}\label{\detokenize{examples:id2}}\\*[\sphinxlongtablecapskipadjust]
\hline
\sphinxstyletheadfamily 
id
&\sphinxstyletheadfamily 
guid
&\sphinxstyletheadfamily 
friend\_id
&\sphinxstyletheadfamily 
created\_at
\\
\hline
\endfirsthead

\multicolumn{4}{c}%
{\makebox[0pt]{\sphinxtablecontinued{\tablename\ \thetable{} -- continued from previous page}}}\\
\hline
\sphinxstyletheadfamily 
id
&\sphinxstyletheadfamily 
guid
&\sphinxstyletheadfamily 
friend\_id
&\sphinxstyletheadfamily 
created\_at
\\
\hline
\endhead

\hline
\multicolumn{4}{r}{\makebox[0pt][r]{\sphinxtablecontinued{Continued on next page}}}\\
\endfoot

\endlastfoot

18
&
b12b59ba…
&
9952
&
1547204429
\\
\hline
18
&
b12b59ba…
&
3391
&
1547204429
\\
\hline
18
&
b12b59ba…
&
9637
&
1547204429
\\
\hline
18
&
b12b59ba…
&
9939
&
1547204429
\\
\hline
18
&
b12b59ba…
&
18994
&
1547204429
\\
\hline
…
&
…
&
…
&
…
\\
\hline
\end{longtable}\sphinxatlongtableend\end{savenotes}


\subsubsection{Application Code for Updating the Users Table}
\label{\detokenize{examples:application-code-for-updating-the-users-table}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{spooq2}\PYG{n+nn}{.}\PYG{n+nn}{pipeline} \PYG{k+kn}{import} \PYG{n}{Pipeline}
\PYG{k+kn}{import} \PYG{n+nn}{spooq2}\PYG{n+nn}{.}\PYG{n+nn}{extractor} \PYG{k}{as} \PYG{n+nn}{E}
\PYG{k+kn}{import} \PYG{n+nn}{spooq2}\PYG{n+nn}{.}\PYG{n+nn}{transformer} \PYG{k}{as} \PYG{n+nn}{T}
\PYG{k+kn}{import} \PYG{n+nn}{spooq2}\PYG{n+nn}{.}\PYG{n+nn}{loader} \PYG{k}{as} \PYG{n+nn}{L}

\PYG{n}{users\PYGZus{}mapping} \PYG{o}{=} \PYG{p}{[}
    \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{id}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}              \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{id}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}                     \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{IntegerType}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
    \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{guid}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{guid}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}                   \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{StringType}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
    \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{forename}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{attributes.first\PYGZus{}name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}  \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{StringType}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
    \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{surename}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{attributes.last\PYGZus{}name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}   \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{StringType}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
    \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{gender}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}          \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{attributes.gender}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}      \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{StringType}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
    \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{has\PYGZus{}email}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}       \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{attributes.email}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}       \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{StringBoolean}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
    \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{has\PYGZus{}university}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}  \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{attributes.university}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}  \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{StringBoolean}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
    \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{created\PYGZus{}at}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}      \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{meta.created\PYGZus{}at\PYGZus{}ms}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}     \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{timestamp\PYGZus{}ms\PYGZus{}to\PYGZus{}s}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
\PYG{p}{]}

\PYG{n}{users\PYGZus{}pipeline} \PYG{o}{=} \PYG{n}{Pipeline}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{users\PYGZus{}pipeline}\PYG{o}{.}\PYG{n}{set\PYGZus{}extractor}\PYG{p}{(}\PYG{n}{E}\PYG{o}{.}\PYG{n}{JSONExtractor}\PYG{p}{(}\PYG{n}{input\PYGZus{}path}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tests/data/schema\PYGZus{}v1/sequenceFiles}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{users\PYGZus{}pipeline}\PYG{o}{.}\PYG{n}{add\PYGZus{}transformers}\PYG{p}{(}
    \PYG{p}{[}
        \PYG{n}{T}\PYG{o}{.}\PYG{n}{Mapper}\PYG{p}{(}\PYG{n}{mapping}\PYG{o}{=}\PYG{n}{users\PYGZus{}mapping}\PYG{p}{)}\PYG{p}{,}
        \PYG{n}{T}\PYG{o}{.}\PYG{n}{ThresholdCleaner}\PYG{p}{(}
            \PYG{n}{range\PYGZus{}definitions}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{created\PYGZus{}at}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{min}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{max}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{1580737513}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{default}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{k+kc}{None}\PYG{p}{\PYGZcb{}}\PYG{p}{\PYGZcb{}}
        \PYG{p}{)}\PYG{p}{,}
        \PYG{n}{T}\PYG{o}{.}\PYG{n}{NewestByGroup}\PYG{p}{(}\PYG{n}{group\PYGZus{}by}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{id}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{order\PYGZus{}by}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{created\PYGZus{}at}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
    \PYG{p}{]}
\PYG{p}{)}

\PYG{n}{users\PYGZus{}pipeline}\PYG{o}{.}\PYG{n}{set\PYGZus{}loader}\PYG{p}{(}
    \PYG{n}{L}\PYG{o}{.}\PYG{n}{HiveLoader}\PYG{p}{(}
        \PYG{n}{db\PYGZus{}name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{users\PYGZus{}and\PYGZus{}friends}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
        \PYG{n}{table\PYGZus{}name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{users}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
        \PYG{n}{partition\PYGZus{}definitions}\PYG{o}{=}\PYG{p}{[}
            \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{column\PYGZus{}name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{dt}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{column\PYGZus{}type}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{IntegerType}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{default\PYGZus{}value}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{20200201}\PYG{p}{\PYGZcb{}}
        \PYG{p}{]}\PYG{p}{,}
        \PYG{n}{repartition\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,}
    \PYG{p}{)}
\PYG{p}{)}

\PYG{n}{users\PYGZus{}pipeline}\PYG{o}{.}\PYG{n}{execute}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}


\subsubsection{Application Code for Updating the Friends\_Mapping Table}
\label{\detokenize{examples:application-code-for-updating-the-friends-mapping-table}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{spooq2}\PYG{n+nn}{.}\PYG{n+nn}{pipeline} \PYG{k+kn}{import} \PYG{n}{Pipeline}
\PYG{k+kn}{import} \PYG{n+nn}{spooq2}\PYG{n+nn}{.}\PYG{n+nn}{extractor} \PYG{k}{as} \PYG{n+nn}{E}
\PYG{k+kn}{import} \PYG{n+nn}{spooq2}\PYG{n+nn}{.}\PYG{n+nn}{transformer} \PYG{k}{as} \PYG{n+nn}{T}
\PYG{k+kn}{import} \PYG{n+nn}{spooq2}\PYG{n+nn}{.}\PYG{n+nn}{loader} \PYG{k}{as} \PYG{n+nn}{L}


\PYG{n}{friends\PYGZus{}mapping} \PYG{o}{=} \PYG{p}{[}
    \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{id}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}          \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{id}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}                  \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{IntegerType}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
    \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{guid}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{guid}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}                \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{StringType}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
    \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{friend\PYGZus{}id}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}   \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{friend.id}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}           \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{IntegerType}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
    \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{created\PYGZus{}at}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}  \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{meta.created\PYGZus{}at\PYGZus{}ms}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}  \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{timestamp\PYGZus{}ms\PYGZus{}to\PYGZus{}s}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
\PYG{p}{]}

\PYG{n}{friends\PYGZus{}pipeline} \PYG{o}{=} \PYG{n}{Pipeline}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{friends\PYGZus{}pipeline}\PYG{o}{.}\PYG{n}{set\PYGZus{}extractor}\PYG{p}{(}\PYG{n}{E}\PYG{o}{.}\PYG{n}{JSONExtractor}\PYG{p}{(}\PYG{n}{input\PYGZus{}path}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tests/data/schema\PYGZus{}v1/sequenceFiles}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{friends\PYGZus{}pipeline}\PYG{o}{.}\PYG{n}{add\PYGZus{}transformers}\PYG{p}{(}
    \PYG{p}{[}
        \PYG{n}{T}\PYG{o}{.}\PYG{n}{NewestByGroup}\PYG{p}{(}\PYG{n}{group\PYGZus{}by}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{id}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{order\PYGZus{}by}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{meta.created\PYGZus{}at\PYGZus{}ms}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
        \PYG{n}{T}\PYG{o}{.}\PYG{n}{Exploder}\PYG{p}{(}\PYG{n}{path\PYGZus{}to\PYGZus{}array}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{attributes.friends}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{exploded\PYGZus{}elem\PYGZus{}name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{friend}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
        \PYG{n}{T}\PYG{o}{.}\PYG{n}{Mapper}\PYG{p}{(}\PYG{n}{mapping}\PYG{o}{=}\PYG{n}{friends\PYGZus{}mapping}\PYG{p}{)}\PYG{p}{,}
        \PYG{n}{T}\PYG{o}{.}\PYG{n}{ThresholdCleaner}\PYG{p}{(}
            \PYG{n}{range\PYGZus{}definitions}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{created\PYGZus{}at}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{min}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{max}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{1580737513}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{default}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{k+kc}{None}\PYG{p}{\PYGZcb{}}\PYG{p}{\PYGZcb{}}
        \PYG{p}{)}\PYG{p}{,}
    \PYG{p}{]}
\PYG{p}{)}

\PYG{n}{friends\PYGZus{}pipeline}\PYG{o}{.}\PYG{n}{set\PYGZus{}loader}\PYG{p}{(}
    \PYG{n}{L}\PYG{o}{.}\PYG{n}{HiveLoader}\PYG{p}{(}
        \PYG{n}{db\PYGZus{}name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{users\PYGZus{}and\PYGZus{}friends}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
        \PYG{n}{table\PYGZus{}name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{friends\PYGZus{}mapping}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
        \PYG{n}{partition\PYGZus{}definitions}\PYG{o}{=}\PYG{p}{[}
            \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{column\PYGZus{}name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{dt}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{column\PYGZus{}type}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{IntegerType}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{default\PYGZus{}value}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{20200201}\PYG{p}{\PYGZcb{}}
        \PYG{p}{]}\PYG{p}{,}
        \PYG{n}{repartition\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{,}
    \PYG{p}{)}
\PYG{p}{)}

\PYG{n}{friends\PYGZus{}pipeline}\PYG{o}{.}\PYG{n}{execute}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}


\subsubsection{Application Code for Updating Both, the Users and Friends\_Mapping Table, at once}
\label{\detokenize{examples:application-code-for-updating-both-the-users-and-friends-mapping-table-at-once}}
This script extracts and transforms the common activities for both tables as they share the same input data set.
Caching the dataframe avoids redundant processes and reloading when an action is executed (the load step f.e.).
This could have been written with pipeline objects as well (by providing the Pipeline an \sphinxcode{\sphinxupquote{input\_df}} and/or \sphinxcode{\sphinxupquote{output\_df}} to bypass
extractors and loaders) but would have led to unnecessary verbosity. This example should also show the flexibility of
Spooq2 for activities and steps which are not directly supported.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{spooq2}\PYG{n+nn}{.}\PYG{n+nn}{extractor} \PYG{k}{as} \PYG{n+nn}{E}
\PYG{k+kn}{import} \PYG{n+nn}{spooq2}\PYG{n+nn}{.}\PYG{n+nn}{transformer} \PYG{k}{as} \PYG{n+nn}{T}
\PYG{k+kn}{import} \PYG{n+nn}{spooq2}\PYG{n+nn}{.}\PYG{n+nn}{loader} \PYG{k}{as} \PYG{n+nn}{L}

\PYG{n}{mapping} \PYG{o}{=} \PYG{p}{[}
    \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{id}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}              \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{id}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}                     \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{IntegerType}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
    \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{guid}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{guid}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}                   \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{StringType}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
    \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{forename}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{attributes.first\PYGZus{}name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}  \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{StringType}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
    \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{surename}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{attributes.last\PYGZus{}name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}   \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{StringType}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
    \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{gender}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}          \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{attributes.gender}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}      \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{StringType}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
    \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{has\PYGZus{}email}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}       \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{attributes.email}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}       \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{StringBoolean}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
    \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{has\PYGZus{}university}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}  \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{attributes.university}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}  \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{StringBoolean}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
    \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{created\PYGZus{}at}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}      \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{meta.created\PYGZus{}at\PYGZus{}ms}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}     \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{timestamp\PYGZus{}ms\PYGZus{}to\PYGZus{}s}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
    \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{friends}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}         \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{attributes.friends}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}     \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{as\PYGZus{}is}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
\PYG{p}{]}

\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Transformations used by both output tables\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{n}{common\PYGZus{}df} \PYG{o}{=} \PYG{n}{E}\PYG{o}{.}\PYG{n}{JSONExtractor}\PYG{p}{(}\PYG{n}{input\PYGZus{}path}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tests/data/schema\PYGZus{}v1/sequenceFiles}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{extract}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{common\PYGZus{}df} \PYG{o}{=} \PYG{n}{T}\PYG{o}{.}\PYG{n}{Mapper}\PYG{p}{(}\PYG{n}{mapping}\PYG{o}{=}\PYG{n}{mapping}\PYG{p}{)}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{common\PYGZus{}df}\PYG{p}{)}
\PYG{n}{common\PYGZus{}df} \PYG{o}{=} \PYG{n}{T}\PYG{o}{.}\PYG{n}{ThresholdCleaner}\PYG{p}{(}
    \PYG{n}{range\PYGZus{}definitions}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{created\PYGZus{}at}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{min}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{max}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{1580737513}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{default}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{k+kc}{None}\PYG{p}{\PYGZcb{}}\PYG{p}{\PYGZcb{}}
\PYG{p}{)}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{common\PYGZus{}df}\PYG{p}{)}
\PYG{n}{common\PYGZus{}df} \PYG{o}{=} \PYG{n}{T}\PYG{o}{.}\PYG{n}{NewestByGroup}\PYG{p}{(}\PYG{n}{group\PYGZus{}by}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{id}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{order\PYGZus{}by}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{created\PYGZus{}at}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{common\PYGZus{}df}\PYG{p}{)}
\PYG{n}{common\PYGZus{}df}\PYG{o}{.}\PYG{n}{cache}\PYG{p}{(}\PYG{p}{)}

\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Transformations for users\PYGZus{}and\PYGZus{}friends table\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{n}{L}\PYG{o}{.}\PYG{n}{HiveLoader}\PYG{p}{(}
    \PYG{n}{db\PYGZus{}name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{users\PYGZus{}and\PYGZus{}friends}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
    \PYG{n}{table\PYGZus{}name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{users}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
    \PYG{n}{partition\PYGZus{}definitions}\PYG{o}{=}\PYG{p}{[}
        \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{column\PYGZus{}name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{dt}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{column\PYGZus{}type}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{IntegerType}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{default\PYGZus{}value}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{20200201}\PYG{p}{\PYGZcb{}}
    \PYG{p}{]}\PYG{p}{,}
    \PYG{n}{repartition\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,}
\PYG{p}{)}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{common\PYGZus{}df}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{friends}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{)}

\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Transformations for friends\PYGZus{}mapping table\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{n}{friends\PYGZus{}df} \PYG{o}{=} \PYG{n}{T}\PYG{o}{.}\PYG{n}{Exploder}\PYG{p}{(}\PYG{n}{path\PYGZus{}to\PYGZus{}array}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{friends}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{exploded\PYGZus{}elem\PYGZus{}name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{friend}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}
    \PYG{n}{common\PYGZus{}df}
\PYG{p}{)}
\PYG{n}{friends\PYGZus{}df} \PYG{o}{=} \PYG{n}{T}\PYG{o}{.}\PYG{n}{Mapper}\PYG{p}{(}
    \PYG{n}{mapping}\PYG{o}{=}\PYG{p}{[}
        \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{id}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}          \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{id}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}          \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{IntegerType}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
        \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{guid}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{guid}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{StringType}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
        \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{friend\PYGZus{}id}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}   \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{friend.id}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}   \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{IntegerType}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
        \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{created\PYGZus{}at}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}  \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{created\PYGZus{}at}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}  \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{IntegerType}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
    \PYG{p}{]}
\PYG{p}{)}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{friends\PYGZus{}df}\PYG{p}{)}
\PYG{n}{L}\PYG{o}{.}\PYG{n}{HiveLoader}\PYG{p}{(}
    \PYG{n}{db\PYGZus{}name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{users\PYGZus{}and\PYGZus{}friends}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
    \PYG{n}{table\PYGZus{}name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{friends\PYGZus{}mapping}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
    \PYG{n}{partition\PYGZus{}definitions}\PYG{o}{=}\PYG{p}{[}
        \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{column\PYGZus{}name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{dt}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{column\PYGZus{}type}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{IntegerType}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{default\PYGZus{}value}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{20200201}\PYG{p}{\PYGZcb{}}
    \PYG{p}{]}\PYG{p}{,}
    \PYG{n}{repartition\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{,}
\PYG{p}{)}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{friends\PYGZus{}df}\PYG{p}{)}
\end{sphinxVerbatim}


\section{Extractors}
\label{\detokenize{extractor/overview:module-spooq2.extractor.extractor}}\label{\detokenize{extractor/overview:extractors}}\label{\detokenize{extractor/overview::doc}}\index{spooq2.extractor.extractor (module)@\spxentry{spooq2.extractor.extractor}\spxextra{module}}
Extractors are used to fetch, extract and convert a source data set into a PySpark DataFrame.
Exemplary extraction sources are \sphinxstylestrong{JSON Files} on file systems like HDFS, DBFS or EXT4
and relational database systems via \sphinxstylestrong{JDBC}.


\subsection{JSON Files}
\label{\detokenize{extractor/json:module-spooq2.extractor.json_files}}\label{\detokenize{extractor/json:json-files}}\label{\detokenize{extractor/json::doc}}\index{spooq2.extractor.json\_files (module)@\spxentry{spooq2.extractor.json\_files}\spxextra{module}}\index{JSONExtractor (class in spooq2.extractor.json\_files)@\spxentry{JSONExtractor}\spxextra{class in spooq2.extractor.json\_files}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{extractor/json:spooq2.extractor.json_files.JSONExtractor}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{JSONExtractor}}}{\emph{input\_path=None}, \emph{base\_path=None}, \emph{partition=None}}{}
Bases: {\hyperref[\detokenize{base_classes/extractor:spooq2.extractor.extractor.Extractor}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{spooq2.extractor.extractor.Extractor}}}}}

The JSONExtractor class provides an API to extract data stored as JSON format,
deserializes it into a PySpark dataframe and returns it. Currently only
single-line JSON files are supported, stored either as textFile or sequenceFile.
\subsubsection*{Examples}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{spooq2} \PYG{k+kn}{import} \PYG{n}{extractor} \PYG{k}{as} \PYG{n}{E}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{extractor} \PYG{o}{=} \PYG{n}{E}\PYG{o}{.}\PYG{n}{JSONExtractor}\PYG{p}{(}\PYG{n}{input\PYGZus{}path}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tests/data/schema\PYGZus{}v1/sequenceFiles}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{extractor}\PYG{o}{.}\PYG{n}{input\PYGZus{}path} \PYG{o}{==} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tests/data/schema\PYGZus{}v1/sequenceFiles}\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{+} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/*}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{g+go}{True}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{extractor} \PYG{o}{=} \PYG{n}{E}\PYG{o}{.}\PYG{n}{JSONExtractor}\PYG{p}{(}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{base\PYGZus{}path}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tests/data/schema\PYGZus{}v1/sequenceFiles}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{partition}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{20200201}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{extractor}\PYG{o}{.}\PYG{n}{input\PYGZus{}path} \PYG{o}{==} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tests/data/schema\PYGZus{}v1/sequenceFiles}\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{+} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/20/02/01}\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{+} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/*}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{g+go}{True}
\end{sphinxVerbatim}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{input\_path}} (\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}) \textendash{} The path from which the JSON files should be loaded (“/*” will be added if omitted)

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{base\_path}} (\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}) \textendash{} Spooq tries to infer the \sphinxcode{\sphinxupquote{input\_path}} from the \sphinxcode{\sphinxupquote{base\_path}} and the \sphinxcode{\sphinxupquote{partition}} if the
\sphinxcode{\sphinxupquote{input\_path}} is missing.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{partition}} (\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}} or \sphinxhref{https://docs.python.org/3.7/library/functions.html\#int}{\sphinxcode{\sphinxupquote{int}}}) \textendash{} Spooq tries to infer the \sphinxcode{\sphinxupquote{input\_path}} from the \sphinxcode{\sphinxupquote{base\_path}} and the \sphinxcode{\sphinxupquote{partition}} if the
\sphinxcode{\sphinxupquote{input\_path}} is missing.
Only daily partitions in the form of “YYYYMMDD” are supported. e.g., “20200201” =\textgreater{} \textless{}base\_path\textgreater{} + “/20/02/01/*”

\end{itemize}

\item[{Returns}] \leavevmode
The extracted data set as a PySpark DataFrame

\item[{Return type}] \leavevmode
\sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.DataFrame}{\sphinxcode{\sphinxupquote{pyspark.sql.DataFrame}}}

\item[{Raises}] \leavevmode
\sphinxcode{\sphinxupquote{exceptions.AttributeError}} \textendash{} Please define either \sphinxcode{\sphinxupquote{input\_path}} or \sphinxcode{\sphinxupquote{base\_path}} and \sphinxcode{\sphinxupquote{partition}}

\end{description}\end{quote}

\begin{sphinxadmonition}{warning}{Warning:}
Currently only single-line JSON files stored as SequenceFiles or TextFiles are supported!
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Note:}
The init method checks which input parameters are provided and derives the final input\_path
from them accordingly.
\begin{description}
\item[{If \sphinxcode{\sphinxupquote{input\_path}} is not \sphinxhref{https://docs.python.org/3.7/library/constants.html\#None}{\sphinxcode{\sphinxupquote{None}}}:}] \leavevmode
Cleans \sphinxcode{\sphinxupquote{input\_path}} and returns it as the final \sphinxcode{\sphinxupquote{input\_path}}

\item[{Elif \sphinxcode{\sphinxupquote{base\_path}} and \sphinxcode{\sphinxupquote{partition}} are not \sphinxhref{https://docs.python.org/3.7/library/constants.html\#None}{\sphinxcode{\sphinxupquote{None}}}:}] \leavevmode
Cleans \sphinxcode{\sphinxupquote{base\_path}}, infers the sub path from the \sphinxcode{\sphinxupquote{partition}}
and returns the combined string as the final \sphinxcode{\sphinxupquote{input\_path}}

\item[{Else:}] \leavevmode
Raises an \sphinxcode{\sphinxupquote{exceptions.AttributeError}}

\end{description}
\end{sphinxadmonition}
\index{extract() (JSONExtractor method)@\spxentry{extract()}\spxextra{JSONExtractor method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{extractor/json:spooq2.extractor.json_files.JSONExtractor.extract}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{extract}}}{}{}
This is the Public API Method to be called for all classes of Extractors
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
Complex PySpark DataFrame deserialized from the input JSON Files

\item[{Return type}] \leavevmode
\sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.DataFrame}{\sphinxcode{\sphinxupquote{pyspark.sql.DataFrame}}}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\subsection{JDBC Source}
\label{\detokenize{extractor/jdbc:module-spooq2.extractor.jdbc}}\label{\detokenize{extractor/jdbc:jdbc-source}}\label{\detokenize{extractor/jdbc::doc}}\index{spooq2.extractor.jdbc (module)@\spxentry{spooq2.extractor.jdbc}\spxextra{module}}\index{JDBCExtractor (class in spooq2.extractor.jdbc)@\spxentry{JDBCExtractor}\spxextra{class in spooq2.extractor.jdbc}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{extractor/jdbc:spooq2.extractor.jdbc.JDBCExtractor}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{JDBCExtractor}}}{\emph{jdbc\_options}, \emph{cache=True}}{}
Bases: {\hyperref[\detokenize{base_classes/extractor:spooq2.extractor.extractor.Extractor}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{spooq2.extractor.extractor.Extractor}}}}}

\end{fulllineitems}

\index{JDBCExtractorFullLoad (class in spooq2.extractor.jdbc)@\spxentry{JDBCExtractorFullLoad}\spxextra{class in spooq2.extractor.jdbc}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{extractor/jdbc:spooq2.extractor.jdbc.JDBCExtractorFullLoad}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{JDBCExtractorFullLoad}}}{\emph{query}, \emph{jdbc\_options}, \emph{cache=True}}{}
Bases: {\hyperref[\detokenize{extractor/jdbc:spooq2.extractor.jdbc.JDBCExtractor}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{spooq2.extractor.jdbc.JDBCExtractor}}}}}

Connects to a JDBC Source and fetches the data defined by the provided Query.
\subsubsection*{Examples}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{import} \PYG{n+nn}{spooq2}\PYG{n+nn}{.}\PYG{n+nn}{extractor} \PYG{k}{as} \PYG{n+nn}{E}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{extractor} \PYG{o}{=} \PYG{n}{E}\PYG{o}{.}\PYG{n}{JDBCExtractorFullLoad}\PYG{p}{(}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{query}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{select id, first\PYGZus{}name, last\PYGZus{}name, gender, created\PYGZus{}at test\PYGZus{}db.from users}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{jdbc\PYGZus{}options}\PYG{o}{=}\PYG{p}{\PYGZob{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{url}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{jdbc:postgresql://localhost/test\PYGZus{}db}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{driver}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{org.postgresql.Driver}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{user}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{read\PYGZus{}only}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{password}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{test123}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{p}{\PYGZcb{}}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{p}{)}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{extracted\PYGZus{}df} \PYG{o}{=} \PYG{n}{extractor}\PYG{o}{.}\PYG{n}{extract}\PYG{p}{(}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{type}\PYG{p}{(}\PYG{n}{extracted\PYGZus{}df}\PYG{p}{)}
\PYG{g+go}{pyspark.sql.dataframe.DataFrame}
\end{sphinxVerbatim}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{query}} (\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}) \textendash{} Defines the actual query sent to the JDBC Source. This has to be a valid SQL query
with respect to the source system (e.g., T-SQL for Microsoft SQL Server).

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{jdbc\_options}} (\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#dict}{\sphinxcode{\sphinxupquote{dict}}}, optional) \textendash{} \begin{description}
\item[{A set of parameters to configure the connection to the source:}] \leavevmode\begin{itemize}
\item {} 
\sphinxstylestrong{url} (\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}) - A JDBC URL of the form \sphinxurl{jdbc:subprotocol:subname}.
e.g., \sphinxurl{jdbc:postgresql://localhost:5432/dbname}

\item {} 
\sphinxstylestrong{driver} (\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}) - The class name of the JDBC driver to use to connect to this URL.

\item {} 
\sphinxstylestrong{user} (\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}) - Username to authenticate with the source database.

\item {} 
\sphinxstylestrong{password} (\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}) - Password to authenticate with the source database.

\end{itemize}

\end{description}

See \sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.DataFrameReader.jdbc}{\sphinxcode{\sphinxupquote{pyspark.sql.DataFrameReader.jdbc()}}} and
\sphinxurl{https://spark.apache.org/docs/2.4.3/sql-data-sources-jdbc.html} for more information.


\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{cache}} (\sphinxhref{https://docs.python.org/3.7/library/functions.html\#bool}{\sphinxcode{\sphinxupquote{bool}}}, defaults to \sphinxhref{https://docs.python.org/3.7/library/constants.html\#True}{\sphinxcode{\sphinxupquote{True}}}) \textendash{} Defines, weather to \sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.DataFrame.cache}{\sphinxcode{\sphinxupquote{cache()}}} the dataframe, after it is loaded.
Otherwise the Extractor will reload all data from the source system eachtime an action is
performed on the DataFrame.

\end{itemize}

\item[{Raises}] \leavevmode
\sphinxcode{\sphinxupquote{exceptions.AssertionError}}: \textendash{} All jdbc\_options values need to be present as string variables.

\end{description}\end{quote}
\index{extract() (JDBCExtractorFullLoad method)@\spxentry{extract()}\spxextra{JDBCExtractorFullLoad method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{extractor/jdbc:spooq2.extractor.jdbc.JDBCExtractorFullLoad.extract}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{extract}}}{}{}
This is the Public API Method to be called for all classes of Extractors
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
PySpark dataframe from the input JDBC connection.

\item[{Return type}] \leavevmode
\sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.DataFrame}{\sphinxcode{\sphinxupquote{pyspark.sql.DataFrame}}}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{JDBCExtractorIncremental (class in spooq2.extractor.jdbc)@\spxentry{JDBCExtractorIncremental}\spxextra{class in spooq2.extractor.jdbc}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{extractor/jdbc:spooq2.extractor.jdbc.JDBCExtractorIncremental}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{JDBCExtractorIncremental}}}{\emph{partition}, \emph{jdbc\_options}, \emph{source\_table}, \emph{spooq2\_values\_table}, \emph{spooq2\_values\_db='spooq2\_values'}, \emph{spooq2\_values\_partition\_column='updated\_at'}, \emph{cache=True}}{}
Bases: {\hyperref[\detokenize{extractor/jdbc:spooq2.extractor.jdbc.JDBCExtractor}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{spooq2.extractor.jdbc.JDBCExtractor}}}}}

Connects to a JDBC Source and fetches the data with respect to boundaries.
The boundaries are inferred from the partition to load and logs from previous loads
stored in the \sphinxcode{\sphinxupquote{spooq2\_values\_table}}.
\subsubsection*{Examples}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{import} \PYG{n+nn}{spooq2}\PYG{n+nn}{.}\PYG{n+nn}{extractor} \PYG{k}{as} \PYG{n+nn}{E}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{c+c1}{\PYGZsh{} Boundaries derived from previously logged extractions =\PYGZgt{} (\PYGZdq{}2020\PYGZhy{}01\PYGZhy{}31 03:29:59\PYGZdq{}, False)}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{extractor} \PYG{o}{=} \PYG{n}{E}\PYG{o}{.}\PYG{n}{JDBCExtractorIncremental}\PYG{p}{(}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{partition}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{20200201}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{jdbc\PYGZus{}options}\PYG{o}{=}\PYG{p}{\PYGZob{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{url}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{jdbc:postgresql://localhost/test\PYGZus{}db}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{driver}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{org.postgresql.Driver}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{user}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{read\PYGZus{}only}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{password}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{test123}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{p}{\PYGZcb{}}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{source\PYGZus{}table}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{users}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{spooq2\PYGZus{}values\PYGZus{}table}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{spooq2\PYGZus{}jdbc\PYGZus{}log\PYGZus{}users}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{p}{)}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{extractor}\PYG{o}{.}\PYG{n}{\PYGZus{}construct\PYGZus{}query\PYGZus{}for\PYGZus{}partition}\PYG{p}{(}\PYG{n}{extractor}\PYG{o}{.}\PYG{n}{partition}\PYG{p}{)}
\PYG{g+go}{select * from users where updated\PYGZus{}at \PYGZgt{} \PYGZdq{}2020\PYGZhy{}01\PYGZhy{}31 03:29:59\PYGZdq{}}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{extracted\PYGZus{}df} \PYG{o}{=} \PYG{n}{extractor}\PYG{o}{.}\PYG{n}{extract}\PYG{p}{(}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{type}\PYG{p}{(}\PYG{n}{extracted\PYGZus{}df}\PYG{p}{)}
\PYG{g+go}{pyspark.sql.dataframe.DataFrame}
\end{sphinxVerbatim}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{partition}} (\sphinxhref{https://docs.python.org/3.7/library/functions.html\#int}{\sphinxcode{\sphinxupquote{int}}} or \sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}) \textendash{} Partition to extract. Needed for logging the incremental load in
the \sphinxcode{\sphinxupquote{spooq2\_values\_table}}.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{jdbc\_options}} (\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#dict}{\sphinxcode{\sphinxupquote{dict}}}, optional) \textendash{} \begin{description}
\item[{A set of parameters to configure the connection to the source:}] \leavevmode\begin{itemize}
\item {} 
\sphinxstylestrong{url} (\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}) - A JDBC URL of the form \sphinxurl{jdbc:subprotocol:subname}.
e.g., \sphinxurl{jdbc:postgresql://localhost:5432/dbname}

\item {} 
\sphinxstylestrong{driver} (\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}) - The class name of the JDBC driver to use to connect to this URL.

\item {} 
\sphinxstylestrong{user} (\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}) - Username to authenticate with the source database.

\item {} 
\sphinxstylestrong{password} (\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}) - Password to authenticate with the source database.

\end{itemize}

\end{description}

See \sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.DataFrameReader.jdbc}{\sphinxcode{\sphinxupquote{pyspark.sql.DataFrameReader.jdbc()}}} and
\sphinxurl{https://spark.apache.org/docs/2.4.3/sql-data-sources-jdbc.html} for more information.


\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{source\_table}} (\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}) \textendash{} Defines the tablename of the source to be loaded from. For example ‘purchases’.
This is necessary to build the query.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{spooq2\_values\_table}} (\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}) \textendash{} Defines the Hive table where previous and future loads of a specific source table
are logged. This is necessary to derive boundaries for the current partition.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{spooq2\_values\_db}} (\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}, optional) \textendash{} Defines the Database where the \sphinxcode{\sphinxupquote{spooq2\_values\_table}} is stored.
Defaults to \sphinxtitleref{‘spooq2\_values’}.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{spooq2\_values\_partition\_column}} (\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}, optional) \textendash{} The column name which is used for the boundaries.
Defaults to \sphinxtitleref{‘updated\_at’}.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{cache}} (\sphinxhref{https://docs.python.org/3.7/library/functions.html\#bool}{\sphinxcode{\sphinxupquote{bool}}}, defaults to \sphinxhref{https://docs.python.org/3.7/library/constants.html\#True}{\sphinxcode{\sphinxupquote{True}}}) \textendash{} Defines, weather to \sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.DataFrame.cache}{\sphinxcode{\sphinxupquote{cache()}}} the dataframe, after it is loaded. Otherwise the Extractor
will reload all data from the source system again, if a second action upon the dataframe
is performed.

\end{itemize}

\item[{Raises}] \leavevmode
\sphinxcode{\sphinxupquote{exceptions.AssertionError}}: \textendash{} All jdbc\_options values need to be present as string variables.

\end{description}\end{quote}
\index{extract() (JDBCExtractorIncremental method)@\spxentry{extract()}\spxextra{JDBCExtractorIncremental method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{extractor/jdbc:spooq2.extractor.jdbc.JDBCExtractorIncremental.extract}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{extract}}}{}{}
Extracts Data from a Source and converts it into a PySpark DataFrame.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode


\item[{Return type}] \leavevmode
\sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.DataFrame}{\sphinxcode{\sphinxupquote{pyspark.sql.DataFrame}}}

\end{description}\end{quote}

\begin{sphinxadmonition}{note}{Note:}
This method does not take ANY input parameters. All needed parameters are defined
in the initialization of the Extractor Object.
\end{sphinxadmonition}

\end{fulllineitems}


\end{fulllineitems}



\subsection{Class Diagram of Extractor Subpackage}
\label{\detokenize{extractor/overview:class-diagram-of-extractor-subpackage}}
\noindent\sphinxincludegraphics{{plantuml-74170d4486f5108da4925414c7df89c64a28d9b2}.png}


\subsection{Create your own Extractor}
\label{\detokenize{extractor/overview:create-your-own-extractor}}
Please see the {\hyperref[\detokenize{base_classes/extractor:custom-extractor}]{\sphinxcrossref{\DUrole{std,std-ref}{Create your own Extractor}}}} for further details.


\section{Transformers}
\label{\detokenize{transformer/overview:module-spooq2.transformer.transformer}}\label{\detokenize{transformer/overview:transformers}}\label{\detokenize{transformer/overview::doc}}\index{spooq2.transformer.transformer (module)@\spxentry{spooq2.transformer.transformer}\spxextra{module}}
Transformers take a \sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.DataFrame}{\sphinxcode{\sphinxupquote{pyspark.sql.DataFrame}}} as an input, transform it accordingly
and return a PySpark DataFrame.

Each Transformer class has to have a \sphinxtitleref{transform} method which takes no arguments
and returns a PySpark DataFrame.

Possible transformation methods can be \sphinxstylestrong{Selecting the most up to date record by id},
\sphinxstylestrong{Exploding an array}, \sphinxstylestrong{Filter (on an exploded array)}, \sphinxstylestrong{Apply basic threshold cleansing} or
\sphinxstylestrong{Map the incoming DataFrame to at provided structure}.


\subsection{Exploder}
\label{\detokenize{transformer/exploder:module-spooq2.transformer.exploder}}\label{\detokenize{transformer/exploder:exploder}}\label{\detokenize{transformer/exploder::doc}}\index{spooq2.transformer.exploder (module)@\spxentry{spooq2.transformer.exploder}\spxextra{module}}\index{Exploder (class in spooq2.transformer.exploder)@\spxentry{Exploder}\spxextra{class in spooq2.transformer.exploder}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{transformer/exploder:spooq2.transformer.exploder.Exploder}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{Exploder}}}{\emph{path\_to\_array='included'}, \emph{exploded\_elem\_name='elem'}}{}
Bases: {\hyperref[\detokenize{base_classes/transformer:spooq2.transformer.transformer.Transformer}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{spooq2.transformer.transformer.Transformer}}}}}

Explodes an array within a DataFrame and
drops the column containing the source array.
\subsubsection*{Examples}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{transformer} \PYG{o}{=} \PYG{n}{Exploder}\PYG{p}{(}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{path\PYGZus{}to\PYGZus{}array}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{attributes.friends}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{exploded\PYGZus{}elem\PYGZus{}name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{friend}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{p}{)}
\end{sphinxVerbatim}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{path\_to\_array}} (\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}, (Defaults to ‘included’)) \textendash{} Defines the Column Name / Path to the Array.
Dropping nested columns is not supported.
Although, you can still explode them.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{exploded\_elem\_name}} (\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}, (Defaults to ‘elem’)) \textendash{} Defines the column name the exploded column will get.
This is important to know how to access the Field afterwards.
Writing nested columns is not supported.
The output column has to be first level.

\end{itemize}

\end{description}\end{quote}

\begin{sphinxadmonition}{warning}{Warning:}
\sphinxstylestrong{Support for nested column:}
\begin{description}
\item[{path\_to\_array:}] \leavevmode
PySpark cannot drop a field within a struct. This means the specific field
can be referenced and therefore exploded, but not dropped.

\item[{exploded\_elem\_name:}] \leavevmode
If you (re)name a column in the dot notation, is creates a first level column,
just with a dot its name. To create a struct with the column as a field
you have to redefine the structure or use a UDF.

\end{description}
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Note:}
The \sphinxcode{\sphinxupquote{explode()}} method of Spark is used internally.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Note:}
The size of the resulting DataFrame is not guaranteed to be
equal to the Input DataFrame!
\end{sphinxadmonition}
\index{transform() (Exploder method)@\spxentry{transform()}\spxextra{Exploder method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{transformer/exploder:spooq2.transformer.exploder.Exploder.transform}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform}}}{\emph{input\_df}}{}
Performs a transformation on a DataFrame.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{input\_df}} (\sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.DataFrame}{\sphinxcode{\sphinxupquote{pyspark.sql.DataFrame}}}) \textendash{} Input DataFrame

\item[{Returns}] \leavevmode
Transformed DataFrame.

\item[{Return type}] \leavevmode
\sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.DataFrame}{\sphinxcode{\sphinxupquote{pyspark.sql.DataFrame}}}

\end{description}\end{quote}

\begin{sphinxadmonition}{note}{Note:}
This method does only take the Input DataFrame as a parameters. All other needed parameters
are defined in the initialization of the Transformator Object.
\end{sphinxadmonition}

\end{fulllineitems}


\end{fulllineitems}



\subsection{Sieve (Filter)}
\label{\detokenize{transformer/sieve:module-spooq2.transformer.sieve}}\label{\detokenize{transformer/sieve:sieve-filter}}\label{\detokenize{transformer/sieve::doc}}\index{spooq2.transformer.sieve (module)@\spxentry{spooq2.transformer.sieve}\spxextra{module}}\index{Sieve (class in spooq2.transformer.sieve)@\spxentry{Sieve}\spxextra{class in spooq2.transformer.sieve}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{transformer/sieve:spooq2.transformer.sieve.Sieve}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{Sieve}}}{\emph{filter\_expression}}{}
Bases: {\hyperref[\detokenize{base_classes/transformer:spooq2.transformer.transformer.Transformer}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{spooq2.transformer.transformer.Transformer}}}}}

Filters rows depending on provided filter expression.
Only records complying with filter condition are kept.
\subsubsection*{Examples}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{transformer} \PYG{o}{=} \PYG{n}{T}\PYG{o}{.}\PYG{n}{Sieve}\PYG{p}{(}\PYG{n}{filter\PYGZus{}expression}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{l+s+s2}{ attributes.last\PYGZus{}name rlike }\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZca{}.}\PYG{l+s+si}{\PYGZob{}7\PYGZcb{}}\PYG{l+s+s2}{\PYGZdl{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ }\PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{transformer} \PYG{o}{=} \PYG{n}{T}\PYG{o}{.}\PYG{n}{Sieve}\PYG{p}{(}\PYG{n}{filter\PYGZus{}expression}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{l+s+s2}{ lower(gender) = }\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ }\PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{filter\_expression}} (\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}) \textendash{} A valid PySpark SQL expression which returns a boolean

\item[{Raises}] \leavevmode
\sphinxcode{\sphinxupquote{exceptions.ValueError}} \textendash{} filter\_expression has to be a valid (Spark)SQL expression provided as a string

\end{description}\end{quote}

\begin{sphinxadmonition}{note}{Note:}
The \sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.DataFrame.filter}{\sphinxcode{\sphinxupquote{filter()}}} method is used internally.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Note:}
The Size of the resulting DataFrame is not guaranteed to be equal to the Input DataFrame!
\end{sphinxadmonition}
\index{transform() (Sieve method)@\spxentry{transform()}\spxextra{Sieve method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{transformer/sieve:spooq2.transformer.sieve.Sieve.transform}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform}}}{\emph{input\_df}}{}
Performs a transformation on a DataFrame.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{input\_df}} (\sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.DataFrame}{\sphinxcode{\sphinxupquote{pyspark.sql.DataFrame}}}) \textendash{} Input DataFrame

\item[{Returns}] \leavevmode
Transformed DataFrame.

\item[{Return type}] \leavevmode
\sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.DataFrame}{\sphinxcode{\sphinxupquote{pyspark.sql.DataFrame}}}

\end{description}\end{quote}

\begin{sphinxadmonition}{note}{Note:}
This method does only take the Input DataFrame as a parameters. All other needed parameters
are defined in the initialization of the Transformator Object.
\end{sphinxadmonition}

\end{fulllineitems}


\end{fulllineitems}



\subsection{Mapper}
\label{\detokenize{transformer/mapper:mapper}}\label{\detokenize{transformer/mapper::doc}}

\subsubsection{Class}
\label{\detokenize{transformer/mapper:module-spooq2.transformer.mapper}}\label{\detokenize{transformer/mapper:class}}\index{spooq2.transformer.mapper (module)@\spxentry{spooq2.transformer.mapper}\spxextra{module}}\index{Mapper (class in spooq2.transformer.mapper)@\spxentry{Mapper}\spxextra{class in spooq2.transformer.mapper}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{transformer/mapper:spooq2.transformer.mapper.Mapper}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{Mapper}}}{\emph{mapping}, \emph{ignore\_missing\_columns=True}}{}
Bases: {\hyperref[\detokenize{base_classes/transformer:spooq2.transformer.transformer.Transformer}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{spooq2.transformer.transformer.Transformer}}}}}

Constructs and applies a PySpark SQL expression, based on the provided mapping.
\subsubsection*{Examples}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{mapping} \PYG{o}{=} \PYG{p}{[}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}              \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{data.relationships.food.data.id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}     \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{StringType}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{message\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}      \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{data.id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}                             \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{StringType}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{type}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}            \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{data.relationships.food.data.type}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}   \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{StringType}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{created\PYGZus{}at}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}      \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{elem.attributes.created\PYGZus{}at}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}          \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{timestamp\PYGZus{}ms\PYGZus{}to\PYGZus{}s}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{updated\PYGZus{}at}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}      \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{elem.attributes.updated\PYGZus{}at}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}          \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{timestamp\PYGZus{}ms\PYGZus{}to\PYGZus{}s}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{deleted\PYGZus{}at}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}      \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{elem.attributes.deleted\PYGZus{}at}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}          \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{timestamp\PYGZus{}ms\PYGZus{}to\PYGZus{}s}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{brand}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}           \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{elem.attributes.brand}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}               \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{StringType}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{transformer} \PYG{o}{=} \PYG{n}{Mapper}\PYG{p}{(}\PYG{n}{mapping}\PYG{o}{=}\PYG{n}{mapping}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{mapping} \PYG{o}{=} \PYG{p}{[}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}          \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{data.relationships.food.data.id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}   \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{StringType}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{updated\PYGZus{}at}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}  \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{elem.attributes.updated\PYGZus{}at}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}        \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{timestamp\PYGZus{}ms\PYGZus{}to\PYGZus{}s}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{deleted\PYGZus{}at}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}  \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{elem.attributes.deleted\PYGZus{}at}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}        \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{timestamp\PYGZus{}ms\PYGZus{}to\PYGZus{}s}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}        \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{elem.attributes.name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}              \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{array}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{transformer} \PYG{o}{=} \PYG{n}{Mapper}\PYG{p}{(}\PYG{n}{mapping}\PYG{o}{=}\PYG{n}{mapping}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{mapping}} (\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#list}{\sphinxcode{\sphinxupquote{list}}} of \sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#tuple}{\sphinxcode{\sphinxupquote{tuple}}} containing three \sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}) \textendash{} This is the main parameter for this transformation. It essentially gives information
about the column names for the output DataFrame, the column names (paths)
from the input DataFrame, and their data types. Custom data types are also supported, which can
clean, pivot, anonymize, … the data itself. Please have a look at the
{\hyperref[\detokenize{transformer/mapper:module-spooq2.transformer.mapper_custom_data_types}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{spooq2.transformer.mapper\_custom\_data\_types}}}}} module for more information.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{ignore\_missing\_columns}} (\sphinxhref{https://docs.python.org/3.7/library/functions.html\#bool}{\sphinxcode{\sphinxupquote{bool}}}, Defaults to True) \textendash{} Specifies if the mapping transformation should raise an exception if a referenced input
column is missing in the provided DataFrame.

\end{itemize}

\end{description}\end{quote}

\begin{sphinxadmonition}{note}{Note:}
Let’s talk about Mappings:

The mapping should be a list of tuples which are containing all information per column.
\begin{itemize}
\item {} \begin{description}
\item[{Column Name}] \leavevmode{[}\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}{]}
Sets the name of the column in the resulting output DataFrame.

\end{description}

\item {} \begin{description}
\item[{Source Path / Name}] \leavevmode{[}\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}{]}
Points to the name of the column in the input DataFrame. If the input
is a flat DataFrame, it will essentially be the column name. If it is of complex
type, it will point to the path of the actual value. For example:
\sphinxcode{\sphinxupquote{data.relationships.sample.data.id}}, where id is the value we want.

\end{description}

\item {} \begin{description}
\item[{DataType}] \leavevmode{[}\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}{]}
DataTypes can be types from \sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#module-pyspark.sql.types}{\sphinxcode{\sphinxupquote{pyspark.sql.types}}}, selected custom datatypes or
injected, ad-hoc custom datatypes.
The datatype will be interpreted as a PySpark built-in if it is a member of \sphinxcode{\sphinxupquote{pyspark.sql.types}}.
If it is not an importable PySpark data type, a method to construct the statement will be
called by the data type’s name.

\end{description}

\end{itemize}
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Note:}
The available input columns can vary from batch to batch if you use schema inference
(f.e. on json data) for the extraction. Ignoring missing columns on the input DataFrame is
highly encouraged in this case. Although, if you have tight control over the structure
of the extracted DataFrame, setting \sphinxtitleref{ignore\_missing\_columns} to True is advised
as it can uncover typos and bugs.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Note:}
Please see {\hyperref[\detokenize{transformer/mapper:module-spooq2.transformer.mapper_custom_data_types}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{spooq2.transformer.mapper\_custom\_data\_types}}}}} for all available custom
data types and how to inject your own.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Note:}
Attention: Decimal is NOT SUPPORTED by Hive! Please use Double instead!
\end{sphinxadmonition}
\index{transform() (Mapper method)@\spxentry{transform()}\spxextra{Mapper method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{transformer/mapper:spooq2.transformer.mapper.Mapper.transform}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform}}}{\emph{input\_df}}{}
Performs a transformation on a DataFrame.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{input\_df}} (\sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.DataFrame}{\sphinxcode{\sphinxupquote{pyspark.sql.DataFrame}}}) \textendash{} Input DataFrame

\item[{Returns}] \leavevmode
Transformed DataFrame.

\item[{Return type}] \leavevmode
\sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.DataFrame}{\sphinxcode{\sphinxupquote{pyspark.sql.DataFrame}}}

\end{description}\end{quote}

\begin{sphinxadmonition}{note}{Note:}
This method does only take the Input DataFrame as a parameters. All other needed parameters
are defined in the initialization of the Transformator Object.
\end{sphinxadmonition}

\end{fulllineitems}


\end{fulllineitems}



\subsubsection{Activity Diagram}
\label{\detokenize{transformer/mapper:activity-diagram}}
\noindent\sphinxincludegraphics{{plantuml-95d88ab79bc74cc16a47d960c784615eb24258cf}.png}


\subsubsection{Custom Mapping Methods}
\label{\detokenize{transformer/mapper:module-spooq2.transformer.mapper_custom_data_types}}\label{\detokenize{transformer/mapper:custom-mapping-methods}}\index{spooq2.transformer.mapper\_custom\_data\_types (module)@\spxentry{spooq2.transformer.mapper\_custom\_data\_types}\spxextra{module}}
This is a collection of module level methods to construct a specific
PySpark DataFrame query for custom defined data types.

These methods are not meant to be called directly but via the
the {\hyperref[\detokenize{transformer/mapper:spooq2.transformer.mapper.Mapper}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{Mapper}}}}} transformer.
Please see that particular class on how to apply custom data types.

For injecting your \sphinxstylestrong{own custom data types}, please have a visit to the
{\hyperref[\detokenize{transformer/mapper:spooq2.transformer.mapper_custom_data_types.add_custom_data_type}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{add\_custom\_data\_type()}}}}} method!
\index{add\_custom\_data\_type() (in module spooq2.transformer.mapper\_custom\_data\_types)@\spxentry{add\_custom\_data\_type()}\spxextra{in module spooq2.transformer.mapper\_custom\_data\_types}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{transformer/mapper:spooq2.transformer.mapper_custom_data_types.add_custom_data_type}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{add\_custom\_data\_type}}}{\emph{function\_name}, \emph{func}}{}
Registers a custom data type in runtime to be used with the {\hyperref[\detokenize{transformer/mapper:spooq2.transformer.mapper.Mapper}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{Mapper}}}}} transformer.
\subsubsection*{Example}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{import} \PYG{n+nn}{spooq2}\PYG{n+nn}{.}\PYG{n+nn}{transformer}\PYG{n+nn}{.}\PYG{n+nn}{mapper\PYGZus{}custom\PYGZus{}data\PYGZus{}types} \PYG{k}{as} \PYG{n+nn}{custom\PYGZus{}types}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{import} \PYG{n+nn}{spooq2}\PYG{n+nn}{.}\PYG{n+nn}{transformer} \PYG{k}{as} \PYG{n+nn}{T}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{pyspark}\PYG{n+nn}{.}\PYG{n+nn}{sql} \PYG{k+kn}{import} \PYG{n}{Row}\PYG{p}{,} \PYG{n}{functions} \PYG{k}{as} \PYG{n}{F}\PYG{p}{,} \PYG{n}{types} \PYG{k}{as} \PYG{n}{sql\PYGZus{}types}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k}{def} \PYG{n+nf}{hello\PYGZus{}world}\PYG{p}{(}\PYG{n}{source\PYGZus{}column}\PYG{p}{,} \PYG{n}{name}\PYG{p}{)}\PYG{p}{:}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{A UDF (User Defined Function) in Python}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{k}{def} \PYG{n+nf}{\PYGZus{}to\PYGZus{}hello\PYGZus{}world}\PYG{p}{(}\PYG{n}{col}\PYG{p}{)}\PYG{p}{:}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }        \PYG{k}{if} \PYG{o+ow}{not} \PYG{n}{col}\PYG{p}{:}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }            \PYG{k}{return} \PYG{k+kc}{None}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }        \PYG{k}{else}\PYG{p}{:}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }            \PYG{k}{return} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Hello World}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{udf\PYGZus{}hello\PYGZus{}world} \PYG{o}{=} \PYG{n}{F}\PYG{o}{.}\PYG{n}{udf}\PYG{p}{(}\PYG{n}{\PYGZus{}to\PYGZus{}hello\PYGZus{}world}\PYG{p}{,} \PYG{n}{sql\PYGZus{}types}\PYG{o}{.}\PYG{n}{StringType}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{k}{return} \PYG{n}{udf\PYGZus{}hello\PYGZus{}world}\PYG{p}{(}\PYG{n}{source\PYGZus{}column}\PYG{p}{)}\PYG{o}{.}\PYG{n}{alias}\PYG{p}{(}\PYG{n}{name}\PYG{p}{)}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{input\PYGZus{}df} \PYG{o}{=} \PYG{n}{spark}\PYG{o}{.}\PYG{n}{createDataFrame}\PYG{p}{(}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{p}{[}\PYG{n}{Row}\PYG{p}{(}\PYG{n}{hello\PYGZus{}from}\PYG{o}{=}\PYG{l+s+sa}{u}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tsivorn1@who.int}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }     \PYG{n}{Row}\PYG{p}{(}\PYG{n}{hello\PYGZus{}from}\PYG{o}{=}\PYG{l+s+sa}{u}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }     \PYG{n}{Row}\PYG{p}{(}\PYG{n}{hello\PYGZus{}from}\PYG{o}{=}\PYG{l+s+sa}{u}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gisaksen4@skype.com}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{p}{)}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{custom\PYGZus{}types}\PYG{o}{.}\PYG{n}{add\PYGZus{}custom\PYGZus{}data\PYGZus{}type}\PYG{p}{(}\PYG{n}{function\PYGZus{}name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{hello\PYGZus{}world}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{func}\PYG{o}{=}\PYG{n}{hello\PYGZus{}world}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{transformer} \PYG{o}{=} \PYG{n}{T}\PYG{o}{.}\PYG{n}{Mapper}\PYG{p}{(}\PYG{n}{mapping}\PYG{o}{=}\PYG{p}{[}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{hello\PYGZus{}who}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{hello\PYGZus{}from}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{hello\PYGZus{}world}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{df} \PYG{o}{=} \PYG{n}{transformer}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{input\PYGZus{}df}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{df}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\PYG{g+go}{+\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+}
\PYG{g+go}{\textbar{}  hello\PYGZus{}who\textbar{}}
\PYG{g+go}{+\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+}
\PYG{g+go}{\textbar{}Hello World\textbar{}}
\PYG{g+go}{\textbar{}       null\textbar{}}
\PYG{g+go}{\textbar{}Hello World\textbar{}}
\PYG{g+go}{+\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k}{def} \PYG{n+nf}{first\PYGZus{}and\PYGZus{}last\PYGZus{}name}\PYG{p}{(}\PYG{n}{source\PYGZus{}column}\PYG{p}{,} \PYG{n}{name}\PYG{p}{)}\PYG{p}{:}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{A PySpark SQL expression referencing multiple columns}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{k}{return} \PYG{n}{F}\PYG{o}{.}\PYG{n}{concat\PYGZus{}ws}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZus{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{source\PYGZus{}column}\PYG{p}{,} \PYG{n}{F}\PYG{o}{.}\PYG{n}{col}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{attributes.last\PYGZus{}name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{alias}\PYG{p}{(}\PYG{n}{name}\PYG{p}{)}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{custom\PYGZus{}types}\PYG{o}{.}\PYG{n}{add\PYGZus{}custom\PYGZus{}data\PYGZus{}type}\PYG{p}{(}\PYG{n}{function\PYGZus{}name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{full\PYGZus{}name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{func}\PYG{o}{=}\PYG{n}{first\PYGZus{}and\PYGZus{}last\PYGZus{}name}\PYG{p}{)}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{transformer} \PYG{o}{=} \PYG{n}{T}\PYG{o}{.}\PYG{n}{Mapper}\PYG{p}{(}\PYG{n}{mapping}\PYG{o}{=}\PYG{p}{[}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{first\PYGZus{}name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{attributes.first\PYGZus{}name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{StringType}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{last\PYGZus{}name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}  \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{attributes.last\PYGZus{}name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}  \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{StringType}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{full\PYGZus{}name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}  \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{attributes.first\PYGZus{}name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{full\PYGZus{}name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{function\_name}} (\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}) \textendash{} The name of your custom data type

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{func}} (\sphinxstyleliteralemphasis{\sphinxupquote{compatible function}}) \textendash{} The PySpark dataframe function which will be called on a column, defined in the mapping
of the Mapper class.
Required input parameters are \sphinxcode{\sphinxupquote{source\_column}} and \sphinxcode{\sphinxupquote{name}}.
Please see the note about required input parameter of custom data types for more information!

\end{itemize}

\end{description}\end{quote}

\begin{sphinxadmonition}{note}{Note:}
Required input parameter of custom data types:
\begin{description}
\item[{\sphinxstylestrong{source\_column} (\sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.Column}{\sphinxcode{\sphinxupquote{pyspark.sql.Column}}}) - This is where your logic will be applied.}] \leavevmode
The mapper transformer takes care of calling this method with the right column so you can just
handle it like an object which you would get from \sphinxcode{\sphinxupquote{df{[}"some\_attribute"{]}}}.

\item[{\sphinxstylestrong{name} (\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}) - The name how the resulting column will be named. Nested attributes are not}] \leavevmode
supported. The Mapper transformer takes care of calling this method with the right column name.

\end{description}
\end{sphinxadmonition}

\end{fulllineitems}

\index{\_get\_select\_expression\_for\_custom\_type() (in module spooq2.transformer.mapper\_custom\_data\_types)@\spxentry{\_get\_select\_expression\_for\_custom\_type()}\spxextra{in module spooq2.transformer.mapper\_custom\_data\_types}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{transformer/mapper:spooq2.transformer.mapper_custom_data_types._get_select_expression_for_custom_type}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_get\_select\_expression\_for\_custom\_type}}}{\emph{source\_column}, \emph{name}, \emph{data\_type}}{}
Internal method for calling functions dynamically

\end{fulllineitems}

\index{\_generate\_select\_expression\_for\_as\_is() (in module spooq2.transformer.mapper\_custom\_data\_types)@\spxentry{\_generate\_select\_expression\_for\_as\_is()}\spxextra{in module spooq2.transformer.mapper\_custom\_data\_types}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{transformer/mapper:spooq2.transformer.mapper_custom_data_types._generate_select_expression_for_as_is}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_generate\_select\_expression\_for\_as\_is}}}{\emph{source\_column}, \emph{name}}{}
alias for \sphinxtitleref{\_generate\_select\_expression\_without\_casting}

\end{fulllineitems}

\index{\_generate\_select\_expression\_for\_keep() (in module spooq2.transformer.mapper\_custom\_data\_types)@\spxentry{\_generate\_select\_expression\_for\_keep()}\spxextra{in module spooq2.transformer.mapper\_custom\_data\_types}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{transformer/mapper:spooq2.transformer.mapper_custom_data_types._generate_select_expression_for_keep}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_generate\_select\_expression\_for\_keep}}}{\emph{source\_column}, \emph{name}}{}
alias for \sphinxtitleref{\_generate\_select\_expression\_without\_casting}

\end{fulllineitems}

\index{\_generate\_select\_expression\_for\_no\_change() (in module spooq2.transformer.mapper\_custom\_data\_types)@\spxentry{\_generate\_select\_expression\_for\_no\_change()}\spxextra{in module spooq2.transformer.mapper\_custom\_data\_types}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{transformer/mapper:spooq2.transformer.mapper_custom_data_types._generate_select_expression_for_no_change}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_generate\_select\_expression\_for\_no\_change}}}{\emph{source\_column}, \emph{name}}{}
alias for \sphinxtitleref{\_generate\_select\_expression\_without\_casting}

\end{fulllineitems}

\index{\_generate\_select\_expression\_without\_casting() (in module spooq2.transformer.mapper\_custom\_data\_types)@\spxentry{\_generate\_select\_expression\_without\_casting()}\spxextra{in module spooq2.transformer.mapper\_custom\_data\_types}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{transformer/mapper:spooq2.transformer.mapper_custom_data_types._generate_select_expression_without_casting}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_generate\_select\_expression\_without\_casting}}}{\emph{source\_column}, \emph{name}}{}
Returns a column without casting. This is especially useful if you need to
keep a complex data type, like an array, list or a struct.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{spooq2}\PYG{n+nn}{.}\PYG{n+nn}{transformer} \PYG{k+kn}{import} \PYG{n}{Mapper}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{input\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{g+go}{[Row(friends=[Row(first\PYGZus{}name=None, id=3993, last\PYGZus{}name=None), Row(first\PYGZus{}name=u\PYGZsq{}Ruò\PYGZsq{}, id=17484, last\PYGZus{}name=u\PYGZsq{}Trank\PYGZsq{})]),}
\PYG{g+go}{ Row(friends=[]),}
\PYG{g+go}{ Row(friends=[Row(first\PYGZus{}name=u\PYGZsq{}Daphnée\PYGZsq{}, id=16707, last\PYGZus{}name=u\PYGZsq{}Lyddiard\PYGZsq{}), Row(first\PYGZus{}name=u\PYGZsq{}Adélaïde\PYGZsq{}, id=17429, last\PYGZus{}name=u\PYGZsq{}Wisdom\PYGZsq{})])]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{mapping} \PYG{o}{=} \PYG{p}{[}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{my\PYGZus{}friends}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{friends}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{as\PYGZus{}is}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{output\PYGZus{}df} \PYG{o}{=} \PYG{n}{Mapper}\PYG{p}{(}\PYG{n}{mapping}\PYG{p}{)}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{input\PYGZus{}df}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{output\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{g+go}{[Row(my\PYGZus{}friends=[Row(first\PYGZus{}name=None, id=3993, last\PYGZus{}name=None), Row(first\PYGZus{}name=u\PYGZsq{}Ruò\PYGZsq{}, id=17484, last\PYGZus{}name=u\PYGZsq{}Trank\PYGZsq{})]),}
\PYG{g+go}{ Row(my\PYGZus{}friends=[]),}
\PYG{g+go}{ Row(my\PYGZus{}friends=[Row(first\PYGZus{}name=u\PYGZsq{}Daphnée\PYGZsq{}, id=16707, last\PYGZus{}name=u\PYGZsq{}Lyddiard\PYGZsq{}), Row(first\PYGZus{}name=u\PYGZsq{}Adélaïde\PYGZsq{}, id=17429, last\PYGZus{}name=u\PYGZsq{}Wisdom\PYGZsq{})])]}
\end{sphinxVerbatim}

\end{fulllineitems}

\index{\_generate\_select\_expression\_for\_json\_string() (in module spooq2.transformer.mapper\_custom\_data\_types)@\spxentry{\_generate\_select\_expression\_for\_json\_string()}\spxextra{in module spooq2.transformer.mapper\_custom\_data\_types}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{transformer/mapper:spooq2.transformer.mapper_custom_data_types._generate_select_expression_for_json_string}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_generate\_select\_expression\_for\_json\_string}}}{\emph{source\_column}, \emph{name}}{}
Returns a column as json compatible string.
Nested hierarchies are supported.
The unicode representation of a column will be returned if an error occurs.
\subsubsection*{Example}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{spooq2}\PYG{n+nn}{.}\PYG{n+nn}{transformer} \PYG{k+kn}{import} \PYG{n}{Mapper}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{input\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{g+go}{[Row(friends=[Row(first\PYGZus{}name=None, id=3993, last\PYGZus{}name=None), Row(first\PYGZus{}name=u\PYGZsq{}Ruò\PYGZsq{}, id=17484, last\PYGZus{}name=u\PYGZsq{}Trank\PYGZsq{})]),}
\PYG{g+go}{ Row(friends=[]),}
\PYG{g+go}{ Row(friends=[Row(first\PYGZus{}name=u\PYGZsq{}Daphnée\PYGZsq{}, id=16707, last\PYGZus{}name=u\PYGZsq{}Lyddiard\PYGZsq{}), Row(first\PYGZus{}name=u\PYGZsq{}Adélaïde\PYGZsq{}, id=17429, last\PYGZus{}name=u\PYGZsq{}Wisdom\PYGZsq{})])]    \PYGZgt{}\PYGZgt{}\PYGZgt{} mapping = [(\PYGZdq{}friends\PYGZus{}json\PYGZdq{}, \PYGZdq{}friends\PYGZdq{}, \PYGZdq{}json\PYGZus{}string\PYGZdq{})]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{mapping} \PYG{o}{=} \PYG{p}{[}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{friends\PYGZus{}json}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{friends}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{json\PYGZus{}string}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{output\PYGZus{}df} \PYG{o}{=} \PYG{n}{Mapper}\PYG{p}{(}\PYG{n}{mapping}\PYG{p}{)}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{input\PYGZus{}df}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{output\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{g+go}{[Row(friends\PYGZus{}json=u\PYGZsq{}[\PYGZob{}\PYGZdq{}first\PYGZus{}name\PYGZdq{}: null, \PYGZdq{}last\PYGZus{}name\PYGZdq{}: null, \PYGZdq{}id\PYGZdq{}: 3993\PYGZcb{}, \PYGZob{}\PYGZdq{}first\PYGZus{}name\PYGZdq{}: \PYGZdq{}Ru\PYGZbs{}u00f2\PYGZdq{}, \PYGZdq{}last\PYGZus{}name\PYGZdq{}: \PYGZdq{}Trank\PYGZdq{}, \PYGZdq{}id\PYGZdq{}: 17484\PYGZcb{}]\PYGZsq{}),}
\PYG{g+go}{ Row(friends\PYGZus{}json=None),}
\PYG{g+go}{ Row(friends\PYGZus{}json=u\PYGZsq{}[\PYGZob{}\PYGZdq{}first\PYGZus{}name\PYGZdq{}: \PYGZdq{}Daphn\PYGZbs{}u00e9e\PYGZdq{}, \PYGZdq{}last\PYGZus{}name\PYGZdq{}: \PYGZdq{}Lyddiard\PYGZdq{}, \PYGZdq{}id\PYGZdq{}: 16707\PYGZcb{}, \PYGZob{}\PYGZdq{}first\PYGZus{}name\PYGZdq{}: \PYGZdq{}Ad\PYGZbs{}u00e9la\PYGZbs{}u00efde\PYGZdq{}, \PYGZdq{}last\PYGZus{}name\PYGZdq{}: \PYGZdq{}Wisdom\PYGZdq{}, \PYGZdq{}id\PYGZdq{}: 17429\PYGZcb{}]\PYGZsq{})]}
\end{sphinxVerbatim}

\end{fulllineitems}

\index{\_generate\_select\_expression\_for\_timestamp\_ms\_to\_ms() (in module spooq2.transformer.mapper\_custom\_data\_types)@\spxentry{\_generate\_select\_expression\_for\_timestamp\_ms\_to\_ms()}\spxextra{in module spooq2.transformer.mapper\_custom\_data\_types}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{transformer/mapper:spooq2.transformer.mapper_custom_data_types._generate_select_expression_for_timestamp_ms_to_ms}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_generate\_select\_expression\_for\_timestamp\_ms\_to\_ms}}}{\emph{source\_column}, \emph{name}}{}
This Constructor is used for unix timestamps. The values are cleaned
next to casting and renaming.
If the values are not between \sphinxtitleref{01.01.1970} and \sphinxtitleref{31.12.2099},
NULL will be returned.
Cast to \sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.types.LongType}{\sphinxcode{\sphinxupquote{pyspark.sql.types.LongType}}}
\subsubsection*{Example}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{pyspark}\PYG{n+nn}{.}\PYG{n+nn}{sql} \PYG{k+kn}{import} \PYG{n}{Row}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{spooq2}\PYG{n+nn}{.}\PYG{n+nn}{transformer} \PYG{k+kn}{import} \PYG{n}{Mapper}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{input\PYGZus{}df} \PYG{o}{=} \PYG{n}{spark}\PYG{o}{.}\PYG{n}{createDataFrame}\PYG{p}{(}\PYG{p}{[}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{Row}\PYG{p}{(}\PYG{n}{time\PYGZus{}sec}\PYG{o}{=}\PYG{l+m+mi}{1581540839000}\PYG{p}{)}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} 02/12/2020 @ 8:53pm (UTC)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{Row}\PYG{p}{(}\PYG{n}{time\PYGZus{}sec}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{4887839000}\PYG{p}{)}\PYG{p}{,}    \PYG{c+c1}{\PYGZsh{} Invalid!}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{Row}\PYG{p}{(}\PYG{n}{time\PYGZus{}sec}\PYG{o}{=}\PYG{l+m+mi}{4737139200000}\PYG{p}{)}   \PYG{c+c1}{\PYGZsh{} 02/12/2120 @ 12:00am (UTC)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{p}{]}\PYG{p}{)}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{mapping} \PYG{o}{=} \PYG{p}{[}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{unix\PYGZus{}ts}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{time\PYGZus{}sec}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{timestamp\PYGZus{}ms\PYGZus{}to\PYGZus{}ms}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{output\PYGZus{}df} \PYG{o}{=} \PYG{n}{Mapper}\PYG{p}{(}\PYG{n}{mapping}\PYG{p}{)}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{input\PYGZus{}df}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{output\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{g+go}{[Row(unix\PYGZus{}ts=1581540839000), Row(unix\PYGZus{}ts=None), Row(unix\PYGZus{}ts=None)]}
\end{sphinxVerbatim}

\begin{sphinxadmonition}{note}{Note:}
\sphinxstyleemphasis{input}  in \sphinxstylestrong{milli seconds}
\sphinxstyleemphasis{output} in \sphinxstylestrong{milli seconds}
\end{sphinxadmonition}

\end{fulllineitems}

\index{\_generate\_select\_expression\_for\_timestamp\_ms\_to\_s() (in module spooq2.transformer.mapper\_custom\_data\_types)@\spxentry{\_generate\_select\_expression\_for\_timestamp\_ms\_to\_s()}\spxextra{in module spooq2.transformer.mapper\_custom\_data\_types}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{transformer/mapper:spooq2.transformer.mapper_custom_data_types._generate_select_expression_for_timestamp_ms_to_s}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_generate\_select\_expression\_for\_timestamp\_ms\_to\_s}}}{\emph{source\_column}, \emph{name}}{}
This Constructor is used for unix timestamps. The values are cleaned
next to casting and renaming.
If the values are not between \sphinxtitleref{01.01.1970} and \sphinxtitleref{31.12.2099},
NULL will be returned.
Cast to \sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.types.LongType}{\sphinxcode{\sphinxupquote{pyspark.sql.types.LongType}}}
\subsubsection*{Example}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{pyspark}\PYG{n+nn}{.}\PYG{n+nn}{sql} \PYG{k+kn}{import} \PYG{n}{Row}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{spooq2}\PYG{n+nn}{.}\PYG{n+nn}{transformer} \PYG{k+kn}{import} \PYG{n}{Mapper}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{input\PYGZus{}df} \PYG{o}{=} \PYG{n}{spark}\PYG{o}{.}\PYG{n}{createDataFrame}\PYG{p}{(}\PYG{p}{[}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{Row}\PYG{p}{(}\PYG{n}{time\PYGZus{}sec}\PYG{o}{=}\PYG{l+m+mi}{1581540839000}\PYG{p}{)}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} 02/12/2020 @ 8:53pm (UTC)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{Row}\PYG{p}{(}\PYG{n}{time\PYGZus{}sec}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{4887839000}\PYG{p}{)}\PYG{p}{,}    \PYG{c+c1}{\PYGZsh{} Invalid!}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{Row}\PYG{p}{(}\PYG{n}{time\PYGZus{}sec}\PYG{o}{=}\PYG{l+m+mi}{4737139200000}\PYG{p}{)}   \PYG{c+c1}{\PYGZsh{} 02/12/2120 @ 12:00am (UTC)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{p}{]}\PYG{p}{)}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{mapping} \PYG{o}{=} \PYG{p}{[}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{unix\PYGZus{}ts}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{time\PYGZus{}sec}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{timestamp\PYGZus{}ms\PYGZus{}to\PYGZus{}s}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{output\PYGZus{}df} \PYG{o}{=} \PYG{n}{Mapper}\PYG{p}{(}\PYG{n}{mapping}\PYG{p}{)}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{input\PYGZus{}df}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{output\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{g+go}{[Row(unix\PYGZus{}ts=1581540839), Row(unix\PYGZus{}ts=None), Row(unix\PYGZus{}ts=None)]}
\end{sphinxVerbatim}

\begin{sphinxadmonition}{note}{Note:}
\sphinxstyleemphasis{input}  in \sphinxstylestrong{milli seconds}
\sphinxstyleemphasis{output} in \sphinxstylestrong{seconds}
\end{sphinxadmonition}

\end{fulllineitems}

\index{\_generate\_select\_expression\_for\_timestamp\_s\_to\_ms() (in module spooq2.transformer.mapper\_custom\_data\_types)@\spxentry{\_generate\_select\_expression\_for\_timestamp\_s\_to\_ms()}\spxextra{in module spooq2.transformer.mapper\_custom\_data\_types}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{transformer/mapper:spooq2.transformer.mapper_custom_data_types._generate_select_expression_for_timestamp_s_to_ms}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_generate\_select\_expression\_for\_timestamp\_s\_to\_ms}}}{\emph{source\_column}, \emph{name}}{}
This Constructor is used for unix timestamps. The values are cleaned
next to casting and renaming.
If the values are not between \sphinxtitleref{01.01.1970} and \sphinxtitleref{31.12.2099},
NULL will be returned.
Cast to \sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.types.LongType}{\sphinxcode{\sphinxupquote{pyspark.sql.types.LongType}}}
\subsubsection*{Example}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{pyspark}\PYG{n+nn}{.}\PYG{n+nn}{sql} \PYG{k+kn}{import} \PYG{n}{Row}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{spooq2}\PYG{n+nn}{.}\PYG{n+nn}{transformer} \PYG{k+kn}{import} \PYG{n}{Mapper}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{input\PYGZus{}df} \PYG{o}{=} \PYG{n}{spark}\PYG{o}{.}\PYG{n}{createDataFrame}\PYG{p}{(}\PYG{p}{[}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{Row}\PYG{p}{(}\PYG{n}{time\PYGZus{}sec}\PYG{o}{=}\PYG{l+m+mi}{1581540839}\PYG{p}{)}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} 02/12/2020 @ 8:53pm (UTC)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{Row}\PYG{p}{(}\PYG{n}{time\PYGZus{}sec}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{4887839}\PYG{p}{)}\PYG{p}{,}    \PYG{c+c1}{\PYGZsh{} Invalid!}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{Row}\PYG{p}{(}\PYG{n}{time\PYGZus{}sec}\PYG{o}{=}\PYG{l+m+mi}{4737139200}\PYG{p}{)}   \PYG{c+c1}{\PYGZsh{} 02/12/2120 @ 12:00am (UTC)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{p}{]}\PYG{p}{)}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{mapping} \PYG{o}{=} \PYG{p}{[}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{unix\PYGZus{}ts}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{time\PYGZus{}sec}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{timestamp\PYGZus{}s\PYGZus{}to\PYGZus{}ms}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{output\PYGZus{}df} \PYG{o}{=} \PYG{n}{Mapper}\PYG{p}{(}\PYG{n}{mapping}\PYG{p}{)}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{input\PYGZus{}df}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{output\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{g+go}{[Row(unix\PYGZus{}ts=1581540839000), Row(unix\PYGZus{}ts=None), Row(unix\PYGZus{}ts=None)]}
\end{sphinxVerbatim}

\begin{sphinxadmonition}{note}{Note:}
\sphinxstyleemphasis{input}  in \sphinxstylestrong{seconds}
\sphinxstyleemphasis{output} in \sphinxstylestrong{milli seconds}
\end{sphinxadmonition}

\end{fulllineitems}

\index{\_generate\_select\_expression\_for\_timestamp\_s\_to\_s() (in module spooq2.transformer.mapper\_custom\_data\_types)@\spxentry{\_generate\_select\_expression\_for\_timestamp\_s\_to\_s()}\spxextra{in module spooq2.transformer.mapper\_custom\_data\_types}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{transformer/mapper:spooq2.transformer.mapper_custom_data_types._generate_select_expression_for_timestamp_s_to_s}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_generate\_select\_expression\_for\_timestamp\_s\_to\_s}}}{\emph{source\_column}, \emph{name}}{}
This Constructor is used for unix timestamps. The values are cleaned
next to casting and renaming.
If the values are not between \sphinxtitleref{01.01.1970} and \sphinxtitleref{31.12.2099},
NULL will be returned.
Cast to \sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.types.LongType}{\sphinxcode{\sphinxupquote{pyspark.sql.types.LongType}}}
\subsubsection*{Example}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{pyspark}\PYG{n+nn}{.}\PYG{n+nn}{sql} \PYG{k+kn}{import} \PYG{n}{Row}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{spooq2}\PYG{n+nn}{.}\PYG{n+nn}{transformer} \PYG{k+kn}{import} \PYG{n}{Mapper}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{input\PYGZus{}df} \PYG{o}{=} \PYG{n}{spark}\PYG{o}{.}\PYG{n}{createDataFrame}\PYG{p}{(}\PYG{p}{[}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{Row}\PYG{p}{(}\PYG{n}{time\PYGZus{}sec}\PYG{o}{=}\PYG{l+m+mi}{1581540839}\PYG{p}{)}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} 02/12/2020 @ 8:53pm (UTC)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{Row}\PYG{p}{(}\PYG{n}{time\PYGZus{}sec}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{4887839}\PYG{p}{)}\PYG{p}{,}    \PYG{c+c1}{\PYGZsh{} Invalid!}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{Row}\PYG{p}{(}\PYG{n}{time\PYGZus{}sec}\PYG{o}{=}\PYG{l+m+mi}{4737139200}\PYG{p}{)}   \PYG{c+c1}{\PYGZsh{} 02/12/2120 @ 12:00am (UTC)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{p}{]}\PYG{p}{)}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{mapping} \PYG{o}{=} \PYG{p}{[}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{unix\PYGZus{}ts}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{time\PYGZus{}sec}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{timestamp\PYGZus{}s\PYGZus{}to\PYGZus{}ms}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{output\PYGZus{}df} \PYG{o}{=} \PYG{n}{Mapper}\PYG{p}{(}\PYG{n}{mapping}\PYG{p}{)}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{input\PYGZus{}df}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{output\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{g+go}{[Row(unix\PYGZus{}ts=1581540839), Row(unix\PYGZus{}ts=None), Row(unix\PYGZus{}ts=None)]}
\end{sphinxVerbatim}

\begin{sphinxadmonition}{note}{Note:}
\sphinxstyleemphasis{input}  in \sphinxstylestrong{seconds}
\sphinxstyleemphasis{output} in \sphinxstylestrong{seconds}
\end{sphinxadmonition}

\end{fulllineitems}

\index{\_generate\_select\_expression\_for\_StringNull() (in module spooq2.transformer.mapper\_custom\_data\_types)@\spxentry{\_generate\_select\_expression\_for\_StringNull()}\spxextra{in module spooq2.transformer.mapper\_custom\_data\_types}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{transformer/mapper:spooq2.transformer.mapper_custom_data_types._generate_select_expression_for_StringNull}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_generate\_select\_expression\_for\_StringNull}}}{\emph{source\_column}, \emph{name}}{}
Used for Anonymizing.
Input values will be ignored and replaced by NULL,
Cast to \sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.types.StringType}{\sphinxcode{\sphinxupquote{pyspark.sql.types.StringType}}}
\subsubsection*{Example}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{pyspark}\PYG{n+nn}{.}\PYG{n+nn}{sql} \PYG{k+kn}{import} \PYG{n}{Row}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{spooq2}\PYG{n+nn}{.}\PYG{n+nn}{transformer} \PYG{k+kn}{import} \PYG{n}{Mapper}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{input\PYGZus{}df} \PYG{o}{=} \PYG{n}{spark}\PYG{o}{.}\PYG{n}{createDataFrame}\PYG{p}{(}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{p}{[}\PYG{n}{Row}\PYG{p}{(}\PYG{n}{email}\PYG{o}{=}\PYG{l+s+sa}{u}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tsivorn1@who.int}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }     \PYG{n}{Row}\PYG{p}{(}\PYG{n}{email}\PYG{o}{=}\PYG{l+s+sa}{u}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }     \PYG{n}{Row}\PYG{p}{(}\PYG{n}{email}\PYG{o}{=}\PYG{l+s+sa}{u}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gisaksen4@skype.com}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{p}{)}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{mapping} \PYG{o}{=} \PYG{p}{[}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{email}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{email}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{StringNull}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{output\PYGZus{}df} \PYG{o}{=} \PYG{n}{Mapper}\PYG{p}{(}\PYG{n}{mapping}\PYG{p}{)}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{input\PYGZus{}df}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{output\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{g+go}{[Row(email=None), Row(email=None), Row(email=None)]}
\end{sphinxVerbatim}

\end{fulllineitems}

\index{\_generate\_select\_expression\_for\_IntNull() (in module spooq2.transformer.mapper\_custom\_data\_types)@\spxentry{\_generate\_select\_expression\_for\_IntNull()}\spxextra{in module spooq2.transformer.mapper\_custom\_data\_types}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{transformer/mapper:spooq2.transformer.mapper_custom_data_types._generate_select_expression_for_IntNull}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_generate\_select\_expression\_for\_IntNull}}}{\emph{source\_column}, \emph{name}}{}
Used for Anonymizing.
Input values will be ignored and replaced by NULL,
Cast to \sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.types.IntegerType}{\sphinxcode{\sphinxupquote{pyspark.sql.types.IntegerType}}}
\subsubsection*{Example}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{pyspark}\PYG{n+nn}{.}\PYG{n+nn}{sql} \PYG{k+kn}{import} \PYG{n}{Row}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{spooq2}\PYG{n+nn}{.}\PYG{n+nn}{transformer} \PYG{k+kn}{import} \PYG{n}{Mapper}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{input\PYGZus{}df} \PYG{o}{=} \PYG{n}{spark}\PYG{o}{.}\PYG{n}{createDataFrame}\PYG{p}{(}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{p}{[}\PYG{n}{Row}\PYG{p}{(}\PYG{n}{facebook\PYGZus{}id}\PYG{o}{=}\PYG{l+m+mi}{3047288}\PYG{p}{)}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }     \PYG{n}{Row}\PYG{p}{(}\PYG{n}{facebook\PYGZus{}id}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }     \PYG{n}{Row}\PYG{p}{(}\PYG{n}{facebook\PYGZus{}id}\PYG{o}{=}\PYG{l+m+mi}{57815}\PYG{p}{)}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{p}{)}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{mapping} \PYG{o}{=} \PYG{p}{[}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{facebook\PYGZus{}id}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{facebook\PYGZus{}id}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{IntNull}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{output\PYGZus{}df} \PYG{o}{=} \PYG{n}{Mapper}\PYG{p}{(}\PYG{n}{mapping}\PYG{p}{)}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{input\PYGZus{}df}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{output\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{g+go}{[Row(facebook\PYGZus{}id=None), Row(facebook\PYGZus{}id=None), Row(facebook\PYGZus{}id=None)]}
\end{sphinxVerbatim}

\end{fulllineitems}

\index{\_generate\_select\_expression\_for\_StringBoolean() (in module spooq2.transformer.mapper\_custom\_data\_types)@\spxentry{\_generate\_select\_expression\_for\_StringBoolean()}\spxextra{in module spooq2.transformer.mapper\_custom\_data\_types}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{transformer/mapper:spooq2.transformer.mapper_custom_data_types._generate_select_expression_for_StringBoolean}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_generate\_select\_expression\_for\_StringBoolean}}}{\emph{source\_column}, \emph{name}}{}
Used for Anonymizing.
The column’s value will be replaced by \sphinxtitleref{“1”} if it is:
\begin{itemize}
\item {} 
not NULL and

\item {} 
not an empty string

\end{itemize}
\subsubsection*{Example}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{pyspark}\PYG{n+nn}{.}\PYG{n+nn}{sql} \PYG{k+kn}{import} \PYG{n}{Row}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{spooq2}\PYG{n+nn}{.}\PYG{n+nn}{transformer} \PYG{k+kn}{import} \PYG{n}{Mapper}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{input\PYGZus{}df} \PYG{o}{=} \PYG{n}{spark}\PYG{o}{.}\PYG{n}{createDataFrame}\PYG{p}{(}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{p}{[}\PYG{n}{Row}\PYG{p}{(}\PYG{n}{email}\PYG{o}{=}\PYG{l+s+sa}{u}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tsivorn1@who.int}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }     \PYG{n}{Row}\PYG{p}{(}\PYG{n}{email}\PYG{o}{=}\PYG{l+s+sa}{u}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }     \PYG{n}{Row}\PYG{p}{(}\PYG{n}{email}\PYG{o}{=}\PYG{l+s+sa}{u}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gisaksen4@skype.com}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{p}{)}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{mapping} \PYG{o}{=} \PYG{p}{[}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{email}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{email}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{StringBoolean}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{output\PYGZus{}df} \PYG{o}{=} \PYG{n}{Mapper}\PYG{p}{(}\PYG{n}{mapping}\PYG{p}{)}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{input\PYGZus{}df}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{output\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{g+go}{[Row(email=u\PYGZsq{}1\PYGZsq{}), Row(email=None), Row(email=u\PYGZsq{}1\PYGZsq{})]}
\end{sphinxVerbatim}

\end{fulllineitems}

\index{\_generate\_select\_expression\_for\_IntBoolean() (in module spooq2.transformer.mapper\_custom\_data\_types)@\spxentry{\_generate\_select\_expression\_for\_IntBoolean()}\spxextra{in module spooq2.transformer.mapper\_custom\_data\_types}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{transformer/mapper:spooq2.transformer.mapper_custom_data_types._generate_select_expression_for_IntBoolean}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_generate\_select\_expression\_for\_IntBoolean}}}{\emph{source\_column}, \emph{name}}{}
Used for Anonymizing.
The column’s value will be replaced by \sphinxtitleref{1} if it contains a non-NULL value.
\subsubsection*{Example}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{pyspark}\PYG{n+nn}{.}\PYG{n+nn}{sql} \PYG{k+kn}{import} \PYG{n}{Row}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{spooq2}\PYG{n+nn}{.}\PYG{n+nn}{transformer} \PYG{k+kn}{import} \PYG{n}{Mapper}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{input\PYGZus{}df} \PYG{o}{=} \PYG{n}{spark}\PYG{o}{.}\PYG{n}{createDataFrame}\PYG{p}{(}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{p}{[}\PYG{n}{Row}\PYG{p}{(}\PYG{n}{facebook\PYGZus{}id}\PYG{o}{=}\PYG{l+m+mi}{3047288}\PYG{p}{)}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }     \PYG{n}{Row}\PYG{p}{(}\PYG{n}{facebook\PYGZus{}id}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }     \PYG{n}{Row}\PYG{p}{(}\PYG{n}{facebook\PYGZus{}id}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{p}{)}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{mapping} \PYG{o}{=} \PYG{p}{[}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{facebook\PYGZus{}id}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{facebook\PYGZus{}id}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{IntBoolean}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{output\PYGZus{}df} \PYG{o}{=} \PYG{n}{Mapper}\PYG{p}{(}\PYG{n}{mapping}\PYG{p}{)}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{input\PYGZus{}df}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{output\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{g+go}{[Row(facebook\PYGZus{}id=1), Row(facebook\PYGZus{}id=1), Row(facebook\PYGZus{}id=None)]}
\end{sphinxVerbatim}

\begin{sphinxadmonition}{note}{Note:}
\sphinxtitleref{0} (zero) or negative numbers are still considered as valid values and therefore converted to \sphinxtitleref{1}.
\end{sphinxadmonition}

\end{fulllineitems}

\index{\_generate\_select\_expression\_for\_TimestampMonth() (in module spooq2.transformer.mapper\_custom\_data\_types)@\spxentry{\_generate\_select\_expression\_for\_TimestampMonth()}\spxextra{in module spooq2.transformer.mapper\_custom\_data\_types}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{transformer/mapper:spooq2.transformer.mapper_custom_data_types._generate_select_expression_for_TimestampMonth}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_generate\_select\_expression\_for\_TimestampMonth}}}{\emph{source\_column}, \emph{name}}{}
Used for Anonymizing. Can be used to keep the age but obscure the explicit birthday.
This custom datatype requires a \sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.types.TimestampType}{\sphinxcode{\sphinxupquote{pyspark.sql.types.TimestampType}}} column as input.
The datetime value will be set to the first day of the month.
\subsubsection*{Example}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{pyspark}\PYG{n+nn}{.}\PYG{n+nn}{sql} \PYG{k+kn}{import} \PYG{n}{Row}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{datetime} \PYG{k+kn}{import} \PYG{n}{datetime}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{spooq2}\PYG{n+nn}{.}\PYG{n+nn}{transformer} \PYG{k+kn}{import} \PYG{n}{Mapper}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{input\PYGZus{}df} \PYG{o}{=} \PYG{n}{spark}\PYG{o}{.}\PYG{n}{createDataFrame}\PYG{p}{(}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{p}{[}\PYG{n}{Row}\PYG{p}{(}\PYG{n}{birthday}\PYG{o}{=}\PYG{n}{datetime}\PYG{p}{(}\PYG{l+m+mi}{2019}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{9}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{45}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }     \PYG{n}{Row}\PYG{p}{(}\PYG{n}{birthday}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }     \PYG{n}{Row}\PYG{p}{(}\PYG{n}{birthday}\PYG{o}{=}\PYG{n}{datetime}\PYG{p}{(}\PYG{l+m+mi}{1988}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{31}\PYG{p}{,} \PYG{l+m+mi}{8}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{p}{)}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{mapping} \PYG{o}{=} \PYG{p}{[}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{birthday}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{birthday}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{TimestampMonth}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{output\PYGZus{}df} \PYG{o}{=} \PYG{n}{Mapper}\PYG{p}{(}\PYG{n}{mapping}\PYG{p}{)}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{input\PYGZus{}df}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{output\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{g+go}{[Row(birthday=datetime.datetime(2019, 2, 1, 0, 0)),}
\PYG{g+go}{ Row(birthday=None),}
\PYG{g+go}{ Row(birthday=datetime.datetime(1988, 1, 1, 0, 0))]}
\end{sphinxVerbatim}

\end{fulllineitems}

\index{\_generate\_select\_expression\_for\_meters\_to\_cm() (in module spooq2.transformer.mapper\_custom\_data\_types)@\spxentry{\_generate\_select\_expression\_for\_meters\_to\_cm()}\spxextra{in module spooq2.transformer.mapper\_custom\_data\_types}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{transformer/mapper:spooq2.transformer.mapper_custom_data_types._generate_select_expression_for_meters_to_cm}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_generate\_select\_expression\_for\_meters\_to\_cm}}}{\emph{source\_column}, \emph{name}}{}
Convert meters to cm and cast the result to an IntegerType.
\subsubsection*{Example}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{pyspark}\PYG{n+nn}{.}\PYG{n+nn}{sql} \PYG{k+kn}{import} \PYG{n}{Row}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{spooq2}\PYG{n+nn}{.}\PYG{n+nn}{transformer} \PYG{k+kn}{import} \PYG{n}{Mapper}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{input\PYGZus{}df} \PYG{o}{=} \PYG{n}{spark}\PYG{o}{.}\PYG{n}{createDataFrame}\PYG{p}{(}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{p}{[}\PYG{n}{Row}\PYG{p}{(}\PYG{n}{size\PYGZus{}in\PYGZus{}m}\PYG{o}{=}\PYG{l+m+mf}{1.80}\PYG{p}{)}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }     \PYG{n}{Row}\PYG{p}{(}\PYG{n}{size\PYGZus{}in\PYGZus{}m}\PYG{o}{=}\PYG{l+m+mf}{1.65}\PYG{p}{)}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }     \PYG{n}{Row}\PYG{p}{(}\PYG{n}{size\PYGZus{}in\PYGZus{}m}\PYG{o}{=}\PYG{l+m+mf}{2.05}\PYG{p}{)}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{p}{)}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{mapping} \PYG{o}{=} \PYG{p}{[}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{size\PYGZus{}in\PYGZus{}cm}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{size\PYGZus{}in\PYGZus{}m}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{meters\PYGZus{}to\PYGZus{}cm}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{output\PYGZus{}df} \PYG{o}{=} \PYG{n}{Mapper}\PYG{p}{(}\PYG{n}{mapping}\PYG{p}{)}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{input\PYGZus{}df}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{output\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{g+go}{[Row(size\PYGZus{}in\PYGZus{}cm=180),}
\PYG{g+go}{ Row(size\PYGZus{}in\PYGZus{}cm=165),}
\PYG{g+go}{ Row(size\PYGZus{}in\PYGZus{}cm=205)]}
\end{sphinxVerbatim}

\end{fulllineitems}

\index{\_generate\_select\_expression\_for\_unix\_timestamp\_ms\_to\_spark\_timestamp() (in module spooq2.transformer.mapper\_custom\_data\_types)@\spxentry{\_generate\_select\_expression\_for\_unix\_timestamp\_ms\_to\_spark\_timestamp()}\spxextra{in module spooq2.transformer.mapper\_custom\_data\_types}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{transformer/mapper:spooq2.transformer.mapper_custom_data_types._generate_select_expression_for_unix_timestamp_ms_to_spark_timestamp}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_generate\_select\_expression\_for\_unix\_timestamp\_ms\_to\_spark\_timestamp}}}{\emph{source\_column}, \emph{name}}{}
Convert unix timestamps in milliseconds to a Spark TimeStampType. It is assumed that the
timezone is already set to UTC in spark / java to avoid implicit timezone conversions.
\subsubsection*{Example}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{pyspark}\PYG{n+nn}{.}\PYG{n+nn}{sql} \PYG{k+kn}{import} \PYG{n}{Row}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{spooq2}\PYG{n+nn}{.}\PYG{n+nn}{transformer} \PYG{k+kn}{import} \PYG{n}{Mapper}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{input\PYGZus{}df} \PYG{o}{=} \PYG{n}{spark}\PYG{o}{.}\PYG{n}{createDataFrame}\PYG{p}{(}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{p}{[}\PYG{n}{Row}\PYG{p}{(}\PYG{n}{unix\PYGZus{}timestamp\PYGZus{}in\PYGZus{}ms}\PYG{o}{=}\PYG{l+m+mi}{1591627696951}\PYG{p}{)}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }     \PYG{n}{Row}\PYG{p}{(}\PYG{n}{unix\PYGZus{}timestamp\PYGZus{}in\PYGZus{}ms}\PYG{o}{=}\PYG{l+m+mi}{1596812952000}\PYG{p}{)}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }     \PYG{n}{Row}\PYG{p}{(}\PYG{n}{unix\PYGZus{}timestamp\PYGZus{}in\PYGZus{}ms}\PYG{o}{=}\PYG{l+m+mi}{946672200000}\PYG{p}{)}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{p}{)}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{mapping} \PYG{o}{=} \PYG{p}{[}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{spark\PYGZus{}timestamp}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{unix\PYGZus{}timestamp\PYGZus{}in\PYGZus{}ms}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{unix\PYGZus{}timestamp\PYGZus{}ms\PYGZus{}to\PYGZus{}spark\PYGZus{}timestamp}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{output\PYGZus{}df} \PYG{o}{=} \PYG{n}{Mapper}\PYG{p}{(}\PYG{n}{mapping}\PYG{p}{)}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{input\PYGZus{}df}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{output\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{g+go}{[Row(spark\PYGZus{}timestamp=datetime.datetime(2020, 6, 8, 16, 48, 16, 951000)),}
\PYG{g+go}{ Row(spark\PYGZus{}timestamp=datetime.datetime(2020, 8, 7, 17, 9, 12)),}
\PYG{g+go}{ Row(spark\PYGZus{}timestamp=datetime.datetime(1999, 12, 31, 21, 30))]}
\end{sphinxVerbatim}

\end{fulllineitems}

\index{\_generate\_select\_expression\_for\_extended\_string\_to\_int() (in module spooq2.transformer.mapper\_custom\_data\_types)@\spxentry{\_generate\_select\_expression\_for\_extended\_string\_to\_int()}\spxextra{in module spooq2.transformer.mapper\_custom\_data\_types}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{transformer/mapper:spooq2.transformer.mapper_custom_data_types._generate_select_expression_for_extended_string_to_int}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_generate\_select\_expression\_for\_extended\_string\_to\_int}}}{\emph{source\_column}, \emph{name}}{}
More robust conversion from StringType to IntegerType.
Is able to additionally handle (compared to implicit Spark conversion):
\begin{itemize}
\item {} 
Preceding whitespace

\item {} 
Trailing whitespace

\item {} 
Preceeding and trailing whitespace

\item {} 
underscores as thousand separators

\end{itemize}

\begin{sphinxadmonition}{hint}{Hint:}
Please have a look at the tests to get a better feeling how it behaves under
tests/unit/transformer/test\_mapper\_custom\_data\_types.py::TestConversionsFromString
\end{sphinxadmonition}
\subsubsection*{Example}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{spooq2}\PYG{n+nn}{.}\PYG{n+nn}{transformer} \PYG{k+kn}{import} \PYG{n}{Mapper}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{input\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{g+go}{[Row(input\PYGZus{}string=\PYGZdq{}  123456 \PYGZdq{}),}
\PYG{g+go}{ Row(input\PYGZus{}string=\PYGZdq{}Hello\PYGZdq{}),}
\PYG{g+go}{ Row(input\PYGZus{}string=\PYGZdq{}123\PYGZus{}456\PYGZdq{})]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{mapping} \PYG{o}{=} \PYG{p}{[}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{output\PYGZus{}value}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{input\PYGZus{}string}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{extended\PYGZus{}string\PYGZus{}to\PYGZus{}int}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{output\PYGZus{}df} \PYG{o}{=} \PYG{n}{Mapper}\PYG{p}{(}\PYG{n}{mapping}\PYG{p}{)}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{input\PYGZus{}df}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{output\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{g+go}{[Row(input\PYGZus{}string=123456),}
\PYG{g+go}{ Row(input\PYGZus{}string=None),}
\PYG{g+go}{ Row(input\PYGZus{}string=123456)]}
\end{sphinxVerbatim}

\end{fulllineitems}

\index{\_generate\_select\_expression\_for\_extended\_string\_to\_long() (in module spooq2.transformer.mapper\_custom\_data\_types)@\spxentry{\_generate\_select\_expression\_for\_extended\_string\_to\_long()}\spxextra{in module spooq2.transformer.mapper\_custom\_data\_types}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{transformer/mapper:spooq2.transformer.mapper_custom_data_types._generate_select_expression_for_extended_string_to_long}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_generate\_select\_expression\_for\_extended\_string\_to\_long}}}{\emph{source\_column}, \emph{name}}{}
More robust conversion from StringType to LongType.
Is able to additionally handle (compared to implicit Spark conversion):
\begin{itemize}
\item {} 
Preceding whitespace

\item {} 
Trailing whitespace

\item {} 
Preceeding and trailing whitespace

\item {} 
underscores as thousand separators

\end{itemize}

\begin{sphinxadmonition}{hint}{Hint:}
Please have a look at the tests to get a better feeling how it behaves under
tests/unit/transformer/test\_mapper\_custom\_data\_types.py::TestConversionsFromString
\end{sphinxadmonition}
\subsubsection*{Example}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{spooq2}\PYG{n+nn}{.}\PYG{n+nn}{transformer} \PYG{k+kn}{import} \PYG{n}{Mapper}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{input\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{g+go}{[Row(input\PYGZus{}string=\PYGZdq{}  21474836470 \PYGZdq{}),}
\PYG{g+go}{ Row(input\PYGZus{}string=\PYGZdq{}Hello\PYGZdq{}),}
\PYG{g+go}{ Row(input\PYGZus{}string=\PYGZdq{}21\PYGZus{}474\PYGZus{}836\PYGZus{}470\PYGZdq{})]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{mapping} \PYG{o}{=} \PYG{p}{[}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{output\PYGZus{}value}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{input\PYGZus{}string}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{extended\PYGZus{}string\PYGZus{}to\PYGZus{}long}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{output\PYGZus{}df} \PYG{o}{=} \PYG{n}{Mapper}\PYG{p}{(}\PYG{n}{mapping}\PYG{p}{)}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{input\PYGZus{}df}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{output\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{g+go}{[Row(input\PYGZus{}string=21474836470),}
\PYG{g+go}{ Row(input\PYGZus{}string=None),}
\PYG{g+go}{ Row(input\PYGZus{}string=21474836470)]}
\end{sphinxVerbatim}

\end{fulllineitems}

\index{\_generate\_select\_expression\_for\_extended\_string\_to\_float() (in module spooq2.transformer.mapper\_custom\_data\_types)@\spxentry{\_generate\_select\_expression\_for\_extended\_string\_to\_float()}\spxextra{in module spooq2.transformer.mapper\_custom\_data\_types}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{transformer/mapper:spooq2.transformer.mapper_custom_data_types._generate_select_expression_for_extended_string_to_float}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_generate\_select\_expression\_for\_extended\_string\_to\_float}}}{\emph{source\_column}, \emph{name}}{}
More robust conversion from StringType to FloatType.
Is able to additionally handle (compared to implicit Spark conversion):
\begin{itemize}
\item {} 
Preceding whitespace

\item {} 
Trailing whitespace

\item {} 
Preceeding and trailing whitespace

\item {} 
underscores as thousand separators

\end{itemize}

\begin{sphinxadmonition}{hint}{Hint:}
Please have a look at the tests to get a better feeling how it behaves under
tests/unit/transformer/test\_mapper\_custom\_data\_types.py::TestConversionsFromString
\end{sphinxadmonition}
\subsubsection*{Example}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{spooq2}\PYG{n+nn}{.}\PYG{n+nn}{transformer} \PYG{k+kn}{import} \PYG{n}{Mapper}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{input\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{g+go}{[Row(input\PYGZus{}string=\PYGZdq{}  836470.819 \PYGZdq{}),}
\PYG{g+go}{ Row(input\PYGZus{}string=\PYGZdq{}Hello\PYGZdq{}),}
\PYG{g+go}{ Row(input\PYGZus{}string=\PYGZdq{}836\PYGZus{}470.819\PYGZdq{})]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{mapping} \PYG{o}{=} \PYG{p}{[}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{output\PYGZus{}value}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{input\PYGZus{}string}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{extended\PYGZus{}string\PYGZus{}to\PYGZus{}float}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{output\PYGZus{}df} \PYG{o}{=} \PYG{n}{Mapper}\PYG{p}{(}\PYG{n}{mapping}\PYG{p}{)}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{input\PYGZus{}df}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{output\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{g+go}{[Row(input\PYGZus{}string=836470.819),}
\PYG{g+go}{ Row(input\PYGZus{}string=None),}
\PYG{g+go}{ Row(input\PYGZus{}string=836470.819)]}
\end{sphinxVerbatim}

\end{fulllineitems}

\index{\_generate\_select\_expression\_for\_extended\_string\_to\_double() (in module spooq2.transformer.mapper\_custom\_data\_types)@\spxentry{\_generate\_select\_expression\_for\_extended\_string\_to\_double()}\spxextra{in module spooq2.transformer.mapper\_custom\_data\_types}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{transformer/mapper:spooq2.transformer.mapper_custom_data_types._generate_select_expression_for_extended_string_to_double}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_generate\_select\_expression\_for\_extended\_string\_to\_double}}}{\emph{source\_column}, \emph{name}}{}
More robust conversion from StringType to DoubleType.
Is able to additionally handle (compared to implicit Spark conversion):
\begin{itemize}
\item {} 
Preceding whitespace

\item {} 
Trailing whitespace

\item {} 
Preceeding and trailing whitespace

\item {} 
underscores as thousand separators

\end{itemize}

\begin{sphinxadmonition}{hint}{Hint:}
Please have a look at the tests to get a better feeling how it behaves under
tests/unit/transformer/test\_mapper\_custom\_data\_types.py::TestConversionsFromString
\end{sphinxadmonition}
\subsubsection*{Example}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{spooq2}\PYG{n+nn}{.}\PYG{n+nn}{transformer} \PYG{k+kn}{import} \PYG{n}{Mapper}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{input\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{g+go}{[Row(input\PYGZus{}string=\PYGZdq{}  21474838464.70 \PYGZdq{}),}
\PYG{g+go}{ Row(input\PYGZus{}string=\PYGZdq{}Hello\PYGZdq{}),}
\PYG{g+go}{ Row(input\PYGZus{}string=\PYGZdq{}21\PYGZus{}474\PYGZus{}838\PYGZus{}464.70\PYGZdq{})]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{mapping} \PYG{o}{=} \PYG{p}{[}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{output\PYGZus{}value}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{input\PYGZus{}string}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{extended\PYGZus{}string\PYGZus{}to\PYGZus{}double}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{output\PYGZus{}df} \PYG{o}{=} \PYG{n}{Mapper}\PYG{p}{(}\PYG{n}{mapping}\PYG{p}{)}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{input\PYGZus{}df}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{output\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{g+go}{[Row(input\PYGZus{}string=21474838464.7),}
\PYG{g+go}{ Row(input\PYGZus{}string=None),}
\PYG{g+go}{ Row(input\PYGZus{}string=21474838464.70)]}
\end{sphinxVerbatim}

\end{fulllineitems}

\index{\_generate\_select\_expression\_for\_extended\_string\_to\_boolean() (in module spooq2.transformer.mapper\_custom\_data\_types)@\spxentry{\_generate\_select\_expression\_for\_extended\_string\_to\_boolean()}\spxextra{in module spooq2.transformer.mapper\_custom\_data\_types}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{transformer/mapper:spooq2.transformer.mapper_custom_data_types._generate_select_expression_for_extended_string_to_boolean}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_generate\_select\_expression\_for\_extended\_string\_to\_boolean}}}{\emph{source\_column}, \emph{name}}{}
More robust conversion from StringType to BooleanType.
Is able to additionally handle (compared to implicit Spark conversion):
\begin{itemize}
\item {} 
Preceding whitespace

\item {} 
Trailing whitespace

\item {} 
Preceeding and trailing whitespace

\end{itemize}

\begin{sphinxadmonition}{hint}{Hint:}
Please have a look at the tests to get a better feeling how it behaves under
tests/unit/transformer/test\_mapper\_custom\_data\_types.py::TestConversionsFromString
\end{sphinxadmonition}
\subsubsection*{Example}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{spooq2}\PYG{n+nn}{.}\PYG{n+nn}{transformer} \PYG{k+kn}{import} \PYG{n}{Mapper}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{input\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{g+go}{[Row(input\PYGZus{}string=\PYGZdq{}  true \PYGZdq{}),}
\PYG{g+go}{ Row(input\PYGZus{}string=\PYGZdq{}0\PYGZdq{}),}
\PYG{g+go}{ Row(input\PYGZus{}string=\PYGZdq{}y\PYGZdq{})]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{mapping} \PYG{o}{=} \PYG{p}{[}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{output\PYGZus{}value}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{input\PYGZus{}string}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{extended\PYGZus{}string\PYGZus{}to\PYGZus{}boolean}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{output\PYGZus{}df} \PYG{o}{=} \PYG{n}{Mapper}\PYG{p}{(}\PYG{n}{mapping}\PYG{p}{)}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{input\PYGZus{}df}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{output\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{g+go}{[Row(input\PYGZus{}string=True),}
\PYG{g+go}{ Row(input\PYGZus{}string=False),}
\PYG{g+go}{ Row(input\PYGZus{}string=True)]}
\end{sphinxVerbatim}

\end{fulllineitems}

\index{\_generate\_select\_expression\_for\_extended\_string\_to\_timestamp() (in module spooq2.transformer.mapper\_custom\_data\_types)@\spxentry{\_generate\_select\_expression\_for\_extended\_string\_to\_timestamp()}\spxextra{in module spooq2.transformer.mapper\_custom\_data\_types}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{transformer/mapper:spooq2.transformer.mapper_custom_data_types._generate_select_expression_for_extended_string_to_timestamp}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_generate\_select\_expression\_for\_extended\_string\_to\_timestamp}}}{\emph{source\_column}, \emph{name}}{}
More robust conversion from StringType to TimestampsType. It is assumed that the
timezone is already set to UTC in spark / java to avoid implicit timezone conversions.
Is able to additionally handle (compared to implicit Spark conversion):
\begin{itemize}
\item {} 
Unix timestamps in seconds

\item {} 
Preceding whitespace

\item {} 
Trailing whitespace

\item {} 
Preceeding and trailing whitespace

\end{itemize}

\begin{sphinxadmonition}{hint}{Hint:}
Please have a look at the tests to get a better feeling how it behaves under
tests/unit/transformer/test\_mapper\_custom\_data\_types.py::TestConversionsFromString
\end{sphinxadmonition}
\subsubsection*{Example}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{spooq2}\PYG{n+nn}{.}\PYG{n+nn}{transformer} \PYG{k+kn}{import} \PYG{n}{Mapper}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{input\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{g+go}{[Row(input\PYGZus{}string=\PYGZdq{}2020\PYGZhy{}08\PYGZhy{}12T12:43:14+0000\PYGZdq{}),}
\PYG{g+go}{ Row(input\PYGZus{}string=\PYGZdq{}1597069446\PYGZdq{}),}
\PYG{g+go}{ Row(input\PYGZus{}string=\PYGZdq{}2020\PYGZhy{}08\PYGZhy{}12\PYGZdq{})]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{mapping} \PYG{o}{=} \PYG{p}{[}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{output\PYGZus{}value}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{input\PYGZus{}string}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{extended\PYGZus{}string\PYGZus{}to\PYGZus{}timestamp}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{output\PYGZus{}df} \PYG{o}{=} \PYG{n}{Mapper}\PYG{p}{(}\PYG{n}{mapping}\PYG{p}{)}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{input\PYGZus{}df}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{output\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{g+go}{[Row(input\PYGZus{}string=datetime.datetime(2020, 8, 12, 12, 43, 14)),}
\PYG{g+go}{ Row(input\PYGZus{}string=datetime.datetime(2020, 8, 10, 14, 24, 6)),}
\PYG{g+go}{ Row(input\PYGZus{}string=datetime.datetime(2020, 8, 12, 0, 0, 0))]}
\end{sphinxVerbatim}

\end{fulllineitems}



\subsection{Threshold-based Cleaner}
\label{\detokenize{transformer/threshold_cleaner:module-spooq2.transformer.threshold_cleaner}}\label{\detokenize{transformer/threshold_cleaner:threshold-based-cleaner}}\label{\detokenize{transformer/threshold_cleaner::doc}}\index{spooq2.transformer.threshold\_cleaner (module)@\spxentry{spooq2.transformer.threshold\_cleaner}\spxextra{module}}\index{ThresholdCleaner (class in spooq2.transformer.threshold\_cleaner)@\spxentry{ThresholdCleaner}\spxextra{class in spooq2.transformer.threshold\_cleaner}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{transformer/threshold_cleaner:spooq2.transformer.threshold_cleaner.ThresholdCleaner}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{ThresholdCleaner}}}{\emph{thresholds=\{\}}}{}
Bases: {\hyperref[\detokenize{base_classes/transformer:spooq2.transformer.transformer.Transformer}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{spooq2.transformer.transformer.Transformer}}}}}

Sets outiers within a DataFrame to a default value.
Takes a dictionary with valid value ranges for each column to be cleaned.
\subsubsection*{Example}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{transformer} \PYG{o}{=} \PYG{n}{ThresholdCleaner}\PYG{p}{(}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{thresholds}\PYG{o}{=}\PYG{p}{\PYGZob{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{created\PYGZus{}at}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{min}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{0}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{max}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{1580737513}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{default}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{k+kc}{None}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }        \PYG{p}{\PYGZcb{}}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{size\PYGZus{}cm}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{min}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{70}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{max}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{250}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{default}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{k+kc}{None}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }        \PYG{p}{\PYGZcb{}}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{p}{\PYGZcb{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{p}{)}
\end{sphinxVerbatim}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{thresholds}} (\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#dict}{\sphinxcode{\sphinxupquote{dict}}}) \textendash{} Dictionary containing column names and respective valid ranges

\item[{Returns}] \leavevmode
The transformed DataFrame

\item[{Return type}] \leavevmode
\sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.DataFrame}{\sphinxcode{\sphinxupquote{pyspark.sql.DataFrame}}}

\item[{Raises}] \leavevmode
\sphinxcode{\sphinxupquote{exceptions.ValueError}} \textendash{} Threshold-based cleaning only supports Numeric, Date and Timestamp Types!
Column with name: \{col\_name\} and type of: \{col\_type\} was provided

\end{description}\end{quote}

\begin{sphinxadmonition}{warning}{Warning:}
Only Numeric, TimestampType, and DateType data types are supported!
\end{sphinxadmonition}
\index{transform() (ThresholdCleaner method)@\spxentry{transform()}\spxextra{ThresholdCleaner method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{transformer/threshold_cleaner:spooq2.transformer.threshold_cleaner.ThresholdCleaner.transform}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform}}}{\emph{input\_df}}{}
Performs a transformation on a DataFrame.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{input\_df}} (\sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.DataFrame}{\sphinxcode{\sphinxupquote{pyspark.sql.DataFrame}}}) \textendash{} Input DataFrame

\item[{Returns}] \leavevmode
Transformed DataFrame.

\item[{Return type}] \leavevmode
\sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.DataFrame}{\sphinxcode{\sphinxupquote{pyspark.sql.DataFrame}}}

\end{description}\end{quote}

\begin{sphinxadmonition}{note}{Note:}
This method does only take the Input DataFrame as a parameters. All other needed parameters
are defined in the initialization of the Transformator Object.
\end{sphinxadmonition}

\end{fulllineitems}


\end{fulllineitems}



\subsection{Newest by Group (Most current record per ID)}
\label{\detokenize{transformer/newest_by_group:module-spooq2.transformer.newest_by_group}}\label{\detokenize{transformer/newest_by_group:newest-by-group-most-current-record-per-id}}\label{\detokenize{transformer/newest_by_group::doc}}\index{spooq2.transformer.newest\_by\_group (module)@\spxentry{spooq2.transformer.newest\_by\_group}\spxextra{module}}\index{NewestByGroup (class in spooq2.transformer.newest\_by\_group)@\spxentry{NewestByGroup}\spxextra{class in spooq2.transformer.newest\_by\_group}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{transformer/newest_by_group:spooq2.transformer.newest_by_group.NewestByGroup}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{NewestByGroup}}}{\emph{group\_by={[}'id'{]}, order\_by={[}'updated\_at', 'deleted\_at'{]}}}{}
Bases: {\hyperref[\detokenize{base_classes/transformer:spooq2.transformer.transformer.Transformer}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{spooq2.transformer.transformer.Transformer}}}}}

Groups, orders and selects first element per group.
\subsubsection*{Example}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{transformer} \PYG{o}{=} \PYG{n}{NewestByGroup}\PYG{p}{(}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{group\PYGZus{}by}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{first\PYGZus{}name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{last\PYGZus{}name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{order\PYGZus{}by}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{created\PYGZus{}at\PYGZus{}ms}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{version}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{p}{)}
\end{sphinxVerbatim}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{group\_by}} (\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}} or \sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#list}{\sphinxcode{\sphinxupquote{list}}} of \sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}, (Defaults to {[}‘id’{]})) \textendash{} List of attributes to be used within the Window Function as Grouping Arguments.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{order\_by}} (\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}} or \sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#list}{\sphinxcode{\sphinxupquote{list}}} of \sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}, (Defaults to {[}‘updated\_at’, ‘deleted\_at’{]})) \textendash{} List of attributes to be used within the Window Function as Ordering Arguments.
All columns will be sorted in \sphinxstylestrong{descending} order.

\end{itemize}

\item[{Raises}] \leavevmode
\sphinxcode{\sphinxupquote{exceptions.AttributeError}} \textendash{} If any Attribute in \sphinxcode{\sphinxupquote{group\_by}} or \sphinxcode{\sphinxupquote{order\_by}} is not contained in the
input DataFrame.

\end{description}\end{quote}

\begin{sphinxadmonition}{note}{Note:}
PySpark’s \sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.Window}{\sphinxcode{\sphinxupquote{Window}}} function is used internally
The first row (\sphinxcode{\sphinxupquote{row\_number()}}) per window will be selected and returned.
\end{sphinxadmonition}
\index{transform() (NewestByGroup method)@\spxentry{transform()}\spxextra{NewestByGroup method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{transformer/newest_by_group:spooq2.transformer.newest_by_group.NewestByGroup.transform}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform}}}{\emph{input\_df}}{}
Performs a transformation on a DataFrame.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{input\_df}} (\sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.DataFrame}{\sphinxcode{\sphinxupquote{pyspark.sql.DataFrame}}}) \textendash{} Input DataFrame

\item[{Returns}] \leavevmode
Transformed DataFrame.

\item[{Return type}] \leavevmode
\sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.DataFrame}{\sphinxcode{\sphinxupquote{pyspark.sql.DataFrame}}}

\end{description}\end{quote}

\begin{sphinxadmonition}{note}{Note:}
This method does only take the Input DataFrame as a parameters. All other needed parameters
are defined in the initialization of the Transformator Object.
\end{sphinxadmonition}

\end{fulllineitems}


\end{fulllineitems}



\subsection{Class Diagram of Transformer Subpackage}
\label{\detokenize{transformer/overview:class-diagram-of-transformer-subpackage}}
\noindent\sphinxincludegraphics{{plantuml-a3417a3ea8f19e7458e42a227d228903e302f0b7}.png}


\subsection{Create your own Transformer}
\label{\detokenize{transformer/overview:create-your-own-transformer}}
Please see the {\hyperref[\detokenize{base_classes/transformer:custom-transformer}]{\sphinxcrossref{\DUrole{std,std-ref}{Create your own Transformer}}}} for further details.


\section{Loaders}
\label{\detokenize{loader/overview:module-spooq2.loader.loader}}\label{\detokenize{loader/overview:loaders}}\label{\detokenize{loader/overview::doc}}\index{spooq2.loader.loader (module)@\spxentry{spooq2.loader.loader}\spxextra{module}}
Loaders take a \sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.DataFrame}{\sphinxcode{\sphinxupquote{pyspark.sql.DataFrame}}} as an input and save it to a sink.

Each Loader class has to have a \sphinxtitleref{load} method which takes a DataFrame as single paremter.

Possible Loader sinks can be \sphinxstylestrong{Hive Tables}, \sphinxstylestrong{Kudu Tables}, \sphinxstylestrong{HBase Tables}, \sphinxstylestrong{JDBC
Sinks} or \sphinxstylestrong{ParquetFiles}.


\subsection{Hive Database}
\label{\detokenize{loader/hive_loader:module-spooq2.loader.hive_loader}}\label{\detokenize{loader/hive_loader:hive-database}}\label{\detokenize{loader/hive_loader::doc}}\index{spooq2.loader.hive\_loader (module)@\spxentry{spooq2.loader.hive\_loader}\spxextra{module}}\index{HiveLoader (class in spooq2.loader.hive\_loader)@\spxentry{HiveLoader}\spxextra{class in spooq2.loader.hive\_loader}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{loader/hive_loader:spooq2.loader.hive_loader.HiveLoader}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{HiveLoader}}}{\emph{db\_name, table\_name, partition\_definitions={[}\{'column\_name': 'dt', 'column\_type': 'IntegerType', 'default\_value': None\}{]}, clear\_partition=True, repartition\_size=40, auto\_create\_table=True, overwrite\_partition\_value=True}}{}
Bases: {\hyperref[\detokenize{base_classes/loader:spooq2.loader.loader.Loader}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{spooq2.loader.loader.Loader}}}}}

Persists a PySpark DataFrame into a Hive Table.
\subsubsection*{Examples}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{HiveLoader}\PYG{p}{(}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{db\PYGZus{}name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{users\PYGZus{}and\PYGZus{}friends}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{table\PYGZus{}name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{friends\PYGZus{}partitioned}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{partition\PYGZus{}definitions}\PYG{o}{=}\PYG{p}{[}\PYG{p}{\PYGZob{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{column\PYGZus{}name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{dt}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{column\PYGZus{}type}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{IntegerType}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{default\PYGZus{}value}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{20200201}\PYG{p}{\PYGZcb{}}\PYG{p}{]}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{clear\PYGZus{}partition}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{repartition\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{overwrite\PYGZus{}partition\PYGZus{}value}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{auto\PYGZus{}create\PYGZus{}table}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{p}{)}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{input\PYGZus{}df}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{HiveLoader}\PYG{p}{(}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{db\PYGZus{}name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{users\PYGZus{}and\PYGZus{}friends}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{table\PYGZus{}name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{all\PYGZus{}friends}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{partition\PYGZus{}definitions}\PYG{o}{=}\PYG{p}{[}\PYG{p}{]}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{repartition\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{200}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{auto\PYGZus{}create\PYGZus{}table}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{p}{)}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{input\PYGZus{}df}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{db\_name}} (\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}) \textendash{} The database name to load the data into.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{table\_name}} (\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}) \textendash{} The table name to load the data into. The database name must not be included in this
parameter as it is already defined in the \sphinxtitleref{db\_name} parameter.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{partition\_definitions}} (\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#list}{\sphinxcode{\sphinxupquote{list}}} of \sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#dict}{\sphinxcode{\sphinxupquote{dict}}}) \textendash{} 
(Defaults to \sphinxtitleref{{[}\{“column\_name”: “dt”, “column\_type”: “IntegerType”, “default\_value”: None\}{]}}).
\begin{itemize}
\item {} 
\sphinxstylestrong{column\_name} (\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}) - The Column’s Name to partition by.

\item {} 
\sphinxstylestrong{column\_type} (\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}) - The PySpark SQL DataType for the Partition Value as
a String. This should normally either be ‘IntegerType()’ or ‘StringType()’

\item {} 
\sphinxstylestrong{default\_value} (\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}} or \sphinxhref{https://docs.python.org/3.7/library/functions.html\#int}{\sphinxcode{\sphinxupquote{int}}}) - If \sphinxtitleref{column\_name} does not contain
a value or \sphinxtitleref{overwrite\_partition\_value} is set, this value will be used for the
partitioning

\end{itemize}


\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{clear\_partition}} (\sphinxhref{https://docs.python.org/3.7/library/functions.html\#bool}{\sphinxcode{\sphinxupquote{bool}}}, (Defaults to True)) \textendash{} This flag tells the Loader to delete the defined partitions before
inserting the input DataFrame into the target table. Has no effect if no partitions are
defined.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{repartition\_size}} (\sphinxhref{https://docs.python.org/3.7/library/functions.html\#int}{\sphinxcode{\sphinxupquote{int}}}, (Defaults to 40)) \textendash{} The DataFrame will be repartitioned on Spark level before inserting into the table.
This effects the number of output files on which the Hive table is based.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{auto\_create\_table}} (\sphinxhref{https://docs.python.org/3.7/library/functions.html\#bool}{\sphinxcode{\sphinxupquote{bool}}}, (Defaults to True)) \textendash{} Whether the target table will be created if it does not yet exist.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{overwrite\_partition\_value}} (\sphinxhref{https://docs.python.org/3.7/library/functions.html\#bool}{\sphinxcode{\sphinxupquote{bool}}}, (Defaults to True)) \textendash{} Defines whether the values of columns defined in \sphinxtitleref{partition\_definitions} should
explicitly set by default\_values.

\end{itemize}

\item[{Raises}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{exceptions.AssertionError}}: \textendash{} partition\_definitions has to be a list containing dicts. Expected dict content:
‘column\_name’, ‘column\_type’, ‘default\_value’ per partition\_definitions item.

\item {} 
\sphinxcode{\sphinxupquote{exceptions.AssertionError}}: \textendash{} Items of partition\_definitions have to be dictionaries.

\item {} 
\sphinxcode{\sphinxupquote{exceptions.AssertionError}}: \textendash{} No column name set!

\item {} 
\sphinxcode{\sphinxupquote{exceptions.AssertionError}}: \textendash{} Not a valid (PySpark) datatype for the partition column \{name\} \textbar{} \{type\}.

\item {} 
\sphinxcode{\sphinxupquote{exceptions.AssertionError}}: \textendash{} \sphinxtitleref{clear\_partition} is only supported if \sphinxtitleref{overwrite\_partition\_value} is also enabled.
This would otherwise result in clearing partitions on basis of dynamically values
(from DataFrame) instead of explicitly defining the partition(s) to clear.

\end{itemize}

\end{description}\end{quote}
\index{load() (HiveLoader method)@\spxentry{load()}\spxextra{HiveLoader method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{loader/hive_loader:spooq2.loader.hive_loader.HiveLoader.load}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{load}}}{\emph{input\_df}}{}
Persists data from a PySpark DataFrame to a target table.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{input\_df}} (\sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.DataFrame}{\sphinxcode{\sphinxupquote{pyspark.sql.DataFrame}}}) \textendash{} Input DataFrame which has to be loaded to a target destination.

\end{description}\end{quote}

\begin{sphinxadmonition}{note}{Note:}
This method takes only a single DataFrame as an input parameter. All other needed
parameters are defined in the initialization of the Loader object.
\end{sphinxadmonition}

\end{fulllineitems}


\end{fulllineitems}



\subsubsection{Activity Diagram}
\label{\detokenize{loader/hive_loader:activity-diagram}}
\noindent\sphinxincludegraphics{{plantuml-21ced8d394a9bd5e4bc11df1bfc169f26f98c783}.png}


\subsection{Class Diagram of Loader Subpackage}
\label{\detokenize{loader/overview:class-diagram-of-loader-subpackage}}
\noindent\sphinxincludegraphics{{plantuml-ff1a3d60fa859fe38af7e710199c5dcfe2bafbd7}.png}


\subsection{Create your own Loader}
\label{\detokenize{loader/overview:create-your-own-loader}}
Please see the {\hyperref[\detokenize{base_classes/loader:custom-loader}]{\sphinxcrossref{\DUrole{std,std-ref}{Create your own Loader}}}} for further details.


\section{Pipeline}
\label{\detokenize{pipeline/overview:pipeline}}\label{\detokenize{pipeline/overview::doc}}

\subsection{Pipeline}
\label{\detokenize{pipeline/pipeline:module-spooq2.pipeline.pipeline}}\label{\detokenize{pipeline/pipeline:pipeline}}\label{\detokenize{pipeline/pipeline::doc}}\index{spooq2.pipeline.pipeline (module)@\spxentry{spooq2.pipeline.pipeline}\spxextra{module}}
This type of object glues the aforementioned processes together and  extracts, transforms
(Transformer chain possible) and loads the data from start to end.
\index{Pipeline (class in spooq2.pipeline.pipeline)@\spxentry{Pipeline}\spxextra{class in spooq2.pipeline.pipeline}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pipeline/pipeline:spooq2.pipeline.pipeline.Pipeline}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{Pipeline}}}{\emph{input\_df=None}, \emph{bypass\_loader=False}}{}
Bases: \sphinxhref{https://docs.python.org/3.7/library/functions.html\#object}{\sphinxcode{\sphinxupquote{object}}}

Represents a Pipeline of an Extractor, (multiple) Transformers and a Loader Object.
\index{extractor (Pipeline attribute)@\spxentry{extractor}\spxextra{Pipeline attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pipeline/pipeline:spooq2.pipeline.pipeline.Pipeline.extractor}}\pysigline{\sphinxbfcode{\sphinxupquote{extractor}}}
The entry point of the Pipeline. Extracts a DataFrame from a Source.
\begin{quote}\begin{description}
\item[{Type}] \leavevmode
Subclass of \sphinxcode{\sphinxupquote{spooq2.extractor.Extractor}}

\end{description}\end{quote}

\end{fulllineitems}

\index{transformers (Pipeline attribute)@\spxentry{transformers}\spxextra{Pipeline attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pipeline/pipeline:spooq2.pipeline.pipeline.Pipeline.transformers}}\pysigline{\sphinxbfcode{\sphinxupquote{transformers}}}
The Data Wrangling Part of the Pipeline. A chain of Transformers, a single Transformer
or a PassThrough Transformer can be set and used.
\begin{quote}\begin{description}
\item[{Type}] \leavevmode
List of Subclasses of \sphinxcode{\sphinxupquote{spooq2.transformer.Transformer}} Objects

\end{description}\end{quote}

\end{fulllineitems}

\index{loader (Pipeline attribute)@\spxentry{loader}\spxextra{Pipeline attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pipeline/pipeline:spooq2.pipeline.pipeline.Pipeline.loader}}\pysigline{\sphinxbfcode{\sphinxupquote{loader}}}
The exit point of the Pipeline. Loads a DataFrame to a target Sink.
\begin{quote}\begin{description}
\item[{Type}] \leavevmode
Subclass of \sphinxcode{\sphinxupquote{spooq2.loader.Loader}}

\end{description}\end{quote}

\end{fulllineitems}

\index{name (Pipeline attribute)@\spxentry{name}\spxextra{Pipeline attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pipeline/pipeline:spooq2.pipeline.pipeline.Pipeline.name}}\pysigline{\sphinxbfcode{\sphinxupquote{name}}}
Sets the \sphinxtitleref{\_\_name\_\_} of the class’ type as \sphinxtitleref{name}, which is essentially the Class’ Name.
\begin{quote}\begin{description}
\item[{Type}] \leavevmode
\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}

\end{description}\end{quote}

\end{fulllineitems}

\index{logger (Pipeline attribute)@\spxentry{logger}\spxextra{Pipeline attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pipeline/pipeline:spooq2.pipeline.pipeline.Pipeline.logger}}\pysigline{\sphinxbfcode{\sphinxupquote{logger}}}
Shared, class level logger for all instances.
\begin{quote}\begin{description}
\item[{Type}] \leavevmode
\sphinxhref{https://docs.python.org/3.7/library/logging.html\#logging.Logger}{\sphinxcode{\sphinxupquote{logging.Logger}}}

\end{description}\end{quote}

\end{fulllineitems}

\subsubsection*{Example}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{spooq2}\PYG{n+nn}{.}\PYG{n+nn}{pipeline} \PYG{k+kn}{import} \PYG{n}{Pipeline}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{import} \PYG{n+nn}{spooq2}\PYG{n+nn}{.}\PYG{n+nn}{extractor} \PYG{k}{as}   \PYG{n+nn}{E}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{import} \PYG{n+nn}{spooq2}\PYG{n+nn}{.}\PYG{n+nn}{transformer} \PYG{k}{as} \PYG{n+nn}{T}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{import} \PYG{n+nn}{spooq2}\PYG{n+nn}{.}\PYG{n+nn}{loader} \PYG{k}{as}      \PYG{n+nn}{L}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{c+c1}{\PYGZsh{}  Definition how the output table should look like and where the attributes come from:}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{users\PYGZus{}mapping} \PYG{o}{=} \PYG{p}{[}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{id}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}              \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{id}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}                     \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{IntegerType}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{guid}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{guid}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}                   \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{StringType}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{forename}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{attributes.first\PYGZus{}name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}  \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{StringType}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{surename}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{attributes.last\PYGZus{}name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}   \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{StringType}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{gender}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}          \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{attributes.gender}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}      \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{StringType}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{has\PYGZus{}email}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}       \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{attributes.email}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}       \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{StringBoolean}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{has\PYGZus{}university}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}  \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{attributes.university}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}  \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{StringBoolean}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{created\PYGZus{}at}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}      \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{meta.created\PYGZus{}at\PYGZus{}ms}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}     \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{timestamp\PYGZus{}ms\PYGZus{}to\PYGZus{}s}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{p}{]}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{c+c1}{\PYGZsh{}  The main object where all steps are defined:}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{users\PYGZus{}pipeline} \PYG{o}{=} \PYG{n}{Pipeline}\PYG{p}{(}\PYG{p}{)}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{c+c1}{\PYGZsh{}  Defining the EXTRACTION:}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{users\PYGZus{}pipeline}\PYG{o}{.}\PYG{n}{set\PYGZus{}extractor}\PYG{p}{(}\PYG{n}{E}\PYG{o}{.}\PYG{n}{JSONExtractor}\PYG{p}{(}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{input\PYGZus{}path}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tests/data/schema\PYGZus{}v1/sequenceFiles}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{p}{)}\PYG{p}{)}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{c+c1}{\PYGZsh{}  Defining the TRANSFORMATION:}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{users\PYGZus{}pipeline}\PYG{o}{.}\PYG{n}{add\PYGZus{}transformers}\PYG{p}{(}\PYG{p}{[}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{T}\PYG{o}{.}\PYG{n}{Mapper}\PYG{p}{(}\PYG{n}{mapping}\PYG{o}{=}\PYG{n}{users\PYGZus{}mapping}\PYG{p}{)}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{T}\PYG{o}{.}\PYG{n}{ThresholdCleaner}\PYG{p}{(}\PYG{n}{thresholds}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{created\PYGZus{}at}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }                                                \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{min}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{0}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }                                                \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{max}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{1580737513}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }                                                \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{default}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{k+kc}{None}\PYG{p}{\PYGZcb{}}\PYG{p}{\PYGZcb{}}\PYG{p}{)}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{T}\PYG{o}{.}\PYG{n}{NewestByGroup}\PYG{p}{(}\PYG{n}{group\PYGZus{}by}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{id}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{order\PYGZus{}by}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{created\PYGZus{}at}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{p}{]}\PYG{p}{)}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{c+c1}{\PYGZsh{}  Defining the LOAD:}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{users\PYGZus{}pipeline}\PYG{o}{.}\PYG{n}{set\PYGZus{}loader}\PYG{p}{(}\PYG{n}{L}\PYG{o}{.}\PYG{n}{HiveLoader}\PYG{p}{(}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{db\PYGZus{}name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{users\PYGZus{}and\PYGZus{}friends}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{table\PYGZus{}name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{users}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{partition\PYGZus{}definitions}\PYG{o}{=}\PYG{p}{[}\PYG{p}{\PYGZob{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{column\PYGZus{}name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{dt}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{column\PYGZus{}type}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{IntegerType}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{default\PYGZus{}value}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{20200201}\PYG{p}{\PYGZcb{}}\PYG{p}{]}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{repartition\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{p}{)}\PYG{p}{)}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{c+c1}{\PYGZsh{}  Executing the whole ETL pipeline}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{users\PYGZus{}pipeline}\PYG{o}{.}\PYG{n}{execute}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}
\index{execute() (Pipeline method)@\spxentry{execute()}\spxextra{Pipeline method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pipeline/pipeline:spooq2.pipeline.pipeline.Pipeline.execute}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{execute}}}{}{}
Executes the whole Pipeline at once.

Extracts from the Source, transformes the DataFrame and loads it into a target Sink.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\sphinxstylestrong{input\_df} \textendash{} \sphinxstylestrong{If} the \sphinxcode{\sphinxupquote{bypass\_loader}} attribute was set to True in the Pipeline class,
the output DataFrame from the Transformer(s) will be directly returned.

\item[{Return type}] \leavevmode
\sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.DataFrame}{\sphinxcode{\sphinxupquote{pyspark.sql.DataFrame}}}

\end{description}\end{quote}

\begin{sphinxadmonition}{note}{Note:}
This method does not take ANY input parameters. All needed parameters are defined
at the initialization phase.
\end{sphinxadmonition}

\end{fulllineitems}

\index{extract() (Pipeline method)@\spxentry{extract()}\spxextra{Pipeline method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pipeline/pipeline:spooq2.pipeline.pipeline.Pipeline.extract}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{extract}}}{}{}
Calls the extract Method on the Extractor Object.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
The output\_df from the Extractor used as the input for the Transformer (chain).

\item[{Return type}] \leavevmode
\sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.DataFrame}{\sphinxcode{\sphinxupquote{pyspark.sql.DataFrame}}}

\end{description}\end{quote}

\end{fulllineitems}

\index{transform() (Pipeline method)@\spxentry{transform()}\spxextra{Pipeline method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pipeline/pipeline:spooq2.pipeline.pipeline.Pipeline.transform}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform}}}{\emph{input\_df}}{}
Calls the transform Method on the Transformer Object(s) in the order of importing the
Objects while passing the DataFrame.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{input\_df}} (\sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.DataFrame}{\sphinxcode{\sphinxupquote{pyspark.sql.DataFrame}}}) \textendash{} The output DataFrame of the Extractor Object.

\item[{Returns}] \leavevmode
The input DataFrame for the Loader.

\item[{Return type}] \leavevmode
\sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.DataFrame}{\sphinxcode{\sphinxupquote{pyspark.sql.DataFrame}}}

\end{description}\end{quote}

\end{fulllineitems}

\index{load() (Pipeline method)@\spxentry{load()}\spxextra{Pipeline method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pipeline/pipeline:spooq2.pipeline.pipeline.Pipeline.load}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{load}}}{\emph{input\_df}}{}
Calls the load Method on the Loader Object.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{input\_df}} (\sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.DataFrame}{\sphinxcode{\sphinxupquote{pyspark.sql.DataFrame}}}) \textendash{} The output DataFrame from the Transformer(s).

\item[{Returns}] \leavevmode
\sphinxstylestrong{input\_df} \textendash{} \sphinxstylestrong{If} the \sphinxcode{\sphinxupquote{bypass\_loader}} attribute was set to True in the Pipeline class,
the output DataFrame from the Transformer(s) will be directly returned.

\item[{Return type}] \leavevmode
\sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.DataFrame}{\sphinxcode{\sphinxupquote{pyspark.sql.DataFrame}}}

\end{description}\end{quote}

\end{fulllineitems}

\index{set\_extractor() (Pipeline method)@\spxentry{set\_extractor()}\spxextra{Pipeline method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pipeline/pipeline:spooq2.pipeline.pipeline.Pipeline.set_extractor}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{set\_extractor}}}{\emph{extractor}}{}
Sets an Extractor Object to be used within the Pipeline.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{extractor}} (Subclass of \sphinxcode{\sphinxupquote{spooq2.extractor.Extractor}}) \textendash{} An already initialized Object of any Subclass of spooq2.extractor.Extractor.

\item[{Raises}] \leavevmode
\sphinxcode{\sphinxupquote{exceptions.AssertionError}}: \textendash{} An input\_df was already provided which bypasses the extraction action

\end{description}\end{quote}

\end{fulllineitems}

\index{add\_transformers() (Pipeline method)@\spxentry{add\_transformers()}\spxextra{Pipeline method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pipeline/pipeline:spooq2.pipeline.pipeline.Pipeline.add_transformers}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{add\_transformers}}}{\emph{transformers}}{}
Adds a list of Transformer Objects to be used within the Pipeline.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{transformer}} (\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#list}{\sphinxcode{\sphinxupquote{list}}} of Subclass of \sphinxcode{\sphinxupquote{spooq2.transformer.Transformer}}) \textendash{} Already initialized Object of any Subclass of spooq2.transformer.Transformer.

\end{description}\end{quote}

\end{fulllineitems}

\index{clear\_transformers() (Pipeline method)@\spxentry{clear\_transformers()}\spxextra{Pipeline method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pipeline/pipeline:spooq2.pipeline.pipeline.Pipeline.clear_transformers}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{clear\_transformers}}}{}{}
Clears the list of already added Transformers.

\end{fulllineitems}

\index{set\_loader() (Pipeline method)@\spxentry{set\_loader()}\spxextra{Pipeline method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pipeline/pipeline:spooq2.pipeline.pipeline.Pipeline.set_loader}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{set\_loader}}}{\emph{loader}}{}
Sets an Loader Object to be used within the Pipeline.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{loader}} (Subclass of \sphinxcode{\sphinxupquote{spooq2.loader.Loader}}) \textendash{} An already initialized Object of any Subclass of spooq2.loader.Loader.

\item[{Raises}] \leavevmode
\sphinxcode{\sphinxupquote{exceptions.AssertionError}}: \textendash{} You can not set a loader if the \sphinxtitleref{bypass\_loader} parameter is set.

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\subsection{Pipeline Factory}
\label{\detokenize{pipeline/pipeline_factory:module-spooq2.pipeline.factory}}\label{\detokenize{pipeline/pipeline_factory:pipeline-factory}}\label{\detokenize{pipeline/pipeline_factory::doc}}\index{spooq2.pipeline.factory (module)@\spxentry{spooq2.pipeline.factory}\spxextra{module}}
To decrease the complexity of building data pipelines for data engineers, an expert system or
business rules engine can be used to automatically build and configure a data pipeline based on
context variables, groomed metadata, and relevant rules.
\index{PipelineFactory (class in spooq2.pipeline.factory)@\spxentry{PipelineFactory}\spxextra{class in spooq2.pipeline.factory}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pipeline/pipeline_factory:spooq2.pipeline.factory.PipelineFactory}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{PipelineFactory}}}{\emph{url='http://localhost:5000/pipeline/get'}}{}
Bases: \sphinxhref{https://docs.python.org/3.7/library/functions.html\#object}{\sphinxcode{\sphinxupquote{object}}}

Provides an interface to automatically construct pipelines for Spooq.
\subsubsection*{Example}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{pipeline\PYGZus{}factory} \PYG{o}{=} \PYG{n}{PipelineFactory}\PYG{p}{(}\PYG{p}{)}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{c+c1}{\PYGZsh{}  Fetch user data set with applied mapping, filtering,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{c+c1}{\PYGZsh{}  and cleaning transformers}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{df} \PYG{o}{=} \PYG{n}{pipeline\PYGZus{}factory}\PYG{o}{.}\PYG{n}{execute}\PYG{p}{(}\PYG{p}{\PYGZob{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }     \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{entity\PYGZus{}type}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{user}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }     \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{date}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{2018\PYGZhy{}10\PYGZhy{}20}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }     \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{time\PYGZus{}range}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{last\PYGZus{}day}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{c+c1}{\PYGZsh{}  Load user data partition with applied mapping, filtering,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{c+c1}{\PYGZsh{}  and cleaning transformers to a hive database}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{pipeline\PYGZus{}factory}\PYG{o}{.}\PYG{n}{execute}\PYG{p}{(}\PYG{p}{\PYGZob{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }     \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{entity\PYGZus{}type}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{user}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }     \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{date}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{2018\PYGZhy{}10\PYGZhy{}20}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }     \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{batch\PYGZus{}size}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{daily}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\end{sphinxVerbatim}
\index{url (PipelineFactory attribute)@\spxentry{url}\spxextra{PipelineFactory attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pipeline/pipeline_factory:spooq2.pipeline.factory.PipelineFactory.url}}\pysigline{\sphinxbfcode{\sphinxupquote{url}}}
The end point of an expert system which will be called to infer names and parameters.
\begin{quote}\begin{description}
\item[{Type}] \leavevmode
\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}, (Defaults to “\sphinxurl{http://localhost:5000/pipeline/get}”)

\end{description}\end{quote}

\end{fulllineitems}


\begin{sphinxadmonition}{note}{Note:}
PipelineFactory is only responsible for querying an expert system with provided parameters
and constructing a Spooq pipeline out of the response. It does not have any reasoning capabilities
itself! It requires therefore a HTTP service responding with a JSON object containing following structure:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{extractor}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Type1Extractor}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{params}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{key 1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{val 1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{key N}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{val N}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{\PYGZcb{}}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{transformers}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{[}
        \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Type1Transformer}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{params}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{key 1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{val 1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{key N}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{val N}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{\PYGZcb{}}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
        \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Type2Transformer}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{params}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{key 1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{val 1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{key N}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{val N}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{\PYGZcb{}}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
        \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Type3Transformer}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{params}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{key 1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{val 1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{key N}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{val N}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{\PYGZcb{}}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
        \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Type4Transformer}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{params}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{key 1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{val 1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{key N}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{val N}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{\PYGZcb{}}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
        \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Type5Transformer}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{params}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{key 1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{val 1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{key N}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{val N}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{\PYGZcb{}}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
    \PYG{p}{]}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{loader}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Type1Loader}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{params}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{key 1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{val 1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{key N}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{val N}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{\PYGZcb{}}\PYG{p}{\PYGZcb{}}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}
\end{sphinxadmonition}

\begin{sphinxadmonition}{hint}{Hint:}
There is an experimental implementation of an expert system which complies with the requirements
of PipelineFactory called \sphinxtitleref{spooq\_rules}. If you are interested, please ask the author of Spooq about it.
\end{sphinxadmonition}
\index{execute() (PipelineFactory method)@\spxentry{execute()}\spxextra{PipelineFactory method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pipeline/pipeline_factory:spooq2.pipeline.factory.PipelineFactory.execute}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{execute}}}{\emph{context\_variables}}{}
Fetches a ready-to-go pipeline instance via {\hyperref[\detokenize{pipeline/pipeline_factory:spooq2.pipeline.factory.PipelineFactory.get_pipeline}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{get\_pipeline()}}}}}
and executes it.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{context\_variables}} (\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#dict}{\sphinxcode{\sphinxupquote{dict}}}) \textendash{} These collection of parameters should describe the current context about the use case
of the pipeline. Please see the examples of the PipelineFactory class’
documentation.

\item[{Returns}] \leavevmode
\begin{itemize}
\item {} 
\sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.DataFrame}{\sphinxcode{\sphinxupquote{pyspark.sql.DataFrame}}} \textendash{} If the loader component is by-passed (in the case of ad\_hoc use cases).

\item {} 
\sphinxhref{https://docs.python.org/3.7/library/constants.html\#None}{\sphinxcode{\sphinxupquote{None}}} \textendash{} If the loader component does not return a value (in the case of persisting data).

\end{itemize}


\end{description}\end{quote}

\end{fulllineitems}

\index{get\_metadata() (PipelineFactory method)@\spxentry{get\_metadata()}\spxextra{PipelineFactory method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pipeline/pipeline_factory:spooq2.pipeline.factory.PipelineFactory.get_metadata}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_metadata}}}{\emph{context\_variables}}{}
Sends a POST request to the defined endpoint (\sphinxtitleref{url}) containing the
supplied context variables.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{context\_variables}} (\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#dict}{\sphinxcode{\sphinxupquote{dict}}}) \textendash{} These collection of parameters should describe the current context about the use case
of the pipeline. Please see the examples of the PipelineFactory class’
documentation.

\item[{Returns}] \leavevmode
Names and parameters of each ETL component to construct a Spooq pipeline

\item[{Return type}] \leavevmode
\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#dict}{\sphinxcode{\sphinxupquote{dict}}}

\end{description}\end{quote}

\end{fulllineitems}

\index{get\_pipeline() (PipelineFactory method)@\spxentry{get\_pipeline()}\spxextra{PipelineFactory method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{pipeline/pipeline_factory:spooq2.pipeline.factory.PipelineFactory.get_pipeline}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_pipeline}}}{\emph{context\_variables}}{}
Fetches the necessary metadata via {\hyperref[\detokenize{pipeline/pipeline_factory:spooq2.pipeline.factory.PipelineFactory.get_metadata}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{get\_metadata()}}}}} and
returns a ready-to-go pipeline instance.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{context\_variables}} (\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#dict}{\sphinxcode{\sphinxupquote{dict}}}) \textendash{} These collection of parameters should describe the current context about the use case
of the pipeline. Please see the examples of the PipelineFactory class’
documentation.

\item[{Returns}] \leavevmode
A Spooq pipeline instance which is fully configured and can still be
adapted and consequently executed.

\item[{Return type}] \leavevmode
\sphinxcode{\sphinxupquote{Pipeline}}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\subsection{Class Diagram of Pipeline Subpackage}
\label{\detokenize{pipeline/overview:class-diagram-of-pipeline-subpackage}}
\noindent\sphinxincludegraphics{{plantuml-6e82c644626acd13dc04b552c7aa0235fc61ace2}.png}


\section{Spooq Base}
\label{\detokenize{base_classes/overview:spooq-base}}\label{\detokenize{base_classes/overview::doc}}

\subsection{Global Logger}
\label{\detokenize{base_classes/spooq2_logger:module-spooq2.spooq2_logger}}\label{\detokenize{base_classes/spooq2_logger:global-logger}}\label{\detokenize{base_classes/spooq2_logger::doc}}\index{spooq2.spooq2\_logger (module)@\spxentry{spooq2.spooq2\_logger}\spxextra{module}}
Global Logger instance used by Spooq2.
\subsubsection*{Example}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{import} \PYG{n+nn}{logging}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{logga} \PYG{o}{=} \PYG{n}{logging}\PYG{o}{.}\PYG{n}{getLogger}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{spooq2}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{g+go}{\PYGZlt{}logging.Logger at 0x7f5dc8eb2890\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{logga}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Hello World}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{g+go}{[spooq2] 2020\PYGZhy{}03\PYGZhy{}21 23:55:48,253 INFO logging\PYGZus{}example::\PYGZlt{}module\PYGZgt{}::4: Hello World}
\end{sphinxVerbatim}
\index{initialize() (in module spooq2.spooq2\_logger)@\spxentry{initialize()}\spxextra{in module spooq2.spooq2\_logger}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{base_classes/spooq2_logger:spooq2.spooq2_logger.initialize}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{initialize}}}{}{}
Initializes the global logger for Spooq with pre-defined levels for \sphinxcode{\sphinxupquote{stdout}} and \sphinxcode{\sphinxupquote{stderr}}.
No input parameters are needed, as the configuration is received via {\hyperref[\detokenize{base_classes/spooq2_logger:spooq2.spooq2_logger.get_logging_level}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{get\_logging\_level()}}}}}.

\begin{sphinxadmonition}{note}{Note:}\begin{description}
\item[{The output format is defined as:}] \leavevmode
\begin{DUlineblock}{0em}
\item[] “{[}\%(name)s{]} \%(asctime)s \%(levelname)s \%(module)s::\%(funcName)s::\%(lineno)d: \%(message)s”
\item[] For example “{[}spooq2{]} 2020-03-11 15:40:59,313 DEBUG newest\_by\_group::\_\_init\_\_::53: group by columns: {[}u’user\_id’{]}”
\end{DUlineblock}

\end{description}
\end{sphinxadmonition}

\begin{sphinxadmonition}{warning}{Warning:}
The \sphinxcode{\sphinxupquote{root}} logger of python is also affected as it has to have a level at least as
fine grained as the logger of Spooq, to be able to produce an output.
\end{sphinxadmonition}

\end{fulllineitems}

\index{get\_logging\_level() (in module spooq2.spooq2\_logger)@\spxentry{get\_logging\_level()}\spxextra{in module spooq2.spooq2\_logger}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{base_classes/spooq2_logger:spooq2.spooq2_logger.get_logging_level}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_logging\_level}}}{}{}
Returns the logging level depending on the environment variable \sphinxtitleref{SPOOQ\_ENV}.

\begin{sphinxadmonition}{note}{Note:}\begin{description}
\item[{If SPOOQ\_ENV is}] \leavevmode\begin{itemize}
\item {} 
\sphinxstylestrong{dev}        -\textgreater{} “DEBUG”

\item {} 
\sphinxstylestrong{test}       -\textgreater{} “ERROR”

\item {} 
something else -\textgreater{} “INFO”

\end{itemize}

\end{description}
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
Logging level

\item[{Return type}] \leavevmode
\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}

\end{description}\end{quote}

\end{fulllineitems}



\subsection{Extractor Base Class}
\label{\detokenize{base_classes/extractor:module-spooq2.extractor.extractor}}\label{\detokenize{base_classes/extractor:extractor-base-class}}\label{\detokenize{base_classes/extractor::doc}}\index{spooq2.extractor.extractor (module)@\spxentry{spooq2.extractor.extractor}\spxextra{module}}
Extractors are used to fetch, extract and convert a source data set into a PySpark DataFrame.
Exemplary extraction sources are \sphinxstylestrong{JSON Files} on file systems like HDFS, DBFS or EXT4
and relational database systems via \sphinxstylestrong{JDBC}.
\index{Extractor (class in spooq2.extractor.extractor)@\spxentry{Extractor}\spxextra{class in spooq2.extractor.extractor}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{base_classes/extractor:spooq2.extractor.extractor.Extractor}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{Extractor}}}
Bases: \sphinxhref{https://docs.python.org/3.7/library/functions.html\#object}{\sphinxcode{\sphinxupquote{object}}}

Base Class of Extractor Classes.
\index{name (Extractor attribute)@\spxentry{name}\spxextra{Extractor attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{base_classes/extractor:spooq2.extractor.extractor.Extractor.name}}\pysigline{\sphinxbfcode{\sphinxupquote{name}}}
Sets the \sphinxtitleref{\_\_name\_\_} of the class’ type as \sphinxtitleref{name}, which is essentially the Class’ Name.
\begin{quote}\begin{description}
\item[{Type}] \leavevmode
\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}

\end{description}\end{quote}

\end{fulllineitems}

\index{logger (Extractor attribute)@\spxentry{logger}\spxextra{Extractor attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{base_classes/extractor:spooq2.extractor.extractor.Extractor.logger}}\pysigline{\sphinxbfcode{\sphinxupquote{logger}}}
Shared, class level logger for all instances.
\begin{quote}\begin{description}
\item[{Type}] \leavevmode
\sphinxhref{https://docs.python.org/3.7/library/logging.html\#logging.Logger}{\sphinxcode{\sphinxupquote{logging.Logger}}}

\end{description}\end{quote}

\end{fulllineitems}

\index{extract() (Extractor method)@\spxentry{extract()}\spxextra{Extractor method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{base_classes/extractor:spooq2.extractor.extractor.Extractor.extract}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{extract}}}{}{}
Extracts Data from a Source and converts it into a PySpark DataFrame.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode


\item[{Return type}] \leavevmode
\sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.DataFrame}{\sphinxcode{\sphinxupquote{pyspark.sql.DataFrame}}}

\end{description}\end{quote}

\begin{sphinxadmonition}{note}{Note:}
This method does not take ANY input parameters. All needed parameters are defined
in the initialization of the Extractor Object.
\end{sphinxadmonition}

\end{fulllineitems}


\end{fulllineitems}



\subsubsection{Create your own Extractor}
\label{\detokenize{base_classes/extractor:create-your-own-extractor}}\label{\detokenize{base_classes/extractor:custom-extractor}}
Let your extractor class inherit from the extractor base class.
This includes the name, string representation and logger attributes from the superclass.

\begin{DUlineblock}{0em}
\item[] The only mandatory thing is to provide an \sphinxtitleref{extract()} method which
\item[] \sphinxstylestrong{takes}
\item[] =\textgreater{} \sphinxstyleemphasis{no input parameters}
\item[] and \sphinxstylestrong{returns} a
\item[] =\textgreater{} \sphinxstyleemphasis{PySpark DataFrame!}
\end{DUlineblock}

All configuration and parameterization should be done while initializing the class instance.

Here would be a simple example for a CSV Extractor:


\paragraph{Exemplary Sample Code}
\label{\detokenize{base_classes/extractor:exemplary-sample-code}}\sphinxSetupCaptionForVerbatim{src/spooq2/extractor/csv\_extractor.py:}
\def\sphinxLiteralBlockLabel{\label{\detokenize{base_classes/extractor:id1}}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{pyspark}\PYG{n+nn}{.}\PYG{n+nn}{sql} \PYG{k+kn}{import} \PYG{n}{SparkSession}

\PYG{k+kn}{from} \PYG{n+nn}{extractor} \PYG{k+kn}{import} \PYG{n}{Extractor}

\PYG{k}{class} \PYG{n+nc}{CSVExtractor}\PYG{p}{(}\PYG{n}{Extractor}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    This is a simplified example on how to implement a new extractor class.}
\PYG{l+s+sd}{    Please take your time to write proper docstrings as they are automatically}
\PYG{l+s+sd}{    parsed via Sphinx to build the HTML and PDF documentation.}
\PYG{l+s+sd}{    Docstrings use the style of Numpy (via the napoleon plug\PYGZhy{}in).}

\PYG{l+s+sd}{    This class uses the :meth:{}`pyspark.sql.DataFrameReader.csv{}` method internally.}

\PYG{l+s+sd}{    Examples}
\PYG{l+s+sd}{    \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{l+s+sd}{    extracted\PYGZus{}df = CSVExtractor(}
\PYG{l+s+sd}{        input\PYGZus{}file=\PYGZsq{}data/input\PYGZus{}data.csv\PYGZsq{}}
\PYG{l+s+sd}{    ).extract()}

\PYG{l+s+sd}{    Parameters}
\PYG{l+s+sd}{    \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{l+s+sd}{    input\PYGZus{}file: :any:{}`str{}`}
\PYG{l+s+sd}{        The explicit file path for the input data set. Globbing support depends}
\PYG{l+s+sd}{        on implementation of Spark\PYGZsq{}s csv reader!}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    Raises}
\PYG{l+s+sd}{    \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{l+s+sd}{    :any:{}`exceptions.TypeError{}`:}
\PYG{l+s+sd}{        path can be only string, list or RDD}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}

    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{input\PYGZus{}file}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb}{super}\PYG{p}{(}\PYG{n}{CSVExtractor}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{o}{.}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{input\PYGZus{}file} \PYG{o}{=} \PYG{n}{input\PYGZus{}file}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{spark} \PYG{o}{=} \PYG{n}{SparkSession}\PYG{o}{.}\PYG{n}{Builder}\PYG{p}{(}\PYG{p}{)}\PYGZbs{}
            \PYG{o}{.}\PYG{n}{enableHiveSupport}\PYG{p}{(}\PYG{p}{)}\PYGZbs{}
            \PYG{o}{.}\PYG{n}{appName}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{spooq2.extractor: }\PYG{l+s+si}{\PYGZob{}nm\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{nm}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{name}\PYG{p}{)}\PYG{p}{)}\PYGZbs{}
            \PYG{o}{.}\PYG{n}{getOrCreate}\PYG{p}{(}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{extract}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{logger}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Loading Raw CSV Files from: }\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{+} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{input\PYGZus{}file}\PYG{p}{)}
        \PYG{n}{output\PYGZus{}df} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{spark}\PYG{o}{.}\PYG{n}{read}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}
            \PYG{n}{input\PYGZus{}file}\PYG{p}{,}
            \PYG{n+nb}{format}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{csv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} 
            \PYG{n}{sep}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{;}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} 
            \PYG{n}{inferSchema}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{true}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} 
            \PYG{n}{header}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{true}\PYG{l+s+s2}{\PYGZdq{}}
        \PYG{p}{)}
        
        \PYG{k}{return} \PYG{n}{output\PYGZus{}df}
\end{sphinxVerbatim}


\paragraph{References to include}
\label{\detokenize{base_classes/extractor:references-to-include}}\sphinxSetupCaptionForVerbatim{src/spooq2/extractor/\_\_init\_\_.py:}
\def\sphinxLiteralBlockLabel{\label{\detokenize{base_classes/extractor:id2}}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gd}{\PYGZhy{}\PYGZhy{}\PYGZhy{} original }
\PYG{g+gi}{+++ adapted }
\PYG{g+gu}{@@ \PYGZhy{}1,8 +1,10 @@}
 from jdbc import JDBCExtractorIncremental, JDBCExtractorFullLoad
 from json\PYGZus{}files import JSONExtractor
\PYG{g+gi}{+from csv\PYGZus{}extractor import CSVExtractor}
 
 \PYGZus{}\PYGZus{}all\PYGZus{}\PYGZus{} = [
     \PYGZdq{}JDBCExtractorIncremental\PYGZdq{},
     \PYGZdq{}JDBCExtractorFullLoad\PYGZdq{},
     \PYGZdq{}JSONExtractor\PYGZdq{},
\PYG{g+gi}{+    \PYGZdq{}CSVExtractor\PYGZdq{},}
 ]
\end{sphinxVerbatim}


\paragraph{Tests}
\label{\detokenize{base_classes/extractor:tests}}
One of Spooq2’s features is to provide tested code for multiple data pipelines.
Please take your time to write sufficient unit tests!
You can reuse test data from \sphinxtitleref{tests/data} or create a new schema / data set if needed.
A SparkSession is provided as a global fixture called \sphinxtitleref{spark\_session}.
\sphinxSetupCaptionForVerbatim{tests/unit/extractor/test\_csv.py:}
\def\sphinxLiteralBlockLabel{\label{\detokenize{base_classes/extractor:id3}}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pytest}

\PYG{k+kn}{from} \PYG{n+nn}{spooq2}\PYG{n+nn}{.}\PYG{n+nn}{extractor} \PYG{k+kn}{import} \PYG{n}{CSVExtractor}

\PYG{n+nd}{@pytest}\PYG{o}{.}\PYG{n}{fixture}\PYG{p}{(}\PYG{p}{)}
\PYG{k}{def} \PYG{n+nf}{default\PYGZus{}extractor}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{CSVExtractor}\PYG{p}{(}\PYG{n}{input\PYGZus{}path}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{data/input\PYGZus{}data.csv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}


\PYG{k}{class} \PYG{n+nc}{TestBasicAttributes}\PYG{p}{(}\PYG{n+nb}{object}\PYG{p}{)}\PYG{p}{:}

    \PYG{k}{def} \PYG{n+nf}{test\PYGZus{}logger\PYGZus{}should\PYGZus{}be\PYGZus{}accessible}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{default\PYGZus{}extractor}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{assert} \PYG{n+nb}{hasattr}\PYG{p}{(}\PYG{n}{default\PYGZus{}extractor}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{logger}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{test\PYGZus{}name\PYGZus{}is\PYGZus{}set}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{default\PYGZus{}extractor}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{assert} \PYG{n}{default\PYGZus{}extractor}\PYG{o}{.}\PYG{n}{name} \PYG{o}{==} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{CSVExtractor}\PYG{l+s+s2}{\PYGZdq{}}

    \PYG{k}{def} \PYG{n+nf}{test\PYGZus{}str\PYGZus{}representation\PYGZus{}is\PYGZus{}correct}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{default\PYGZus{}extractor}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{assert} \PYG{n}{unicode}\PYG{p}{(}\PYG{n}{default\PYGZus{}extractor}\PYG{p}{)} \PYG{o}{==} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Extractor Object of Class CSVExtractor}\PYG{l+s+s2}{\PYGZdq{}}

\PYG{k}{class} \PYG{n+nc}{TestCSVExtraction}\PYG{p}{(}\PYG{n+nb}{object}\PYG{p}{)}\PYG{p}{:}

    \PYG{k}{def} \PYG{n+nf}{test\PYGZus{}count}\PYG{p}{(}\PYG{n}{default\PYGZus{}extractor}\PYG{p}{)}\PYG{p}{:}
        \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Converted DataFrame has the same count as the input data\PYGZdq{}\PYGZdq{}\PYGZdq{}}
        \PYG{n}{expected\PYGZus{}count} \PYG{o}{=} \PYG{l+m+mi}{312}
        \PYG{n}{actual\PYGZus{}count} \PYG{o}{=} \PYG{n}{default\PYGZus{}extractor}\PYG{o}{.}\PYG{n}{extract}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{count}\PYG{p}{(}\PYG{p}{)}
        \PYG{k}{assert} \PYG{n}{expected\PYGZus{}count} \PYG{o}{==} \PYG{n}{actual\PYGZus{}count}

    \PYG{k}{def} \PYG{n+nf}{test\PYGZus{}schema}\PYG{p}{(}\PYG{n}{default\PYGZus{}extractor}\PYG{p}{)}\PYG{p}{:}
        \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Converted DataFrame has the expected schema\PYGZdq{}\PYGZdq{}\PYGZdq{}}
        \PYG{n}{do\PYGZus{}some\PYGZus{}stuff}\PYG{p}{(}\PYG{p}{)}
        \PYG{k}{assert} \PYG{n}{expected} \PYG{o}{==} \PYG{n}{actual}
\end{sphinxVerbatim}


\paragraph{Documentation}
\label{\detokenize{base_classes/extractor:documentation}}
You need to create a \sphinxtitleref{rst} for your extractor
which needs to contain at minimum the \sphinxtitleref{automodule} or the \sphinxtitleref{autoclass} directive.
\sphinxSetupCaptionForVerbatim{docs/source/extractor/csv.rst:}
\def\sphinxLiteralBlockLabel{\label{\detokenize{base_classes/extractor:id4}}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gh}{CSV Extractor}
\PYG{g+gh}{=============}

Some text if you like...

\PYG{p}{..} \PYG{o+ow}{automodule}\PYG{p}{::} spooq2.extractor.csv\PYGZus{}extractor
\end{sphinxVerbatim}

To automatically include your new extractor in the HTML documentation you need to add it to a \sphinxtitleref{toctree} directive. Just refer to your newly created
\sphinxtitleref{csv.rst} file within the extractor overview page.
\sphinxSetupCaptionForVerbatim{docs/source/extractor/overview.rst:}
\def\sphinxLiteralBlockLabel{\label{\detokenize{base_classes/extractor:id5}}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gd}{\PYGZhy{}\PYGZhy{}\PYGZhy{} original }
\PYG{g+gi}{+++ adapted }
\PYG{g+gu}{@@ \PYGZhy{}7,8 +7,9 @@}
 .. toctree::
 
     json
     jdbc
\PYG{g+gi}{+    csv}
 
 Class Diagram of Extractor Subpackage
 \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
 .. uml:: ../diagrams/from\PYGZus{}thesis/class\PYGZus{}diagram/extractors.puml
\end{sphinxVerbatim}

That should be all!


\subsection{Transformer Base Class}
\label{\detokenize{base_classes/transformer:module-spooq2.transformer.transformer}}\label{\detokenize{base_classes/transformer:transformer-base-class}}\label{\detokenize{base_classes/transformer::doc}}\index{spooq2.transformer.transformer (module)@\spxentry{spooq2.transformer.transformer}\spxextra{module}}
Transformers take a \sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.DataFrame}{\sphinxcode{\sphinxupquote{pyspark.sql.DataFrame}}} as an input, transform it accordingly
and return a PySpark DataFrame.

Each Transformer class has to have a \sphinxtitleref{transform} method which takes no arguments
and returns a PySpark DataFrame.

Possible transformation methods can be \sphinxstylestrong{Selecting the most up to date record by id},
\sphinxstylestrong{Exploding an array}, \sphinxstylestrong{Filter (on an exploded array)}, \sphinxstylestrong{Apply basic threshold cleansing} or
\sphinxstylestrong{Map the incoming DataFrame to at provided structure}.
\index{Transformer (class in spooq2.transformer.transformer)@\spxentry{Transformer}\spxextra{class in spooq2.transformer.transformer}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{base_classes/transformer:spooq2.transformer.transformer.Transformer}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{Transformer}}}
Bases: \sphinxhref{https://docs.python.org/3.7/library/functions.html\#object}{\sphinxcode{\sphinxupquote{object}}}

Base Class of Transformer Classes.
\index{name (Transformer attribute)@\spxentry{name}\spxextra{Transformer attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{base_classes/transformer:spooq2.transformer.transformer.Transformer.name}}\pysigline{\sphinxbfcode{\sphinxupquote{name}}}
Sets the \sphinxtitleref{\_\_name\_\_} of the class’ type as \sphinxtitleref{name}, which is essentially the Class’ Name.
\begin{quote}\begin{description}
\item[{Type}] \leavevmode
\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}

\end{description}\end{quote}

\end{fulllineitems}

\index{logger (Transformer attribute)@\spxentry{logger}\spxextra{Transformer attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{base_classes/transformer:spooq2.transformer.transformer.Transformer.logger}}\pysigline{\sphinxbfcode{\sphinxupquote{logger}}}
Shared, class level logger for all instances.
\begin{quote}\begin{description}
\item[{Type}] \leavevmode
\sphinxhref{https://docs.python.org/3.7/library/logging.html\#logging.Logger}{\sphinxcode{\sphinxupquote{logging.Logger}}}

\end{description}\end{quote}

\end{fulllineitems}

\index{transform() (Transformer method)@\spxentry{transform()}\spxextra{Transformer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{base_classes/transformer:spooq2.transformer.transformer.Transformer.transform}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform}}}{\emph{input\_df}}{}
Performs a transformation on a DataFrame.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{input\_df}} (\sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.DataFrame}{\sphinxcode{\sphinxupquote{pyspark.sql.DataFrame}}}) \textendash{} Input DataFrame

\item[{Returns}] \leavevmode
Transformed DataFrame.

\item[{Return type}] \leavevmode
\sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.DataFrame}{\sphinxcode{\sphinxupquote{pyspark.sql.DataFrame}}}

\end{description}\end{quote}

\begin{sphinxadmonition}{note}{Note:}
This method does only take the Input DataFrame as a parameters. All other needed parameters
are defined in the initialization of the Transformator Object.
\end{sphinxadmonition}

\end{fulllineitems}


\end{fulllineitems}



\subsubsection{Create your own Transformer}
\label{\detokenize{base_classes/transformer:create-your-own-transformer}}\label{\detokenize{base_classes/transformer:custom-transformer}}
Let your transformer class inherit from the transformer base class.
This includes the name, string representation and logger attributes from the superclass.

\begin{DUlineblock}{0em}
\item[] The only mandatory thing is to provide a \sphinxtitleref{transform()} method which
\item[] \sphinxstylestrong{takes} a
\item[] =\textgreater{} \sphinxstyleemphasis{PySpark DataFrame!}
\item[] and \sphinxstylestrong{returns} a
\item[] =\textgreater{} \sphinxstyleemphasis{PySpark DataFrame!}
\end{DUlineblock}

All configuration and parameterization should be done while initializing the class instance.

Here would be a simple example for a transformer which drops records without an Id:


\paragraph{Exemplary Sample Code}
\label{\detokenize{base_classes/transformer:exemplary-sample-code}}\sphinxSetupCaptionForVerbatim{src/spooq2/transformer/no\_id\_dropper.py:}
\def\sphinxLiteralBlockLabel{\label{\detokenize{base_classes/transformer:id1}}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{transformer} \PYG{k+kn}{import} \PYG{n}{Transformer}

\PYG{k}{class} \PYG{n+nc}{NoIdDropper}\PYG{p}{(}\PYG{n}{Transformer}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    This is a simplified example on how to implement a new transformer class.}
\PYG{l+s+sd}{    Please take your time to write proper docstrings as they are automatically}
\PYG{l+s+sd}{    parsed via Sphinx to build the HTML and PDF documentation.}
\PYG{l+s+sd}{    Docstrings use the style of Numpy (via the napoleon plug\PYGZhy{}in).}

\PYG{l+s+sd}{    This class uses the :meth:{}`pyspark.sql.DataFrame.dropna{}` method internally.}

\PYG{l+s+sd}{    Examples}
\PYG{l+s+sd}{    \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{l+s+sd}{    input\PYGZus{}df = some\PYGZus{}extractor\PYGZus{}instance.extract()}
\PYG{l+s+sd}{    transformed\PYGZus{}df = NoIdDropper(}
\PYG{l+s+sd}{        id\PYGZus{}columns=\PYGZsq{}user\PYGZus{}id\PYGZsq{}}
\PYG{l+s+sd}{    ).transform(input\PYGZus{}df)}

\PYG{l+s+sd}{    Parameters}
\PYG{l+s+sd}{    \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{l+s+sd}{    id\PYGZus{}columns: :any:{}`str{}` or :any:{}`list{}`}
\PYG{l+s+sd}{        The name of the column containing the identifying Id values.}
\PYG{l+s+sd}{        Defaults to \PYGZdq{}id\PYGZdq{} }
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    Raises}
\PYG{l+s+sd}{    \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{l+s+sd}{    :any:{}`exceptions.ValueError{}`: }
\PYG{l+s+sd}{        \PYGZdq{}how (\PYGZsq{}\PYGZdq{} + how + \PYGZdq{}\PYGZsq{}) should be \PYGZsq{}any\PYGZsq{} or \PYGZsq{}all\PYGZsq{}\PYGZdq{}}
\PYG{l+s+sd}{    :any:{}`exceptions.ValueError{}`: }
\PYG{l+s+sd}{        \PYGZdq{}subset should be a list or tuple of column names\PYGZdq{}}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}

    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{id\PYGZus{}columns}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb}{super}\PYG{p}{(}\PYG{n}{NoIdDropper}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{o}{.}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{id\PYGZus{}columns} \PYG{o}{=} \PYG{n}{id\PYGZus{}columns}


    \PYG{k}{def} \PYG{n+nf}{transform}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{input\PYGZus{}df}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{logger}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Dropping records without an Id (columns to consider: }\PYG{l+s+si}{\PYGZob{}col\PYGZcb{}}\PYG{l+s+s2}{)}\PYG{l+s+s2}{\PYGZdq{}}
            \PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{col}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{id\PYGZus{}columns}\PYG{p}{)}\PYG{p}{)}
        \PYG{n}{output\PYGZus{}df} \PYG{o}{=} \PYG{n}{input\PYGZus{}df}\PYG{o}{.}\PYG{n}{dropna}\PYG{p}{(}
            \PYG{n}{how}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{all}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} 
            \PYG{n}{thresh}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,} 
            \PYG{n}{subset}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{id\PYGZus{}columns}
        \PYG{p}{)}
        
        \PYG{k}{return} \PYG{n}{output\PYGZus{}df}
\end{sphinxVerbatim}


\paragraph{References to include}
\label{\detokenize{base_classes/transformer:references-to-include}}
This makes it possible to import the new transformer class directly
from \sphinxtitleref{spooq2.transformer} instead of \sphinxtitleref{spooq2.transformer.no\_id\_dropper}.
It will also be imported if you use \sphinxtitleref{from spooq2.transformer import *}.
\sphinxSetupCaptionForVerbatim{src/spooq2/transformer/\_\_init\_\_.py:}
\def\sphinxLiteralBlockLabel{\label{\detokenize{base_classes/transformer:id2}}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gd}{\PYGZhy{}\PYGZhy{}\PYGZhy{} original }
\PYG{g+gi}{+++ adapted }
\PYG{g+gu}{@@ \PYGZhy{}1,13 +1,15 @@}
 from newest\PYGZus{}by\PYGZus{}group import NewestByGroup
 from mapper import Mapper
 from exploder import Exploder
 from threshold\PYGZus{}cleaner import ThresholdCleaner
 from sieve import Sieve
\PYG{g+gi}{+from no\PYGZus{}id\PYGZus{}dropper import NoIdDropper}
 
 \PYGZus{}\PYGZus{}all\PYGZus{}\PYGZus{} = [
     \PYGZdq{}NewestByGroup\PYGZdq{},
     \PYGZdq{}Mapper\PYGZdq{},
     \PYGZdq{}Exploder\PYGZdq{},
     \PYGZdq{}ThresholdCleaner\PYGZdq{},
     \PYGZdq{}Sieve\PYGZdq{},
\PYG{g+gi}{+    \PYGZdq{}NoIdDropper\PYGZdq{},}
 ]
\end{sphinxVerbatim}


\paragraph{Tests}
\label{\detokenize{base_classes/transformer:tests}}
One of Spooq2’s features is to provide tested code for multiple data pipelines.
Please take your time to write sufficient unit tests!
You can reuse test data from \sphinxtitleref{tests/data} or create a new schema / data set if needed.
A SparkSession is provided as a global fixture called \sphinxtitleref{spark\_session}.
\sphinxSetupCaptionForVerbatim{tests/unit/transformer/test\_no\_id\_dropper.py:}
\def\sphinxLiteralBlockLabel{\label{\detokenize{base_classes/transformer:id3}}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pytest}
\PYG{k+kn}{from} \PYG{n+nn}{pyspark}\PYG{n+nn}{.}\PYG{n+nn}{sql}\PYG{n+nn}{.}\PYG{n+nn}{dataframe} \PYG{k+kn}{import} \PYG{n}{DataFrame}

\PYG{k+kn}{from} \PYG{n+nn}{spooq2}\PYG{n+nn}{.}\PYG{n+nn}{transformer} \PYG{k+kn}{import} \PYG{n}{NoIdDropper}


\PYG{n+nd}{@pytest}\PYG{o}{.}\PYG{n}{fixture}\PYG{p}{(}\PYG{p}{)}
\PYG{k}{def} \PYG{n+nf}{default\PYGZus{}transformer}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{NoIdDropper}\PYG{p}{(}\PYG{n}{id\PYGZus{}columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{first\PYGZus{}name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{last\PYGZus{}name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}


\PYG{n+nd}{@pytest}\PYG{o}{.}\PYG{n}{fixture}\PYG{p}{(}\PYG{p}{)}
\PYG{k}{def} \PYG{n+nf}{input\PYGZus{}df}\PYG{p}{(}\PYG{n}{spark\PYGZus{}session}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{spark\PYGZus{}session}\PYG{o}{.}\PYG{n}{read}\PYG{o}{.}\PYG{n}{parquet}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{../data/schema\PYGZus{}v1/parquetFiles}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}


\PYG{n+nd}{@pytest}\PYG{o}{.}\PYG{n}{fixture}\PYG{p}{(}\PYG{p}{)}
\PYG{k}{def} \PYG{n+nf}{transformed\PYGZus{}df}\PYG{p}{(}\PYG{n}{default\PYGZus{}transformer}\PYG{p}{,} \PYG{n}{input\PYGZus{}df}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{default\PYGZus{}transformer}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{input\PYGZus{}df}\PYG{p}{)}


\PYG{k}{class} \PYG{n+nc}{TestBasicAttributes}\PYG{p}{(}\PYG{n+nb}{object}\PYG{p}{)}\PYG{p}{:}

    \PYG{k}{def} \PYG{n+nf}{test\PYGZus{}logger\PYGZus{}should\PYGZus{}be\PYGZus{}accessible}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{default\PYGZus{}transformer}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{assert} \PYG{n+nb}{hasattr}\PYG{p}{(}\PYG{n}{default\PYGZus{}transformer}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{logger}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{test\PYGZus{}name\PYGZus{}is\PYGZus{}set}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{default\PYGZus{}transformer}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{assert} \PYG{n}{default\PYGZus{}transformer}\PYG{o}{.}\PYG{n}{name} \PYG{o}{==} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{NoIdDropper}\PYG{l+s+s2}{\PYGZdq{}}

    \PYG{k}{def} \PYG{n+nf}{test\PYGZus{}str\PYGZus{}representation\PYGZus{}is\PYGZus{}correct}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{default\PYGZus{}transformer}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{assert} \PYG{n}{unicode}\PYG{p}{(}\PYG{n}{default\PYGZus{}transformer}\PYG{p}{)} \PYG{o}{==} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Transformer Object of Class NoIdDropper}\PYG{l+s+s2}{\PYGZdq{}}

\PYG{k}{class} \PYG{n+nc}{TestNoIdDropper}\PYG{p}{(}\PYG{n+nb}{object}\PYG{p}{)}\PYG{p}{:}

    \PYG{k}{def} \PYG{n+nf}{test\PYGZus{}records\PYGZus{}are\PYGZus{}dropped}\PYG{p}{(}\PYG{n}{transformed\PYGZus{}df}\PYG{p}{,} \PYG{n}{input\PYGZus{}df}\PYG{p}{)}\PYG{p}{:}
        \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Transformed DataFrame has no records with missing first\PYGZus{}name and last\PYGZus{}name\PYGZdq{}\PYGZdq{}\PYGZdq{}}
        \PYG{k}{assert} \PYG{n}{input\PYGZus{}df}\PYG{o}{.}\PYG{n}{where}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{first\PYGZus{}name is null or last\PYGZus{}name is null}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{count}\PYG{p}{(}\PYG{p}{)} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{0}
        \PYG{k}{assert} \PYG{n}{transformed\PYGZus{}df}\PYG{o}{.}\PYG{n}{where}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{first\PYGZus{}name is null or last\PYGZus{}name is null}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{count}\PYG{p}{(}\PYG{p}{)} \PYG{o}{==} \PYG{l+m+mi}{0}

    \PYG{k}{def} \PYG{n+nf}{test\PYGZus{}schema\PYGZus{}is\PYGZus{}unchanged}\PYG{p}{(}\PYG{n}{transformed\PYGZus{}df}\PYG{p}{,} \PYG{n}{input\PYGZus{}df}\PYG{p}{)}\PYG{p}{:}
        \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Converted DataFrame has the expected schema\PYGZdq{}\PYGZdq{}\PYGZdq{}}
        \PYG{k}{assert} \PYG{n}{transformed\PYGZus{}df}\PYG{o}{.}\PYG{n}{schema} \PYG{o}{==} \PYG{n}{input\PYGZus{}df}\PYG{o}{.}\PYG{n}{schema}
\end{sphinxVerbatim}


\paragraph{Documentation}
\label{\detokenize{base_classes/transformer:documentation}}
You need to create a \sphinxtitleref{rst} for your transformer
which needs to contain at minimum the \sphinxtitleref{automodule} or the \sphinxtitleref{autoclass} directive.
\sphinxSetupCaptionForVerbatim{docs/source/transformer/no\_id\_dropper.rst:}
\def\sphinxLiteralBlockLabel{\label{\detokenize{base_classes/transformer:id4}}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gh}{Record Dropper if Id is missing}
\PYG{g+gh}{===============================}

Some text if you like...

\PYG{p}{..} \PYG{o+ow}{automodule}\PYG{p}{::} spooq2.transformer.no\PYGZus{}id\PYGZus{}dropper
\end{sphinxVerbatim}

To automatically include your new transformer in the HTML / PDF documentation
you need to add it to a \sphinxtitleref{toctree} directive. Just refer to your newly created
\sphinxtitleref{no\_id\_dropper.rst} file within the transformer overview page.
\sphinxSetupCaptionForVerbatim{docs/source/transformer/overview.rst:}
\def\sphinxLiteralBlockLabel{\label{\detokenize{base_classes/transformer:id5}}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gd}{\PYGZhy{}\PYGZhy{}\PYGZhy{} original}
\PYG{g+gi}{+++ adapted}
\PYG{g+gu}{@@ \PYGZhy{}7,14 +7,15 @@}
 .. toctree::

     exploder
     sieve
     mapper
     threshold\PYGZus{}cleaner
     newest\PYGZus{}by\PYGZus{}group
\PYG{g+gi}{+    no\PYGZus{}id\PYGZus{}dropper}

 Class Diagram of Transformer Subpackage
 \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
 .. uml:: ../diagrams/from\PYGZus{}thesis/class\PYGZus{}diagram/transformers.puml
\end{sphinxVerbatim}

That should be it!


\subsection{Loader Base Class}
\label{\detokenize{base_classes/loader:module-spooq2.loader.loader}}\label{\detokenize{base_classes/loader:loader-base-class}}\label{\detokenize{base_classes/loader::doc}}\index{spooq2.loader.loader (module)@\spxentry{spooq2.loader.loader}\spxextra{module}}
Loaders take a \sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.DataFrame}{\sphinxcode{\sphinxupquote{pyspark.sql.DataFrame}}} as an input and save it to a sink.

Each Loader class has to have a \sphinxtitleref{load} method which takes a DataFrame as single paremter.

Possible Loader sinks can be \sphinxstylestrong{Hive Tables}, \sphinxstylestrong{Kudu Tables}, \sphinxstylestrong{HBase Tables}, \sphinxstylestrong{JDBC
Sinks} or \sphinxstylestrong{ParquetFiles}.
\index{Loader (class in spooq2.loader.loader)@\spxentry{Loader}\spxextra{class in spooq2.loader.loader}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{base_classes/loader:spooq2.loader.loader.Loader}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{Loader}}}
Bases: \sphinxhref{https://docs.python.org/3.7/library/functions.html\#object}{\sphinxcode{\sphinxupquote{object}}}

Base Class of Loader Objects.
\index{name (Loader attribute)@\spxentry{name}\spxextra{Loader attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{base_classes/loader:spooq2.loader.loader.Loader.name}}\pysigline{\sphinxbfcode{\sphinxupquote{name}}}
Sets the \sphinxtitleref{\_\_name\_\_} of the class’ type as \sphinxtitleref{name}, which is essentially the Class’ Name.
\begin{quote}\begin{description}
\item[{Type}] \leavevmode
\sphinxhref{https://docs.python.org/3.7/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}

\end{description}\end{quote}

\end{fulllineitems}

\index{logger (Loader attribute)@\spxentry{logger}\spxextra{Loader attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{base_classes/loader:spooq2.loader.loader.Loader.logger}}\pysigline{\sphinxbfcode{\sphinxupquote{logger}}}
Shared, class level logger for all instances.
\begin{quote}\begin{description}
\item[{Type}] \leavevmode
\sphinxhref{https://docs.python.org/3.7/library/logging.html\#logging.Logger}{\sphinxcode{\sphinxupquote{logging.Logger}}}

\end{description}\end{quote}

\end{fulllineitems}

\index{load() (Loader method)@\spxentry{load()}\spxextra{Loader method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{base_classes/loader:spooq2.loader.loader.Loader.load}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{load}}}{\emph{input\_df}}{}
Persists data from a PySpark DataFrame to a target table.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{input\_df}} (\sphinxhref{https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html\#pyspark.sql.DataFrame}{\sphinxcode{\sphinxupquote{pyspark.sql.DataFrame}}}) \textendash{} Input DataFrame which has to be loaded to a target destination.

\end{description}\end{quote}

\begin{sphinxadmonition}{note}{Note:}
This method takes only a single DataFrame as an input parameter. All other needed
parameters are defined in the initialization of the Loader object.
\end{sphinxadmonition}

\end{fulllineitems}


\end{fulllineitems}



\subsubsection{Create your own Loader}
\label{\detokenize{base_classes/loader:create-your-own-loader}}\label{\detokenize{base_classes/loader:custom-loader}}
Let your loader class inherit from the loader base class.
This includes the name, string representation and logger attributes from the superclass.

\begin{DUlineblock}{0em}
\item[] The only mandatory thing is to provide a \sphinxtitleref{load()} method which
\item[] \sphinxstylestrong{takes} a
\item[] =\textgreater{} \sphinxstyleemphasis{PySpark DataFrame!}
\item[] and \sphinxstylestrong{returns}
\item[] \sphinxstyleemphasis{nothing} (or at least the API does not expect anything)
\end{DUlineblock}

All configuration and parameterization should be done while initializing the class instance.

Here would be a simple example for a loader which save a DataFrame to parquet files:


\paragraph{Exemplary Sample Code}
\label{\detokenize{base_classes/loader:exemplary-sample-code}}\sphinxSetupCaptionForVerbatim{src/spooq2/loader/parquet.py:}
\def\sphinxLiteralBlockLabel{\label{\detokenize{base_classes/loader:id1}}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{pyspark}\PYG{n+nn}{.}\PYG{n+nn}{sql} \PYG{k+kn}{import} \PYG{n}{functions} \PYG{k}{as} \PYG{n}{F}

\PYG{k+kn}{from} \PYG{n+nn}{loader} \PYG{k+kn}{import} \PYG{n}{Loader}

\PYG{k}{class} \PYG{n+nc}{ParquetLoader}\PYG{p}{(}\PYG{n}{loader}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    This is a simplified example on how to implement a new loader class.}
\PYG{l+s+sd}{    Please take your time to write proper docstrings as they are automatically}
\PYG{l+s+sd}{    parsed via Sphinx to build the HTML and PDF documentation.}
\PYG{l+s+sd}{    Docstrings use the style of Numpy (via the napoleon plug\PYGZhy{}in).}

\PYG{l+s+sd}{    This class uses the :meth:{}`pyspark.sql.DataFrameWriter.parquet{}` method internally.}

\PYG{l+s+sd}{    Examples}
\PYG{l+s+sd}{    \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{l+s+sd}{    input\PYGZus{}df = some\PYGZus{}extractor\PYGZus{}instance.extract()}
\PYG{l+s+sd}{    output\PYGZus{}df = some\PYGZus{}transformer\PYGZus{}instance.transform(input\PYGZus{}df)}
\PYG{l+s+sd}{    ParquetLoader(}
\PYG{l+s+sd}{        path=\PYGZdq{}data/parquet\PYGZus{}files\PYGZdq{},}
\PYG{l+s+sd}{        partition\PYGZus{}by=\PYGZdq{}dt\PYGZdq{},}
\PYG{l+s+sd}{        explicit\PYGZus{}partition\PYGZus{}values=20200201,}
\PYG{l+s+sd}{        compression=\PYGZdq{}\PYGZdq{}gzip\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    ).load(output\PYGZus{}df)}

\PYG{l+s+sd}{    Parameters}
\PYG{l+s+sd}{    \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{l+s+sd}{    path: :any:{}`str{}`}
\PYG{l+s+sd}{        The path to where the loader persists the output parquet files.}
\PYG{l+s+sd}{        If partitioning is set, this will be the base path where the partitions }
\PYG{l+s+sd}{        are stored.}

\PYG{l+s+sd}{    partition\PYGZus{}by: :any:{}`str{}` or :any:{}`list{}` of (:any:{}`str{}`)}
\PYG{l+s+sd}{        The column name or names by which the output should be partitioned.}
\PYG{l+s+sd}{        If the partition\PYGZus{}by parameter is set to None, no partitioning will be }
\PYG{l+s+sd}{        performed.}
\PYG{l+s+sd}{        Defaults to \PYGZdq{}dt\PYGZdq{}}

\PYG{l+s+sd}{    explicit\PYGZus{}partition\PYGZus{}values: :any:{}`str{}` or :any:{}`int{}` }
\PYG{l+s+sd}{                                or :any:{}`list{}` of (:any:{}`str{}` and :any:{}`int{}`)}
\PYG{l+s+sd}{        Only allowed if partition\PYGZus{}by is not None.}
\PYG{l+s+sd}{        If explicit\PYGZus{}partition\PYGZus{}values is not None, the dataframe will}
\PYG{l+s+sd}{            * overwrite the partition\PYGZus{}by columns values if it already exists or}
\PYG{l+s+sd}{            * create and fill the partition\PYGZus{}by columns if they do not yet exist}
\PYG{l+s+sd}{        Defaults to None}

\PYG{l+s+sd}{    compression: :any:{}`str{}`}
\PYG{l+s+sd}{        The compression codec used for the parquet output files.}
\PYG{l+s+sd}{        Defaults to \PYGZdq{}snappy\PYGZdq{}}

\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    Raises}
\PYG{l+s+sd}{    \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{l+s+sd}{    :any:{}`exceptions.AssertionError{}`:}
\PYG{l+s+sd}{        explicit\PYGZus{}partition\PYGZus{}values can only be used when partition\PYGZus{}by is not None}
\PYG{l+s+sd}{    :any:{}`exceptions.AssertionError{}`:}
\PYG{l+s+sd}{        explicit\PYGZus{}partition\PYGZus{}values and partition\PYGZus{}by must have the same length}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}

    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{path}\PYG{p}{,} \PYG{n}{partition\PYGZus{}by}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{dt}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{explicit\PYGZus{}partition\PYGZus{}values}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,} \PYG{n}{compression\PYGZus{}codec}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{snappy}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb}{super}\PYG{p}{(}\PYG{n}{ParquetLoader}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{o}{.}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{path} \PYG{o}{=} \PYG{n}{path}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{partition\PYGZus{}by} \PYG{o}{=} \PYG{n}{partition\PYGZus{}by}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{explicit\PYGZus{}partition\PYGZus{}values} \PYG{o}{=} \PYG{n}{explicit\PYGZus{}partition\PYGZus{}values}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{compression\PYGZus{}codec} \PYG{o}{=} \PYG{n}{compression\PYGZus{}codec}
        \PYG{k}{if} \PYG{n}{explicit\PYGZus{}partition\PYGZus{}values} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
            \PYG{k}{assert} \PYG{p}{(}\PYG{n}{partition\PYGZus{}by} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{,}
                \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{explicit\PYGZus{}partition\PYGZus{}values can only be used when partition\PYGZus{}by is not None}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
            \PYG{k}{assert} \PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{partition\PYGZus{}by}\PYG{p}{)} \PYG{o}{==} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{explicit\PYGZus{}partition\PYGZus{}values}\PYG{p}{)}\PYG{p}{,}
                \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{explicit\PYGZus{}partition\PYGZus{}values and partition\PYGZus{}by must have the same length}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{load}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{input\PYGZus{}df}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{logger}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Persisting DataFrame as Parquet Files to }\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{+} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{path}\PYG{p}{)}

        \PYG{k}{if} \PYG{n+nb}{isinstance}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{explicit\PYGZus{}partition\PYGZus{}values}\PYG{p}{,} \PYG{n+nb}{list}\PYG{p}{)}\PYG{p}{:}
            \PYG{k}{for} \PYG{p}{(}\PYG{n}{k}\PYG{p}{,} \PYG{n}{v}\PYG{p}{)} \PYG{o+ow}{in} \PYG{n+nb}{zip}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{partition\PYGZus{}by}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{explicit\PYGZus{}partition\PYGZus{}values}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{input\PYGZus{}df} \PYG{o}{=} \PYG{n}{input\PYGZus{}df}\PYG{o}{.}\PYG{n}{withColumn}\PYG{p}{(}\PYG{n}{k}\PYG{p}{,} \PYG{n}{F}\PYG{o}{.}\PYG{n}{lit}\PYG{p}{(}\PYG{n}{v}\PYG{p}{)}\PYG{p}{)}
        \PYG{k}{elif} \PYG{n+nb}{isinstance}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{explicit\PYGZus{}partition\PYGZus{}values}\PYG{p}{,} \PYG{n}{basestring}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{input\PYGZus{}df} \PYG{o}{=} \PYG{n}{input\PYGZus{}df}\PYG{o}{.}\PYG{n}{withColumn}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{partition\PYGZus{}by}\PYG{p}{,} \PYG{n}{F}\PYG{o}{.}\PYG{n}{lit}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{explicit\PYGZus{}partition\PYGZus{}values}\PYG{p}{)}\PYG{p}{)}

        \PYG{n}{input\PYGZus{}df}\PYG{o}{.}\PYG{n}{write}\PYG{o}{.}\PYG{n}{parquet}\PYG{p}{(}
            \PYG{n}{path}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{path}\PYG{p}{,}
            \PYG{n}{partitionBy}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{partition\PYGZus{}by}\PYG{p}{,}
            \PYG{n}{compression}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{compression\PYGZus{}codec}
        \PYG{p}{)}
\end{sphinxVerbatim}


\paragraph{References to include}
\label{\detokenize{base_classes/loader:references-to-include}}
This makes it possible to import the new loader class directly
from \sphinxtitleref{spooq2.loader} instead of \sphinxtitleref{spooq2.loader.parquet}.
It will also be imported if you use \sphinxtitleref{from spooq2.loader import *}.
\sphinxSetupCaptionForVerbatim{src/spooq2/loader/\_\_init\_\_.py:}
\def\sphinxLiteralBlockLabel{\label{\detokenize{base_classes/loader:id2}}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gd}{\PYGZhy{}\PYGZhy{}\PYGZhy{} original }
\PYG{g+gi}{+++ adapted }
\PYG{g+gu}{@@ \PYGZhy{}1,7 +1,9 @@}
 from loader import Loader
 from hive\PYGZus{}loader import HiveLoader
\PYG{g+gi}{+from parquet import ParquetLoader}
 
 \PYGZus{}\PYGZus{}all\PYGZus{}\PYGZus{} = [
     \PYGZdq{}Loader\PYGZdq{},
     \PYGZdq{}HiveLoader\PYGZdq{},
\PYG{g+gi}{+    \PYGZdq{}ParquetLoader\PYGZdq{},}
 ]
\end{sphinxVerbatim}


\paragraph{Tests}
\label{\detokenize{base_classes/loader:tests}}
One of Spooq2’s features is to provide tested code for multiple data pipelines.
Please take your time to write sufficient unit tests!
You can reuse test data from \sphinxtitleref{tests/data} or create a new schema / data set if needed.
A SparkSession is provided as a global fixture called \sphinxtitleref{spark\_session}.
\sphinxSetupCaptionForVerbatim{tests/unit/loader/test\_parquet.py:}
\def\sphinxLiteralBlockLabel{\label{\detokenize{base_classes/loader:id3}}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pytest}
\PYG{k+kn}{from} \PYG{n+nn}{pyspark}\PYG{n+nn}{.}\PYG{n+nn}{sql}\PYG{n+nn}{.}\PYG{n+nn}{dataframe} \PYG{k+kn}{import} \PYG{n}{DataFrame}

\PYG{k+kn}{from} \PYG{n+nn}{spooq2}\PYG{n+nn}{.}\PYG{n+nn}{loader} \PYG{k+kn}{import} \PYG{n}{ParquetLoader}


\PYG{n+nd}{@pytest}\PYG{o}{.}\PYG{n}{fixture}\PYG{p}{(}\PYG{n}{scope}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{module}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{k}{def} \PYG{n+nf}{output\PYGZus{}path}\PYG{p}{(}\PYG{n}{tmpdir\PYGZus{}factory}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{tmpdir\PYGZus{}factory}\PYG{o}{.}\PYG{n}{mktemp}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{parquet\PYGZus{}output}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{)}


\PYG{n+nd}{@pytest}\PYG{o}{.}\PYG{n}{fixture}\PYG{p}{(}\PYG{n}{scope}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{module}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{k}{def} \PYG{n+nf}{default\PYGZus{}loader}\PYG{p}{(}\PYG{n}{output\PYGZus{}path}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{ParquetLoader}\PYG{p}{(}
        \PYG{n}{path}\PYG{o}{=}\PYG{n}{output\PYGZus{}path}\PYG{p}{,}
        \PYG{n}{partition\PYGZus{}by}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{attributes.gender}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
        \PYG{n}{explicit\PYGZus{}partition\PYGZus{}values}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,}
        \PYG{n}{compression\PYGZus{}codec}\PYG{o}{=}\PYG{k+kc}{None}
    \PYG{p}{)}


\PYG{n+nd}{@pytest}\PYG{o}{.}\PYG{n}{fixture}\PYG{p}{(}\PYG{n}{scope}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{module}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{k}{def} \PYG{n+nf}{input\PYGZus{}df}\PYG{p}{(}\PYG{n}{spark\PYGZus{}session}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{spark\PYGZus{}session}\PYG{o}{.}\PYG{n}{read}\PYG{o}{.}\PYG{n}{parquet}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{../data/schema\PYGZus{}v1/parquetFiles}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}


\PYG{n+nd}{@pytest}\PYG{o}{.}\PYG{n}{fixture}\PYG{p}{(}\PYG{n}{scope}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{module}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{k}{def} \PYG{n+nf}{loaded\PYGZus{}df}\PYG{p}{(}\PYG{n}{default\PYGZus{}loader}\PYG{p}{,} \PYG{n}{input\PYGZus{}df}\PYG{p}{,} \PYG{n}{spark\PYGZus{}session}\PYG{p}{,} \PYG{n}{output\PYGZus{}path}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{default\PYGZus{}loader}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{input\PYGZus{}df}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{spark\PYGZus{}session}\PYG{o}{.}\PYG{n}{read}\PYG{o}{.}\PYG{n}{parquet}\PYG{p}{(}\PYG{n}{output\PYGZus{}path}\PYG{p}{)}


\PYG{k}{class} \PYG{n+nc}{TestBasicAttributes}\PYG{p}{(}\PYG{n+nb}{object}\PYG{p}{)}\PYG{p}{:}

    \PYG{k}{def} \PYG{n+nf}{test\PYGZus{}logger\PYGZus{}should\PYGZus{}be\PYGZus{}accessible}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{default\PYGZus{}loader}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{assert} \PYG{n+nb}{hasattr}\PYG{p}{(}\PYG{n}{default\PYGZus{}loader}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{logger}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{test\PYGZus{}name\PYGZus{}is\PYGZus{}set}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{default\PYGZus{}loader}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{assert} \PYG{n}{default\PYGZus{}loader}\PYG{o}{.}\PYG{n}{name} \PYG{o}{==} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ParquetLoader}\PYG{l+s+s2}{\PYGZdq{}}

    \PYG{k}{def} \PYG{n+nf}{test\PYGZus{}str\PYGZus{}representation\PYGZus{}is\PYGZus{}correct}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{default\PYGZus{}loader}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{assert} \PYG{n}{unicode}\PYG{p}{(}\PYG{n}{default\PYGZus{}loader}\PYG{p}{)} \PYG{o}{==} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{loader Object of Class ParquetLoader}\PYG{l+s+s2}{\PYGZdq{}}

\PYG{k}{class} \PYG{n+nc}{TestParquetLoader}\PYG{p}{(}\PYG{n+nb}{object}\PYG{p}{)}\PYG{p}{:}

    \PYG{k}{def} \PYG{n+nf}{test\PYGZus{}count\PYGZus{}did\PYGZus{}not\PYGZus{}change}\PYG{p}{(}\PYG{n}{loaded\PYGZus{}df}\PYG{p}{,} \PYG{n}{input\PYGZus{}df}\PYG{p}{)}\PYG{p}{:}
        \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Persisted DataFrame has the same number of records than the input DataFrame\PYGZdq{}\PYGZdq{}\PYGZdq{}}
        \PYG{k}{assert} \PYG{n}{input\PYGZus{}df}\PYG{o}{.}\PYG{n}{count}\PYG{p}{(}\PYG{p}{)} \PYG{o}{==} \PYG{n}{output\PYGZus{}df}\PYG{o}{.}\PYG{n}{count}\PYG{p}{(}\PYG{p}{)} \PYG{o+ow}{and} \PYG{n}{input\PYGZus{}df}\PYG{o}{.}\PYG{n}{count}\PYG{p}{(}\PYG{p}{)} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{0}

    \PYG{k}{def} \PYG{n+nf}{test\PYGZus{}schema\PYGZus{}is\PYGZus{}unchanged}\PYG{p}{(}\PYG{n}{loaded\PYGZus{}df}\PYG{p}{,} \PYG{n}{input\PYGZus{}df}\PYG{p}{)}\PYG{p}{:}
        \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Loaded DataFrame has the same schema as the input DataFrame\PYGZdq{}\PYGZdq{}\PYGZdq{}}
        \PYG{k}{assert} \PYG{n}{loaded}\PYG{o}{.}\PYG{n}{schema} \PYG{o}{==} \PYG{n}{input\PYGZus{}df}\PYG{o}{.}\PYG{n}{schema}
\end{sphinxVerbatim}


\paragraph{Documentation}
\label{\detokenize{base_classes/loader:documentation}}
You need to create a \sphinxtitleref{rst} for your loader
which needs to contain at minimum the \sphinxtitleref{automodule} or the \sphinxtitleref{autoclass} directive.
\sphinxSetupCaptionForVerbatim{docs/source/loader/parquet.rst:}
\def\sphinxLiteralBlockLabel{\label{\detokenize{base_classes/loader:id4}}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gh}{Parquet Loader}
\PYG{g+gh}{===============================}

Some text if you like...

\PYG{p}{..} \PYG{o+ow}{automodule}\PYG{p}{::} spooq2.loader.parquet
\end{sphinxVerbatim}

To automatically include your new loader in the HTML / PDF documentation
you need to add it to a \sphinxtitleref{toctree} directive. Just refer to your newly created
\sphinxtitleref{parquet.rst} file within the loader overview page.
\sphinxSetupCaptionForVerbatim{docs/source/loader/overview.rst:}
\def\sphinxLiteralBlockLabel{\label{\detokenize{base_classes/loader:id5}}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gd}{\PYGZhy{}\PYGZhy{}\PYGZhy{} original }
\PYG{g+gi}{+++ adapted }
\PYG{g+gu}{@@ \PYGZhy{}7,4 +7,5 @@}
 .. toctree::
     hive\PYGZus{}loader
\PYG{g+gi}{+    parquet}
 
 Class Diagram of Loader Subpackage
\end{sphinxVerbatim}

That should be it!


\section{Setup for Development, Testing, Documenting}
\label{\detokenize{setup_development_testing:setup-for-development-testing-documenting}}\label{\detokenize{setup_development_testing:dev-setup}}\label{\detokenize{setup_development_testing::doc}}
\sphinxstylestrong{Attention: The current version of Spooq is designed (and tested) for Python 2.7/3.7/3.8 on ubuntu, manjaro linux and WSL2 (Windows Subsystem Linux).}


\subsection{Prerequisites}
\label{\detokenize{setup_development_testing:prerequisites}}\begin{itemize}
\item {} 
python 2.7 or python 3.7/3.8

\item {} 
Java 8 (jdk8-openjdk)

\item {} 
pipenv

\item {} 
Latex (for PDF documentation)

\end{itemize}


\subsection{Setting up the Environment}
\label{\detokenize{setup_development_testing:setting-up-the-environment}}
The requirements are stored in the file \sphinxtitleref{Pipfile} separated for production and development packages.

To install the packages needed for development and testing run the following command:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} pipenv install \PYGZhy{}\PYGZhy{}dev
\end{sphinxVerbatim}

This will create a virtual environment in \sphinxtitleref{\textasciitilde{}/.local/share/virtualenvs}.

If you want to have your virtual environment installed as a sub-folder (.venv) you have to set the
environment variable \sphinxtitleref{PIPENV\_VENV\_IN\_PROJECT} to 1.

To remove a virtual environment created with pipenv just change in the folder where you created it
and execute \sphinxtitleref{pipenv \textendash{}rm}.


\subsection{Activate the Virtual Environment}
\label{\detokenize{setup_development_testing:activate-the-virtual-environment}}\sphinxSetupCaptionForVerbatim{To activate the virtual environment enter:}
\def\sphinxLiteralBlockLabel{\label{\detokenize{setup_development_testing:id10}}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} pipenv shell
\end{sphinxVerbatim}
\sphinxSetupCaptionForVerbatim{To deactivate the virtual environment simply enter:}
\def\sphinxLiteralBlockLabel{\label{\detokenize{setup_development_testing:id11}}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} \PYG{n+nb}{exit}
\PYG{c+c1}{\PYGZsh{} or close the shell}
\end{sphinxVerbatim}

For more commands of pipenv call \sphinxtitleref{pipenv -h}.


\subsection{Creating Your Own Components}
\label{\detokenize{setup_development_testing:creating-your-own-components}}
Implementing new extractors, transformers, or loaders is fairly straightforward.
Please refer to following descriptions and examples to get an idea:
\begin{itemize}
\item {} 
{\hyperref[\detokenize{base_classes/extractor:custom-extractor}]{\sphinxcrossref{\DUrole{std,std-ref}{Create your own Extractor}}}}

\item {} 
{\hyperref[\detokenize{base_classes/transformer:custom-transformer}]{\sphinxcrossref{\DUrole{std,std-ref}{Create your own Transformer}}}}

\item {} 
{\hyperref[\detokenize{base_classes/loader:custom-loader}]{\sphinxcrossref{\DUrole{std,std-ref}{Create your own Loader}}}}

\end{itemize}


\subsection{Running Tests}
\label{\detokenize{setup_development_testing:running-tests}}
The tests are implemented with the \sphinxhref{https://docs.pytest.org/en/3.10.1/}{pytest} framework.
\sphinxSetupCaptionForVerbatim{Start all tests:}
\def\sphinxLiteralBlockLabel{\label{\detokenize{setup_development_testing:id12}}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} pipenv shell
\PYGZdl{} \PYG{n+nb}{cd} tests
\PYGZdl{} pytest
\end{sphinxVerbatim}


\subsubsection{Test Plugins}
\label{\detokenize{setup_development_testing:test-plugins}}
Those are the most useful plugins automatically used:


\paragraph{html}
\label{\detokenize{setup_development_testing:id1}}\sphinxSetupCaptionForVerbatim{Generate an HTML report for the test results:}
\def\sphinxLiteralBlockLabel{\label{\detokenize{setup_development_testing:id13}}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} pytest \PYGZhy{}\PYGZhy{}html\PYG{o}{=}report.html
\end{sphinxVerbatim}


\paragraph{random-order}
\label{\detokenize{setup_development_testing:id2}}
Shuffles the order of execution for the tests to avoid / discover dependencies of the tests.

Randomization is set by a seed number. To re-test the same order of execution where you found
an error, just set the seed value to the same as for the failing test.
To temporarily disable this feature run with \sphinxtitleref{pytest -p no:random-order -v}


\paragraph{cov}
\label{\detokenize{setup_development_testing:id3}}
Generates an HTML for the test coverage
\sphinxSetupCaptionForVerbatim{Get a test coverage report in the terminal:}
\def\sphinxLiteralBlockLabel{\label{\detokenize{setup_development_testing:id14}}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} pytest \PYGZhy{}\PYGZhy{}cov\PYGZhy{}report term \PYGZhy{}\PYGZhy{}cov\PYG{o}{=}spooq2
\end{sphinxVerbatim}
\sphinxSetupCaptionForVerbatim{Get the test coverage report as HTML}
\def\sphinxLiteralBlockLabel{\label{\detokenize{setup_development_testing:id15}}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} pytest \PYGZhy{}\PYGZhy{}cov\PYGZhy{}report html:cov\PYGZus{}html \PYGZhy{}\PYGZhy{}cov\PYG{o}{=}spooq2
\end{sphinxVerbatim}


\paragraph{ipdb}
\label{\detokenize{setup_development_testing:id4}}\begin{description}
\item[{To use ipdb (IPython Debugger) add following code at your breakpoint::}] \leavevmode
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{import} \PYG{n+nn}{ipdb}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{ipdb}\PYG{o}{.}\PYG{n}{set\PYGZus{}trace}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{description}

You have to start pytest with \sphinxtitleref{-s} if you want to use interactive debugger.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} pytest \PYGZhy{}s
\end{sphinxVerbatim}


\subsection{Generate Documentation}
\label{\detokenize{setup_development_testing:generate-documentation}}
This project uses \sphinxhref{https://www.sphinx-doc.org/en/1.8/}{Sphinx} for creating its documentation.
Graphs and diagrams are produced with PlantUML.

The main documentation content is defined as docstrings within the source code.
To view the current documentation open \sphinxtitleref{docs/build/html/index.html}
or \sphinxtitleref{docs/build/latex/spooq2.pdf} in your application of choice.
There are symlinks in the root folder for symplicity:
\begin{itemize}
\item {} 
Documentation.html

\item {} 
Documentation.pdf

\end{itemize}

Although, if you are reading this, you have probably already found the documentation…


\subsubsection{Diagrams}
\label{\detokenize{setup_development_testing:diagrams}}
For generating the graphs and diagrams, you need a working plantuml installation
on your computer! Please refer to \sphinxhref{https://pypi.org/project/sphinxcontrib-plantuml/}{sphinxcontrib-plantuml}.


\subsubsection{HTML}
\label{\detokenize{setup_development_testing:id5}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} \PYG{n+nb}{cd} docs
\PYGZdl{} make html
\PYGZdl{} chromium build/html/index.html
\end{sphinxVerbatim}


\subsubsection{PDF}
\label{\detokenize{setup_development_testing:pdf}}
For generating documentation in the PDF format you need to have a working (pdf)latex installation
on your computer! Please refer to \sphinxhref{https://www.tug.org/texlive/}{TexLive} on how to install
TeX Live - a compatible latex distribution. But beware, the download size is huge!

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} \PYG{n+nb}{cd} docs
\PYGZdl{} make latexpdf
\PYGZdl{} evince build/latex/Spooq2.pdf
\end{sphinxVerbatim}


\subsubsection{Configuration}
\label{\detokenize{setup_development_testing:configuration}}
Themes, plugins, settings, … are defined in \sphinxtitleref{docs/source/conf.py}.


\paragraph{napoleon}
\label{\detokenize{setup_development_testing:id6}}
Enables support for parsing docstrings in NumPy / Google Style


\paragraph{intersphinx}
\label{\detokenize{setup_development_testing:id7}}
Allows linking to other projects’ documentation. E.g., PySpark, Python2
To add an external project, at the documentation link to \sphinxtitleref{intersphinx\_mapping} in \sphinxtitleref{conf.py}


\paragraph{recommonmark}
\label{\detokenize{setup_development_testing:id8}}
This allows you to write CommonMark (Markdown) inside of Docutils \& Sphinx projects instead
of rst.


\paragraph{plantuml}
\label{\detokenize{setup_development_testing:id9}}
Allows for inline Plant UML code (uml directive) which is automatically rendered into an
svg image and placed in the document. Allows also to source puml-files. See {\hyperref[\detokenize{architecture:architecture}]{\sphinxcrossref{\DUrole{std,std-ref}{Architecture Overview}}}}
for an example.


\section{Architecture Overview}
\label{\detokenize{architecture:architecture-overview}}\label{\detokenize{architecture:architecture}}\label{\detokenize{architecture::doc}}

\subsection{Typical Data Flow of a Spooq Data Pipeline}
\label{\detokenize{architecture:typical-data-flow-of-a-spooq-data-pipeline}}
\noindent\sphinxincludegraphics{{plantuml-fbc0e5f45d95a0b6c2674411ee1b66beabd77881}.png}


\subsection{Simplified Class Diagram}
\label{\detokenize{architecture:simplified-class-diagram}}
\noindent\sphinxincludegraphics{{plantuml-ecc628d5fbaa09e808a64774f48c0a18543d296b}.png}


\chapter{Indices and tables}
\label{\detokenize{index:indices-and-tables}}\begin{itemize}
\item {} 
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\DUrole{xref,std,std-ref}{search}

\end{itemize}


\renewcommand{\indexname}{Python Module Index}
\begin{sphinxtheindex}
\let\bigletter\sphinxstyleindexlettergroup
\bigletter{s}
\item\relax\sphinxstyleindexentry{spooq2.extractor.extractor}\sphinxstyleindexpageref{extractor/overview:\detokenize{module-spooq2.extractor.extractor}}
\item\relax\sphinxstyleindexentry{spooq2.extractor.jdbc}\sphinxstyleindexpageref{extractor/jdbc:\detokenize{module-spooq2.extractor.jdbc}}
\item\relax\sphinxstyleindexentry{spooq2.extractor.json\_files}\sphinxstyleindexpageref{extractor/json:\detokenize{module-spooq2.extractor.json_files}}
\item\relax\sphinxstyleindexentry{spooq2.loader.hive\_loader}\sphinxstyleindexpageref{loader/hive_loader:\detokenize{module-spooq2.loader.hive_loader}}
\item\relax\sphinxstyleindexentry{spooq2.loader.loader}\sphinxstyleindexpageref{loader/overview:\detokenize{module-spooq2.loader.loader}}
\item\relax\sphinxstyleindexentry{spooq2.pipeline.factory}\sphinxstyleindexpageref{pipeline/pipeline_factory:\detokenize{module-spooq2.pipeline.factory}}
\item\relax\sphinxstyleindexentry{spooq2.pipeline.pipeline}\sphinxstyleindexpageref{pipeline/pipeline:\detokenize{module-spooq2.pipeline.pipeline}}
\item\relax\sphinxstyleindexentry{spooq2.spooq2\_logger}\sphinxstyleindexpageref{base_classes/spooq2_logger:\detokenize{module-spooq2.spooq2_logger}}
\item\relax\sphinxstyleindexentry{spooq2.transformer.exploder}\sphinxstyleindexpageref{transformer/exploder:\detokenize{module-spooq2.transformer.exploder}}
\item\relax\sphinxstyleindexentry{spooq2.transformer.mapper}\sphinxstyleindexpageref{transformer/mapper:\detokenize{module-spooq2.transformer.mapper}}
\item\relax\sphinxstyleindexentry{spooq2.transformer.mapper\_custom\_data\_types}\sphinxstyleindexpageref{transformer/mapper:\detokenize{module-spooq2.transformer.mapper_custom_data_types}}
\item\relax\sphinxstyleindexentry{spooq2.transformer.newest\_by\_group}\sphinxstyleindexpageref{transformer/newest_by_group:\detokenize{module-spooq2.transformer.newest_by_group}}
\item\relax\sphinxstyleindexentry{spooq2.transformer.sieve}\sphinxstyleindexpageref{transformer/sieve:\detokenize{module-spooq2.transformer.sieve}}
\item\relax\sphinxstyleindexentry{spooq2.transformer.threshold\_cleaner}\sphinxstyleindexpageref{transformer/threshold_cleaner:\detokenize{module-spooq2.transformer.threshold_cleaner}}
\item\relax\sphinxstyleindexentry{spooq2.transformer.transformer}\sphinxstyleindexpageref{transformer/overview:\detokenize{module-spooq2.transformer.transformer}}
\end{sphinxtheindex}

\renewcommand{\indexname}{Index}
\raggedright\printindex
\end{document}